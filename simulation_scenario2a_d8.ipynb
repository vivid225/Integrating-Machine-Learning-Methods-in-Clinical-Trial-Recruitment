{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87fd3284-5ee0-45bb-92f3-8c737c02e248",
   "metadata": {
    "executionInfo": {
     "elapsed": 3279,
     "status": "ok",
     "timestamp": 1707464444506,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "87fd3284-5ee0-45bb-92f3-8c737c02e248",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "from scipy.stats import norm, binomtest\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "\n",
    "from sklearn import linear_model, svm, naive_bayes, ensemble\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd9b75-9988-49e9-98c2-566a7f57612f",
   "metadata": {
    "id": "0fbd9b75-9988-49e9-98c2-566a7f57612f"
   },
   "source": [
    "## Read functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ZzkESkMbubvs",
   "metadata": {
    "id": "ZzkESkMbubvs"
   },
   "outputs": [],
   "source": [
    "%run functions_scenario2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKvlxiIUH6fx",
   "metadata": {
    "id": "tKvlxiIUH6fx"
   },
   "source": [
    "## Scenario 2, Case 2 with interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "JT1oO4o7IJwF",
   "metadata": {
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1707464458106,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "JT1oO4o7IJwF"
   },
   "outputs": [],
   "source": [
    "def invlogit(p_list):\n",
    "    invlogit_values = []\n",
    "    for p in p_list:\n",
    "        invlogit_values.append(np.exp(p) / (1 + np.exp(p)))\n",
    "\n",
    "    return invlogit_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "uhuynYYgIJwG",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707464458616,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "uhuynYYgIJwG"
   },
   "outputs": [],
   "source": [
    "design_list = [5,8,10]\n",
    "patient_n_list = [5400,680,170] # n_patient_per_plan\n",
    "\n",
    "# def plan_true_rate(row, impact_rates):\n",
    "#     tmp = row * impact_rates\n",
    "#     tmp = tmp[tmp != 0]\n",
    "#     return np.prod(tmp) * 0.05\n",
    "\n",
    "def generate_data_step1(design_number = 5, n_rounds = 4, n_patient_per_plan = 5400, seed=42):\n",
    "\n",
    "  # design_list = [5,8,10]\n",
    "\n",
    "  # Record the start time\n",
    "#   start_time = time.time()\n",
    "\n",
    "  # scenario 2:\n",
    "  random.seed(seed)\n",
    "\n",
    "  # n_design = design_number\n",
    "  n_patient_per_plan_round = n_patient_per_plan/n_rounds # each round is capped by this patient number\n",
    "\n",
    "  # Create all possible combinations of design features\n",
    "  design_combinations = list(product([1, 0], repeat=n_design))\n",
    "\n",
    "  design_feature_df = pd.DataFrame(design_combinations, columns=[f'Design_Feature_{i+1}' for i in range(n_design)])\n",
    "  # add interaction term:\n",
    "  # interaction = [1 if row['Design_Feature_1'] + row['Design_Feature_2'] == 2 else 0 for _, row in design_feature_df.iterrows()]\n",
    "  # design_feature_df['interaction'] = interaction\n",
    "\n",
    "  design_feature_df['recruitment_plan'] = [x + 1 for x in range(2 ** n_design)]\n",
    "\n",
    "  # design_feature_df\n",
    "  random.seed(seed)\n",
    "  # impact_rates = np.array([random.uniform(0.6, 1.5) for _ in range(n_design)])\n",
    "  impact_rates = np.array([0.5, -0.5, 0, 0, 0, 0, 0, 0]) # design 2 has the highest rr\n",
    "  response_rate_list = np.dot(np.array(design_feature_df.iloc[:,:(n_design)]), impact_rates)\n",
    "  # normalize to [0,1]\n",
    "  # normalized_values = normalize_to_0_1(response_rate_list) * 0.115\n",
    "  # inverse logit\n",
    "  normalized_values = np.array(invlogit(response_rate_list)) * 0.11\n",
    "  design_feature_df['plan_response_rate'] = normalized_values\n",
    "\n",
    "  # Apply plan_true_rate to each row\n",
    "  # result_column = design_feature_df.iloc[:, :n_design].apply(lambda row: plan_true_rate(row, impact_rates), axis=1)\n",
    "\n",
    "  # Add the result_column to the design_feature_df\n",
    "  # design_feature_df['plan_response_rate'] = result_column\n",
    "\n",
    "  # each combination repeat for n_patient_per_plan_round\n",
    "  design_feature_df = pd.DataFrame(np.repeat(design_feature_df.values, n_patient_per_plan_round, axis=0), columns=design_feature_df.columns)\n",
    "\n",
    "  # add response outcome column:\n",
    "  grouped_df = design_feature_df.groupby(list(design_feature_df.columns)).size().reset_index(name='group_size')\n",
    "\n",
    "  random.seed(seed)\n",
    "  random_seeds_for_resp = [random.randint(1, 100000) for _ in range(len(grouped_df))]\n",
    "\n",
    "  def generate_responses(row):\n",
    "      rate = row['plan_response_rate']\n",
    "      num = row['group_size']\n",
    "      # random.seed(42)\n",
    "      seed1 = random_seeds_for_resp[row.name]\n",
    "      np.random.seed(seed1)\n",
    "      return np.random.binomial(n=1, p=rate, size=int(num))\n",
    "\n",
    "  grouped_df['response'] = grouped_df.apply(generate_responses, axis=1)\n",
    "\n",
    "  # Explode the 'response' arrays to expand the DataFrame\n",
    "  expanded_df = grouped_df.explode('response').reset_index(drop=True)\n",
    "  expanded_df['response'] = expanded_df['response'].astype(float)\n",
    "\n",
    "#   end_time = time.time()\n",
    "\n",
    "#   elapsed_time = end_time - start_time\n",
    "\n",
    "#   print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "  return expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hJtnGfuuIMr_",
   "metadata": {
    "id": "hJtnGfuuIMr_"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zVgytCZXbtpQ",
   "metadata": {
    "id": "zVgytCZXbtpQ"
   },
   "outputs": [],
   "source": [
    "def ensemble_model_fit(data, data_pred):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1),\n",
    "        data['response'],\n",
    "        test_size=0.2,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Define the VotingClassifier with the individual classifiers\n",
    "    voting_classifier = ensemble.VotingClassifier(\n",
    "        estimators=[\n",
    "            ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0))\n",
    "#             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0))\n",
    "                    # ('SVM', svm.SVC(kernel='linear', C=1.0, random_state=0, probability=True, class_weight='balanced'))\n",
    "#                     ('RF', ensemble.RandomForestClassifier(n_estimators=200, criterion='gini', random_state=0))\n",
    "                    # ('XGB', XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=0))\n",
    "                   ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameter grid to search\n",
    "    # Best Hyperparameters: {'LR__C': 0.01, 'RF__n_estimators': 50, 'XGB__n_estimators': 50}\n",
    "    param_grid = {\n",
    "        # 'NB__alpha': [0.01, 0.05, 0.1],  # '__' is used to specify hyperparameters for individual classifiers\n",
    "        'LR__C': [0.01, 0.1, 1.0] # [0.01, 0.05, 0.1]\n",
    "        # 'Ridge__C': [0.01]\n",
    "        # 'SVM__C': [0.01, 0.05, 0.1]\n",
    "#         'RF__n_estimators': [50] # [10, 30, 50]\n",
    "        # 'XGB__n_estimators': [50]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "    # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "    final_voting_classifier = grid_search.best_estimator_\n",
    "    final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities instead of binary outcomes on the test set\n",
    "    y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "    y_pred_test = final_voting_classifier.predict(X_test)\n",
    "    X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1)\n",
    "    y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0optT4Hwjkte",
   "metadata": {
    "id": "0optT4Hwjkte"
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def ensemble_model_fit(data, data_pred):\n",
    "#     # Split the data into training and testing sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         data.drop(['recruitment_plan', 'plan_response_rate', 'group_size','response'], axis=1),\n",
    "#         data['response'],\n",
    "#         test_size=0.2,\n",
    "#         random_state=0\n",
    "#     )\n",
    "\n",
    "#     # Define the Logistic Regression model\n",
    "#     logistic_regression = LogisticRegression(C=0.01, max_iter=200,random_state=0)\n",
    "\n",
    "#     # Fit the Logistic Regression model on the training data\n",
    "#     logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "#     # Print out the coefficients of the logistic regression model\n",
    "#     print(\"Coefficients:\", logistic_regression.coef_)\n",
    "#     print(\"Intercept:\", logistic_regression.intercept_)\n",
    "#     print(\"Coefficients shape:\", logistic_regression.coef_.shape)\n",
    "\n",
    "#     # Predict probabilities instead of binary outcomes on the test set\n",
    "#     y_pred_proba_test = logistic_regression.predict_proba(X_test)\n",
    "#     y_pred_test = logistic_regression.predict(X_test)\n",
    "\n",
    "#     # You can also predict probabilities for the prediction data (data_pred)\n",
    "#     X_dt = data_pred.drop(['recruitment_plan', 'plan_response_rate', 'group_size','response'], axis=1)\n",
    "#     y_pred = logistic_regression.predict_proba(X_dt)\n",
    "\n",
    "#     return y_pred\n",
    "\n",
    "# # Call the function\n",
    "# # ensemble_model_fit(data, data_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V73u9G9JJkZZ",
   "metadata": {
    "id": "V73u9G9JJkZZ"
   },
   "source": [
    "### Simulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kGy7VUgnz895",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707464451915,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "kGy7VUgnz895"
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "# create a list of random seeds\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(1, 100000) for _ in range(n_sim)]\n",
    "\n",
    "design_list = [5,8,10]\n",
    "patient_n_list = [5468,680,170] # n_patient_per_plan\n",
    "\n",
    "n_design = 8\n",
    "n_patient_per_plan = 680\n",
    "n_rounds = 6\n",
    "total_n = n_patient_per_plan * (2**n_design)\n",
    "\n",
    "## sample size determination\n",
    "beta = 0.2\n",
    "power = 1 - beta\n",
    "alpha = 0.05\n",
    "delta = 0.01 # effect size\n",
    "\n",
    "## early stopping\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a66c5-ceb1-4ede-abfc-a99411b182ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f0a66c5-ceb1-4ede-abfc-a99411b182ea",
    "outputId": "1c344da1-bed8-44c5-ce38-33f08082fddc"
   },
   "outputs": [],
   "source": [
    "rr_dict = {\"step{}_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "sample_size_dict = {\"step{}_sample_size\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "last_round_sample_size = []\n",
    "random_rr_dict = {\"step{}_random_max_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "random_rr_dict.update({\n",
    "    \"step{}_random_mean_rr\".format(r): [] for r in range(1, n_rounds + 1)\n",
    "})\n",
    "\n",
    "stopping_dict = {\"early_stopping\": [],\n",
    "                 \"early_stopping_plan\":[],\n",
    "                 \"early_stopping_orr\":[],\n",
    "                 \"early_stopping_size\":[]}\n",
    "final_plan_number = []\n",
    "highest_rr_overall = []\n",
    "highest_true_rr = []\n",
    "\n",
    "better_chance = []\n",
    "\n",
    "orr_total = []\n",
    "orr_total_adaptive = []\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for seed in random_seeds:\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "    print([f\"{key}: {len(value)}\" for key, value in rr_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "    event_list = 0\n",
    "    event_list_adaptive = 0\n",
    "    max_rr = []\n",
    "\n",
    "    # step 1:\n",
    "    ## Generate dataset\n",
    "    dt_design_5 = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed)\n",
    "    highest_true_rr.append(np.max(dt_design_5['plan_response_rate']))\n",
    "    print(\"empirical response rate\", dt_design_5['response'].mean())\n",
    "    ## save for benchmark results\n",
    "    maxidx = np.argmax(dt_design_5['plan_response_rate'])\n",
    "    random_rr_dict['step1_random_max_rr'].append(dt_design_5.iloc[maxidx,6])\n",
    "    random_rr_dict['step1_random_mean_rr'].append(dt_design_5['plan_response_rate'].mean())\n",
    "\n",
    "    ## Ensemble modelling:\n",
    "    y_pred = ensemble_model_fit(data=dt_design_5, data_pred = dt_design_5)\n",
    "\n",
    "    ## select recruitment plan\n",
    "    pred_df = pd.DataFrame(np.hstack((dt_design_5,  y_pred[:, 1].reshape(-1, 1))),\n",
    "                             columns=list(dt_design_5.columns) + ['predicted_response_rate'])\n",
    "    pred_df_rr = pred_df.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "    x = pred_df_rr['predicted_response_rate'].values\n",
    "    if len(x) <= 10:\n",
    "        best_k = 2\n",
    "    else:\n",
    "        best_k = kmeans_fit(data = x)['best_k']\n",
    "    kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "    merged_df = pd.merge(pred_df_rr, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "    cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "    highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "    highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "    p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "    highest_cluster['p_vec'] = p_vec_next\n",
    "    highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "    # highest_cluster = pred_df_rr\n",
    "\n",
    "    # highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "    # highest_cluster['cluster_number'] = highest_cluster['recruitment_plan']\n",
    "\n",
    "    ## prepare to chance of better performance:\n",
    "    temp = dt_design_5[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "    event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(\"step1\", np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(temp)\n",
    "    p0 = (temp['plan_response_rate'].mean())\n",
    "\n",
    "    rr_dict[\"step1_rr\"].append(dt_design_5['plan_response_rate'].mean())\n",
    "    sample_size_dict[\"step1_sample_size\"].append(len(dt_design_5))\n",
    "\n",
    "    for round in range(2, n_rounds+1):\n",
    "\n",
    "        rr_dict[\"step\"+str(round)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "        plan_number_dict[\"step\"+str(round)+\"_plan_number\"].append(len(p_vec_next))\n",
    "\n",
    "        ## when it comes to the last round:\n",
    "        if round == n_rounds:\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            if round == 2: # if the last round is round 2\n",
    "                remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "            else:\n",
    "                remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "            print(\"remaining size for last Round \" + str(round) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            ## data generation, combine previous data:\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                          design_number = n_design, n_rounds = n_rounds,\n",
    "                                                          n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(0)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(len(temp), temp['plan_response_rate'].mean())\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        ## If haven't reached the last round:\n",
    "        ## check remaining sample size:\n",
    "        if round == 2:\n",
    "            remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "        else:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "        print(\"remaining size for Round \" + str(round) + \": \" + str(remaining_size))\n",
    "\n",
    "        ## determine whether move on to step 2:\n",
    "        if len(highest_cluster) == 1: # if there is only one plan left\n",
    "\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size) # record the last round sample size\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(1)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        ## sample size determination:\n",
    "        if round == 2:\n",
    "            dt_design_5_step2up_overall = dt_design_5\n",
    "        orr_1 = dt_design_5_step2up_overall['response'].mean() # observed overall response rates for previous rounds\n",
    "        orr_2 = orr_1 + delta\n",
    "        n_1 = len(dt_design_5_step2up_overall)\n",
    "\n",
    "        size_step2up = sample_size_calc(orr_1, n_1, delta=delta, alpha=alpha, power=power) # total size for dataset\n",
    "        if size_step2up > 0 and size_step2up < 1000:\n",
    "            size_step2up = 1000 # if size in [0,1000], then it is 1000 for this round.\n",
    "        elif size_step2up >= 1000:\n",
    "            size_step2up = min(size_step2up, int(total_n/n_rounds)) # dataset size capped by the n_patient_per_plan\n",
    "        else:\n",
    "        # if size_step2up <= 0:\n",
    "            # the process stops at this step\n",
    "            print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up) + 'lt 0, break')\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(1)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up))\n",
    "\n",
    "        sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(size_step2up)\n",
    "\n",
    "        ## save benchmark results for last round:\n",
    "        dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                            n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "        maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "        ## data generation, combine previous data:\n",
    "        dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                  design_number = n_design, n_rounds = n_rounds,\n",
    "                                                  n_patient_per_plan = n_patient_per_plan, size = int(size_step2up), seed=seed)\n",
    "\n",
    "        plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "        if round == 2:\n",
    "            supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)]\n",
    "        else:\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "        dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "        dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "        ## Ensemble model fitting:\n",
    "        dt_design_5_step2up.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "        y_pred2up = ensemble_model_fit(data = dt_design_5_step2up_overall, data_pred = dt_design_5_step2up)\n",
    "        pred_df_step2up = pd.DataFrame(np.hstack((dt_design_5_step2up,  y_pred2up[:, 1].reshape(-1, 1))),\n",
    "                                    columns=list(dt_design_5_step2up.columns) + ['predicted_response_rate'])\n",
    "        pred_df_rr_step2up = pred_df_step2up.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "        ## select recruitment plans:\n",
    "        x = pred_df_rr_step2up['predicted_response_rate'].values\n",
    "\n",
    "        if len(x) <= 10:\n",
    "            best_k = 2\n",
    "        else:\n",
    "            best_k = kmeans_fit(data = x)['best_k']\n",
    "        kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "        ## match the cluster results back to the original data\n",
    "        highest_cluster_previous = highest_cluster # save previous cluster results\n",
    "\n",
    "        merged_df = pd.merge(pred_df_rr_step2up, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "        cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "        highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "        highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "        # p_vec_previous = p_vec_next # save p_vec of previous round\n",
    "        p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "\n",
    "        highest_cluster['p_vec'] = p_vec_next\n",
    "\n",
    "        highest_cluster = pd.merge(highest_cluster, dt_design_5_step2up[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "        ## prepare to chance of better performance:\n",
    "        temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "        event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(temp)\n",
    "\n",
    "        ## check early stopping predicted ORR:\n",
    "        orr_df = pd.merge(highest_cluster_previous, highest_cluster[['recruitment_plan','predicted_response_rate','p_vec']], on='recruitment_plan', how='left')\n",
    "        orr_df.fillna(0, inplace=True)\n",
    "        p_orr_1 = np.dot(np.array(orr_df['p_vec_x']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        p_orr_2 = np.dot(np.array(orr_df['p_vec_y']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        print(\"orr termination\", p_orr_1, p_orr_2, p_orr_2 - p_orr_1)\n",
    "\n",
    "        if (p_orr_2 - p_orr_1 < epsilon):\n",
    "            # step 3 use the same strategy of step2\n",
    "            print(i, p_orr_1, p_orr_2, \"early stop at Round \" + str(round))\n",
    "            rr_dict[\"step\"+str(round+1)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                                n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            ### update remaining size:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "            print(\"early stop, remaining size for Round\" + str(round + 1) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(1)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round+1), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0bC5wHRwkWx",
   "metadata": {
    "id": "t0bC5wHRwkWx"
   },
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7XcvEocqYOP7",
   "metadata": {
    "id": "7XcvEocqYOP7"
   },
   "outputs": [],
   "source": [
    "rr_df = pd.DataFrame(rr_dict)\n",
    "rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FDz-e9CSKaWX",
   "metadata": {
    "id": "FDz-e9CSKaWX"
   },
   "outputs": [],
   "source": [
    "# Calculate average rounds:\n",
    "row_non_nan_counts = rr_df.count(axis=1)\n",
    "\n",
    "print(f\"{np.mean(row_non_nan_counts):.1f} ({np.std(row_non_nan_counts):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9L26KiTYJUZo",
   "metadata": {
    "id": "9L26KiTYJUZo"
   },
   "outputs": [],
   "source": [
    "# orr for each round:\n",
    "result_dict = {}\n",
    "for key, values in rr_df.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hw_mU1Xy9kIw",
   "metadata": {
    "id": "Hw_mU1Xy9kIw"
   },
   "outputs": [],
   "source": [
    "# overall orr:\n",
    "result_dict = np.array(orr_total) / total_n\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dAeUn1ayxgXH",
   "metadata": {
    "id": "dAeUn1ayxgXH"
   },
   "outputs": [],
   "source": [
    "# highest true rr:\n",
    "mean_result=(np.mean(highest_true_rr))\n",
    "std_result=(np.std(highest_true_rr))\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NOSItDYuQwa8",
   "metadata": {
    "id": "NOSItDYuQwa8"
   },
   "outputs": [],
   "source": [
    "# adaptive learning orr:\n",
    "result_dict = np.array(orr_total_adaptive) / (145152)\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C703gqxdH6Fd",
   "metadata": {
    "id": "C703gqxdH6Fd"
   },
   "outputs": [],
   "source": [
    "# average plan number for each round:\n",
    "result_dict = {}\n",
    "for key, values in plan_number_dict.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.1f} ({std_value:.1f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_T2Gb6t9hXq",
   "metadata": {
    "id": "P_T2Gb6t9hXq"
   },
   "outputs": [],
   "source": [
    "# early stopping probabilities:\n",
    "means = {key: np.mean(values) for key, values in stopping_dict.items()}\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W3dHQr2AxzDt",
   "metadata": {
    "id": "W3dHQr2AxzDt"
   },
   "outputs": [],
   "source": [
    "# sample size for the last round:\n",
    "print(np.mean(last_round_sample_size), np.std(last_round_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jSdp6rIVngsj",
   "metadata": {
    "id": "jSdp6rIVngsj"
   },
   "outputs": [],
   "source": [
    "# probability of better performance compared to the benchmark:\n",
    "np.mean(better_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C1KcRlYqwT5H",
   "metadata": {
    "id": "C1KcRlYqwT5H"
   },
   "outputs": [],
   "source": [
    "# plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "result_dict = {}\n",
    "for key, values in sample_size_dict.items():\n",
    "    mean_value = np.nanmean(values)\n",
    "    std_value = np.nanstd(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uHSypRgXKNRd",
   "metadata": {
    "id": "uHSypRgXKNRd"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4T8kpJkkKNRf",
   "metadata": {
    "id": "4T8kpJkkKNRf"
   },
   "outputs": [],
   "source": [
    "def ensemble_model_fit(data, data_pred):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1),\n",
    "        data['response'],\n",
    "        test_size=0.2,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Define the VotingClassifier with the individual classifiers\n",
    "    voting_classifier = ensemble.VotingClassifier(\n",
    "        estimators=[\n",
    "            # ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0))\n",
    "#             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0))\n",
    "                    # ('SVM', svm.SVC(kernel='linear', C=1.0, random_state=0, probability=True, class_weight='balanced'))\n",
    "                    ('RF', ensemble.RandomForestClassifier(criterion='gini', random_state=0))\n",
    "                    # ('XGB', XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=0))\n",
    "                   ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameter grid to search\n",
    "    # Best Hyperparameters: {'LR__C': 0.01, 'RF__n_estimators': 50, 'XGB__n_estimators': 50}\n",
    "    param_grid = {\n",
    "        # 'NB__alpha': [0.01, 0.05, 0.1],  # '__' is used to specify hyperparameters for individual classifiers\n",
    "        # 'LR__C': [0.01] # [0.01, 0.05, 0.1]\n",
    "        # 'Ridge__C': [0.01]\n",
    "        # 'SVM__C': [0.01, 0.05, 0.1]\n",
    "        'RF__n_estimators': [50, 100, 200]\n",
    "#         'RF__max_depth': [10, 20, None]\n",
    "        # 'XGB__n_estimators': [50]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "    # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "    final_voting_classifier = grid_search.best_estimator_\n",
    "    final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities instead of binary outcomes on the test set\n",
    "    y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "    y_pred_test = final_voting_classifier.predict(X_test)\n",
    "    X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1)\n",
    "    y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uVNXr0f9KNRf",
   "metadata": {
    "id": "uVNXr0f9KNRf"
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def ensemble_model_fit(data, data_pred):\n",
    "#     # Split the data into training and testing sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         data.drop(['recruitment_plan', 'plan_response_rate', 'group_size','response'], axis=1),\n",
    "#         data['response'],\n",
    "#         test_size=0.2,\n",
    "#         random_state=0\n",
    "#     )\n",
    "\n",
    "#     # Define the Logistic Regression model\n",
    "#     logistic_regression = LogisticRegression(C=0.01, max_iter=200,random_state=0)\n",
    "\n",
    "#     # Fit the Logistic Regression model on the training data\n",
    "#     logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "#     # Print out the coefficients of the logistic regression model\n",
    "#     print(\"Coefficients:\", logistic_regression.coef_)\n",
    "#     print(\"Intercept:\", logistic_regression.intercept_)\n",
    "#     print(\"Coefficients shape:\", logistic_regression.coef_.shape)\n",
    "\n",
    "#     # Predict probabilities instead of binary outcomes on the test set\n",
    "#     y_pred_proba_test = logistic_regression.predict_proba(X_test)\n",
    "#     y_pred_test = logistic_regression.predict(X_test)\n",
    "\n",
    "#     # You can also predict probabilities for the prediction data (data_pred)\n",
    "#     X_dt = data_pred.drop(['recruitment_plan', 'plan_response_rate', 'group_size','response'], axis=1)\n",
    "#     y_pred = logistic_regression.predict_proba(X_dt)\n",
    "\n",
    "#     return y_pred\n",
    "\n",
    "# # Call the function\n",
    "# # ensemble_model_fit(data, data_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0L4C5O0LKNRg",
   "metadata": {
    "id": "0L4C5O0LKNRg"
   },
   "source": [
    "### Simulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XaFmXCTbKNRg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "error",
     "timestamp": 1707464436206,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "XaFmXCTbKNRg",
    "outputId": "08564dde-8fde-464a-f46c-6042d0650660"
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "# create a list of random seeds\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(1, 100000) for _ in range(n_sim)]\n",
    "\n",
    "design_list = [5,8,10]\n",
    "patient_n_list = [5468,680,170] # n_patient_per_plan\n",
    "\n",
    "n_design = 8\n",
    "n_patient_per_plan = 680\n",
    "n_rounds = 6\n",
    "total_n = n_patient_per_plan * (2**n_design)\n",
    "\n",
    "## sample size determination\n",
    "beta = 0.2\n",
    "power = 1 - beta\n",
    "alpha = 0.05\n",
    "delta = 0.01 # effect size\n",
    "\n",
    "## early stopping\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E1DotwS2KNRg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 542845,
     "status": "ok",
     "timestamp": 1707455312537,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "E1DotwS2KNRg",
    "outputId": "768e8c04-6c96-450f-f88c-3c415c9a9d61"
   },
   "outputs": [],
   "source": [
    "rr_dict = {\"step{}_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "sample_size_dict = {\"step{}_sample_size\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "last_round_sample_size = []\n",
    "random_rr_dict = {\"step{}_random_max_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "random_rr_dict.update({\n",
    "    \"step{}_random_mean_rr\".format(r): [] for r in range(1, n_rounds + 1)\n",
    "})\n",
    "\n",
    "stopping_dict = {\"early_stopping\": [],\n",
    "                 \"early_stopping_plan\":[],\n",
    "                 \"early_stopping_orr\":[],\n",
    "                 \"early_stopping_size\":[]}\n",
    "final_plan_number = []\n",
    "highest_rr_overall = []\n",
    "\n",
    "better_chance = []\n",
    "\n",
    "orr_total = []\n",
    "orr_total_adaptive = []\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for seed in random_seeds:\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "    print([f\"{key}: {len(value)}\" for key, value in rr_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "    event_list = 0\n",
    "    event_list_adaptive = 0\n",
    "    max_rr = []\n",
    "\n",
    "    # step 1:\n",
    "    ## Generate dataset\n",
    "    dt_design_5 = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed)\n",
    "    print(\"empirical response rate\", dt_design_5['response'].mean())\n",
    "    ## save for benchmark results\n",
    "    maxidx = np.argmax(dt_design_5['plan_response_rate'])\n",
    "    random_rr_dict['step1_random_max_rr'].append(dt_design_5.iloc[maxidx,6])\n",
    "    random_rr_dict['step1_random_mean_rr'].append(dt_design_5['plan_response_rate'].mean())\n",
    "\n",
    "    ## Ensemble modelling:\n",
    "    y_pred = ensemble_model_fit(data=dt_design_5, data_pred = dt_design_5)\n",
    "\n",
    "    ## select recruitment plan\n",
    "    pred_df = pd.DataFrame(np.hstack((dt_design_5,  y_pred[:, 1].reshape(-1, 1))),\n",
    "                             columns=list(dt_design_5.columns) + ['predicted_response_rate'])\n",
    "    pred_df_rr = pred_df.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "    x = pred_df_rr['predicted_response_rate'].values\n",
    "    if len(x) <= 10:\n",
    "        best_k = 2\n",
    "    else:\n",
    "        best_k = kmeans_fit(data = x)['best_k']\n",
    "    kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "    merged_df = pd.merge(pred_df_rr, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "    cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "    highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "    highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "    p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "    highest_cluster['p_vec'] = p_vec_next\n",
    "    highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "    # highest_cluster = pred_df_rr\n",
    "\n",
    "    # highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "    # highest_cluster['cluster_number'] = highest_cluster['recruitment_plan']\n",
    "\n",
    "    ## prepare to chance of better performance:\n",
    "    temp = dt_design_5[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "    event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(\"step1\", np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(temp)\n",
    "    p0 = (temp['plan_response_rate'].mean())\n",
    "\n",
    "    rr_dict[\"step1_rr\"].append(dt_design_5['plan_response_rate'].mean())\n",
    "    sample_size_dict[\"step1_sample_size\"].append(len(dt_design_5))\n",
    "\n",
    "    for round in range(2, n_rounds+1):\n",
    "\n",
    "        rr_dict[\"step\"+str(round)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "        plan_number_dict[\"step\"+str(round)+\"_plan_number\"].append(len(p_vec_next))\n",
    "\n",
    "        ## when it comes to the last round:\n",
    "        if round == n_rounds:\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            if round == 2: # if the last round is round 2\n",
    "                remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "            else:\n",
    "                remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "            print(\"remaining size for last Round \" + str(round) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            ## data generation, combine previous data:\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                          design_number = n_design, n_rounds = n_rounds,\n",
    "                                                          n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(0)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(len(temp), temp['plan_response_rate'].mean())\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        ## If haven't reached the last round:\n",
    "        ## check remaining sample size:\n",
    "        if round == 2:\n",
    "            remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "        else:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "        print(\"remaining size for Round \" + str(round) + \": \" + str(remaining_size))\n",
    "\n",
    "        ## determine whether move on to step 2:\n",
    "        if len(highest_cluster) == 1: # if there is only one plan left\n",
    "\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size) # record the last round sample size\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(1)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        ## sample size determination:\n",
    "        if round == 2:\n",
    "            dt_design_5_step2up_overall = dt_design_5\n",
    "        orr_1 = dt_design_5_step2up_overall['response'].mean() # observed overall response rates for previous rounds\n",
    "        orr_2 = orr_1 + delta\n",
    "        n_1 = len(dt_design_5_step2up_overall)\n",
    "\n",
    "        size_step2up = sample_size_calc(orr_1, n_1, delta=delta, alpha=alpha, power=power) # total size for dataset\n",
    "        if size_step2up > 0 and size_step2up < 1000:\n",
    "            size_step2up = 1000 # if size in [0,1000], then it is 1000 for this round.\n",
    "        elif size_step2up >= 1000:\n",
    "            size_step2up = min(size_step2up, int(total_n/n_rounds)) # dataset size capped by the n_patient_per_plan\n",
    "        else:\n",
    "        # if size_step2up <= 0:\n",
    "            # the process stops at this step\n",
    "            print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up) + 'lt 0, break')\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(1)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up))\n",
    "\n",
    "        sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(size_step2up)\n",
    "\n",
    "        ## save benchmark results for last round:\n",
    "        dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                            n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "        maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "        ## data generation, combine previous data:\n",
    "        dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                  design_number = n_design, n_rounds = n_rounds,\n",
    "                                                  n_patient_per_plan = n_patient_per_plan, size = int(size_step2up), seed=seed)\n",
    "\n",
    "        plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "        if round == 2:\n",
    "            supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)]\n",
    "        else:\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "        dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "        dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "        ## Ensemble model fitting:\n",
    "        dt_design_5_step2up.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "        y_pred2up = ensemble_model_fit(data = dt_design_5_step2up_overall, data_pred = dt_design_5_step2up)\n",
    "        pred_df_step2up = pd.DataFrame(np.hstack((dt_design_5_step2up,  y_pred2up[:, 1].reshape(-1, 1))),\n",
    "                                    columns=list(dt_design_5_step2up.columns) + ['predicted_response_rate'])\n",
    "        pred_df_rr_step2up = pred_df_step2up.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "        ## select recruitment plans:\n",
    "        x = pred_df_rr_step2up['predicted_response_rate'].values\n",
    "\n",
    "        if len(x) <= 10:\n",
    "            best_k = 2\n",
    "        else:\n",
    "            best_k = kmeans_fit(data = x)['best_k']\n",
    "        kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "        ## match the cluster results back to the original data\n",
    "        highest_cluster_previous = highest_cluster # save previous cluster results\n",
    "\n",
    "        merged_df = pd.merge(pred_df_rr_step2up, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "        cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "        highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "        highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "        # p_vec_previous = p_vec_next # save p_vec of previous round\n",
    "        p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "\n",
    "        highest_cluster['p_vec'] = p_vec_next\n",
    "\n",
    "        highest_cluster = pd.merge(highest_cluster, dt_design_5_step2up[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "        ## prepare to chance of better performance:\n",
    "        temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "        event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(temp)\n",
    "\n",
    "        ## check early stopping predicted ORR:\n",
    "        orr_df = pd.merge(highest_cluster_previous, highest_cluster[['recruitment_plan','predicted_response_rate','p_vec']], on='recruitment_plan', how='left')\n",
    "        orr_df.fillna(0, inplace=True)\n",
    "        p_orr_1 = np.dot(np.array(orr_df['p_vec_x']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        p_orr_2 = np.dot(np.array(orr_df['p_vec_y']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        print(\"orr termination\", p_orr_1, p_orr_2, p_orr_2 - p_orr_1)\n",
    "\n",
    "        if (p_orr_2 - p_orr_1 < epsilon):\n",
    "            # step 3 use the same strategy of step2\n",
    "            print(i, p_orr_1, p_orr_2, \"early stop at Round \" + str(round))\n",
    "            rr_dict[\"step\"+str(round+1)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                                n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            ### update remaining size:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "            print(\"early stop, remaining size for Round\" + str(round + 1) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(1)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round+1), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZSYI_uHYKNRh",
   "metadata": {
    "id": "ZSYI_uHYKNRh"
   },
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOOp7mX4KNRh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1707455312538,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "XOOp7mX4KNRh",
    "outputId": "4079a555-223e-4303-b5cf-4c8149044368"
   },
   "outputs": [],
   "source": [
    "rr_df = pd.DataFrame(rr_dict)\n",
    "rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UVSfzptULdKK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1707455312539,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "UVSfzptULdKK",
    "outputId": "25238bb4-606a-4335-c57e-2a862740e8a6"
   },
   "outputs": [],
   "source": [
    "# Calculate average rounds:\n",
    "row_non_nan_counts = rr_df.count(axis=1)\n",
    "\n",
    "print(f\"{np.mean(row_non_nan_counts):.1f} ({np.std(row_non_nan_counts):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZwypDIjzKNRi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1707455312539,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "ZwypDIjzKNRi",
    "outputId": "c3f67f52-4e4b-481c-f178-c818e5d49d26"
   },
   "outputs": [],
   "source": [
    "# orr for each round:\n",
    "result_dict = {}\n",
    "for key, values in rr_df.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vg_qWpVzKNRi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1707455666262,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "vg_qWpVzKNRi",
    "outputId": "874fb9b7-97f0-4b10-a7aa-66f996462b82"
   },
   "outputs": [],
   "source": [
    "# overall orr:\n",
    "result_dict = np.array(orr_total) / total_n\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "li_BxdgNKNRi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1707455718378,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "li_BxdgNKNRi",
    "outputId": "6c8b96d4-7110-43dd-c8de-19128d835157"
   },
   "outputs": [],
   "source": [
    "# adaptive learning orr:\n",
    "result_dict = np.array(orr_total_adaptive) / (145152)\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3TMTjJaKNRi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1707455695027,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "f3TMTjJaKNRi",
    "outputId": "7ffecc55-55c3-4be8-fabb-bbddf7a3e7b2"
   },
   "outputs": [],
   "source": [
    "# highest true rr:\n",
    "mean_result = (np.mean(highest_rr_overall))\n",
    "std_result = (np.std(highest_rr_overall))\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ObG-atEyKNRi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1707455738828,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "ObG-atEyKNRi",
    "outputId": "ab0d5c93-6eaf-40f8-f32c-f3f731fdbd2a"
   },
   "outputs": [],
   "source": [
    "# average plan number for each round:\n",
    "result_dict = {}\n",
    "for key, values in plan_number_dict.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.1f} ({std_value:.1f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PxuuVwAwKNRi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1707455312547,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "PxuuVwAwKNRi",
    "outputId": "9681e922-ac43-44a2-b324-fe62886e31a6"
   },
   "outputs": [],
   "source": [
    "# early stopping probabilities:\n",
    "means = {key: np.mean(values) for key, values in stopping_dict.items()}\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mxO9LpGvKNRj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1707455312547,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "mxO9LpGvKNRj",
    "outputId": "56e60500-3374-438a-d590-43247fb17f7a"
   },
   "outputs": [],
   "source": [
    "# sample size for the last round:\n",
    "print(np.mean(last_round_sample_size), np.std(last_round_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rhySjyfeKNRj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1707455312540,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "rhySjyfeKNRj",
    "outputId": "2ba7f5f0-b645-417f-dba0-d2658544dc1e"
   },
   "outputs": [],
   "source": [
    "# plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "result_dict = {}\n",
    "for key, values in sample_size_dict.items():\n",
    "    mean_value = np.nanmean(values)\n",
    "    std_value = np.nanstd(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UntkTQE5VJn0",
   "metadata": {
    "id": "UntkTQE5VJn0"
   },
   "outputs": [],
   "source": [
    "# probability of better performance compared to the benchmark:\n",
    "np.mean(better_chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O9EfgKcZKSFo",
   "metadata": {
    "id": "O9EfgKcZKSFo"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4VbKqb3LKSF_",
   "metadata": {
    "id": "4VbKqb3LKSF_"
   },
   "outputs": [],
   "source": [
    "def ensemble_model_fit(data, data_pred):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1),\n",
    "        data['response'],\n",
    "        test_size=0.2,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Define the VotingClassifier with the individual classifiers\n",
    "    voting_classifier = ensemble.VotingClassifier(\n",
    "        estimators=[\n",
    "            # ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0))\n",
    "#             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0))\n",
    "                    # ('SVM', svm.SVC(kernel='linear', C=1.0, random_state=0, probability=True, class_weight='balanced'))\n",
    "#                     ('RF', ensemble.RandomForestClassifier(n_estimators=200, criterion='gini', random_state=0))\n",
    "                    ('XGB', XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=0))\n",
    "                   ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameter grid to search\n",
    "    # Best Hyperparameters: {'LR__C': 0.01, 'RF__n_estimators': 50, 'XGB__n_estimators': 50}\n",
    "    param_grid = {\n",
    "        # 'NB__alpha': [0.01, 0.05, 0.1],  # '__' is used to specify hyperparameters for individual classifiers\n",
    "        # 'LR__C': [0.01] # [0.01, 0.05, 0.1]\n",
    "        # 'Ridge__C': [0.01]\n",
    "        # 'SVM__C': [0.01, 0.05, 0.1]\n",
    "#         'RF__n_estimators': [50] # [10, 30, 50]\n",
    "        'XGB__learning_rate': [0.01, 0.1, 1],\n",
    "        'XGB__n_estimators': [50, 100, 200]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "    final_voting_classifier = grid_search.best_estimator_\n",
    "    final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities instead of binary outcomes on the test set\n",
    "    y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "    y_pred_test = final_voting_classifier.predict(X_test)\n",
    "    X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1)\n",
    "    y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Rb2RU3mKSGA",
   "metadata": {
    "id": "5Rb2RU3mKSGA"
   },
   "source": [
    "### Simulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oh_V6UXxKSGA",
   "metadata": {
    "id": "oh_V6UXxKSGA"
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "# create a list of random seeds\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(1, 100000) for _ in range(n_sim)]\n",
    "\n",
    "design_list = [5,8,10]\n",
    "patient_n_list = [5468,680,170] # n_patient_per_plan\n",
    "\n",
    "n_design = 8\n",
    "n_patient_per_plan = 680\n",
    "n_rounds = 6\n",
    "total_n = n_patient_per_plan * (2**n_design)\n",
    "\n",
    "## sample size determination\n",
    "beta = 0.2\n",
    "power = 1 - beta\n",
    "alpha = 0.05\n",
    "delta = 0.01 # effect size\n",
    "\n",
    "## early stopping\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sz63YovEKSGA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sz63YovEKSGA",
    "outputId": "83e71af4-bab4-407d-acfc-1d9e8c90ab74"
   },
   "outputs": [],
   "source": [
    "rr_dict = {\"step{}_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "sample_size_dict = {\"step{}_sample_size\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "last_round_sample_size = []\n",
    "random_rr_dict = {\"step{}_random_max_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "random_rr_dict.update({\n",
    "    \"step{}_random_mean_rr\".format(r): [] for r in range(1, n_rounds + 1)\n",
    "})\n",
    "\n",
    "stopping_dict = {\"early_stopping\": [],\n",
    "                 \"early_stopping_plan\":[],\n",
    "                 \"early_stopping_orr\":[],\n",
    "                 \"early_stopping_size\":[]}\n",
    "final_plan_number = []\n",
    "highest_rr_overall = []\n",
    "\n",
    "better_chance = []\n",
    "\n",
    "orr_total = []\n",
    "orr_total_adaptive = []\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for seed in random_seeds:\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "    print([f\"{key}: {len(value)}\" for key, value in rr_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "    event_list = 0\n",
    "    event_list_adaptive = 0\n",
    "    max_rr = []\n",
    "\n",
    "    # step 1:\n",
    "    ## Generate dataset\n",
    "    dt_design_5 = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed)\n",
    "    print(\"empirical response rate\", dt_design_5['response'].mean())\n",
    "    ## save for benchmark results\n",
    "    maxidx = np.argmax(dt_design_5['plan_response_rate'])\n",
    "    random_rr_dict['step1_random_max_rr'].append(dt_design_5.iloc[maxidx,6])\n",
    "    random_rr_dict['step1_random_mean_rr'].append(dt_design_5['plan_response_rate'].mean())\n",
    "\n",
    "    ## Ensemble modelling:\n",
    "    y_pred = ensemble_model_fit(data=dt_design_5, data_pred = dt_design_5)\n",
    "\n",
    "    ## select recruitment plan\n",
    "    pred_df = pd.DataFrame(np.hstack((dt_design_5,  y_pred[:, 1].reshape(-1, 1))),\n",
    "                             columns=list(dt_design_5.columns) + ['predicted_response_rate'])\n",
    "    pred_df_rr = pred_df.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "    x = pred_df_rr['predicted_response_rate'].values\n",
    "    if len(x) <= 10:\n",
    "        best_k = 2\n",
    "    else:\n",
    "        best_k = kmeans_fit(data = x)['best_k']\n",
    "    kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "    merged_df = pd.merge(pred_df_rr, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "    cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "    highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "    highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "    p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "    highest_cluster['p_vec'] = p_vec_next\n",
    "    highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "    # highest_cluster = pred_df_rr\n",
    "\n",
    "    # highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "    # highest_cluster['cluster_number'] = highest_cluster['recruitment_plan']\n",
    "\n",
    "    ## prepare to chance of better performance:\n",
    "    temp = dt_design_5[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "    event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(\"step1\", np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(temp)\n",
    "    p0 = (temp['plan_response_rate'].mean())\n",
    "\n",
    "    rr_dict[\"step1_rr\"].append(dt_design_5['plan_response_rate'].mean())\n",
    "    sample_size_dict[\"step1_sample_size\"].append(len(dt_design_5))\n",
    "\n",
    "    for round in range(2, n_rounds+1):\n",
    "\n",
    "        rr_dict[\"step\"+str(round)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "        plan_number_dict[\"step\"+str(round)+\"_plan_number\"].append(len(p_vec_next))\n",
    "\n",
    "        ## when it comes to the last round:\n",
    "        if round == n_rounds:\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            if round == 2: # if the last round is round 2\n",
    "                remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "            else:\n",
    "                remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "            print(\"remaining size for last Round \" + str(round) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            ## data generation, combine previous data:\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                          design_number = n_design, n_rounds = n_rounds,\n",
    "                                                          n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(0)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(len(temp), temp['plan_response_rate'].mean())\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        ## If haven't reached the last round:\n",
    "        ## check remaining sample size:\n",
    "        if round == 2:\n",
    "            remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "        else:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "        print(\"remaining size for Round \" + str(round) + \": \" + str(remaining_size))\n",
    "\n",
    "        ## determine whether move on to step 2:\n",
    "        if len(highest_cluster) == 1: # if there is only one plan left\n",
    "\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size) # record the last round sample size\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(1)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        ## sample size determination:\n",
    "        if round == 2:\n",
    "            dt_design_5_step2up_overall = dt_design_5\n",
    "        orr_1 = dt_design_5_step2up_overall['response'].mean() # observed overall response rates for previous rounds\n",
    "        orr_2 = orr_1 + delta\n",
    "        n_1 = len(dt_design_5_step2up_overall)\n",
    "\n",
    "        size_step2up = sample_size_calc(orr_1, n_1, delta=delta, alpha=alpha, power=power) # total size for dataset\n",
    "        if size_step2up > 0 and size_step2up < 1000:\n",
    "            size_step2up = 1000 # if size in [0,1000], then it is 1000 for this round.\n",
    "        elif size_step2up >= 1000:\n",
    "            size_step2up = min(size_step2up, int(total_n/n_rounds)) # dataset size capped by the n_patient_per_plan\n",
    "        else:\n",
    "        # if size_step2up <= 0:\n",
    "            # the process stops at this step\n",
    "            print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up) + 'lt 0, break')\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(1)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up))\n",
    "\n",
    "        sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(size_step2up)\n",
    "\n",
    "        ## save benchmark results for last round:\n",
    "        dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                            n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "        maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "        ## data generation, combine previous data:\n",
    "        dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                  design_number = n_design, n_rounds = n_rounds,\n",
    "                                                  n_patient_per_plan = n_patient_per_plan, size = int(size_step2up), seed=seed)\n",
    "\n",
    "        plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "        if round == 2:\n",
    "            supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)]\n",
    "        else:\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "        dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "        dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "        ## Ensemble model fitting:\n",
    "        dt_design_5_step2up.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "        y_pred2up = ensemble_model_fit(data = dt_design_5_step2up_overall, data_pred = dt_design_5_step2up)\n",
    "        pred_df_step2up = pd.DataFrame(np.hstack((dt_design_5_step2up,  y_pred2up[:, 1].reshape(-1, 1))),\n",
    "                                    columns=list(dt_design_5_step2up.columns) + ['predicted_response_rate'])\n",
    "        pred_df_rr_step2up = pred_df_step2up.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "        ## select recruitment plans:\n",
    "        x = pred_df_rr_step2up['predicted_response_rate'].values\n",
    "\n",
    "        if len(x) <= 10:\n",
    "            best_k = 2\n",
    "        else:\n",
    "            best_k = kmeans_fit(data = x)['best_k']\n",
    "        kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "        ## match the cluster results back to the original data\n",
    "        highest_cluster_previous = highest_cluster # save previous cluster results\n",
    "\n",
    "        merged_df = pd.merge(pred_df_rr_step2up, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "        cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "        highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "        highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "        # p_vec_previous = p_vec_next # save p_vec of previous round\n",
    "        p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "\n",
    "        highest_cluster['p_vec'] = p_vec_next\n",
    "\n",
    "        highest_cluster = pd.merge(highest_cluster, dt_design_5_step2up[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "        ## prepare to chance of better performance:\n",
    "        temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "        event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(temp)\n",
    "\n",
    "        ## check early stopping predicted ORR:\n",
    "        orr_df = pd.merge(highest_cluster_previous, highest_cluster[['recruitment_plan','predicted_response_rate','p_vec']], on='recruitment_plan', how='left')\n",
    "        orr_df.fillna(0, inplace=True)\n",
    "        p_orr_1 = np.dot(np.array(orr_df['p_vec_x']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        p_orr_2 = np.dot(np.array(orr_df['p_vec_y']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        print(\"orr termination\", p_orr_1, p_orr_2, p_orr_2 - p_orr_1)\n",
    "\n",
    "        if (p_orr_2 - p_orr_1 < epsilon):\n",
    "            # step 3 use the same strategy of step2\n",
    "            print(i, p_orr_1, p_orr_2, \"early stop at Round \" + str(round))\n",
    "            rr_dict[\"step\"+str(round+1)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                                n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            ### update remaining size:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "            print(\"early stop, remaining size for Round\" + str(round + 1) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(1)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round+1), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TysTqb-hKSGB",
   "metadata": {
    "id": "TysTqb-hKSGB"
   },
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3rE5NPrzKSGB",
   "metadata": {
    "id": "3rE5NPrzKSGB"
   },
   "outputs": [],
   "source": [
    "rr_df = pd.DataFrame(rr_dict)\n",
    "rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51GW3qKgLfRz",
   "metadata": {
    "id": "51GW3qKgLfRz"
   },
   "outputs": [],
   "source": [
    "# Calculate average rounds:\n",
    "row_non_nan_counts = rr_df.count(axis=1)\n",
    "\n",
    "print(f\"{np.mean(row_non_nan_counts):.1f} ({np.std(row_non_nan_counts):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7BpBmgNvKSGB",
   "metadata": {
    "id": "7BpBmgNvKSGB"
   },
   "outputs": [],
   "source": [
    "# orr for each round:\n",
    "result_dict = {}\n",
    "for key, values in rr_df.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DeeO4i9AKSGB",
   "metadata": {
    "id": "DeeO4i9AKSGB"
   },
   "outputs": [],
   "source": [
    "# overall orr:\n",
    "result_dict = np.array(orr_total) / total_n\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f}, StD: {std_result:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N_ACdIs4KSGB",
   "metadata": {
    "id": "N_ACdIs4KSGB"
   },
   "outputs": [],
   "source": [
    "# highest true rr:\n",
    "print(np.mean(highest_true_rr))\n",
    "print(np.std(highest_true_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "azjdUixVKSGB",
   "metadata": {
    "id": "azjdUixVKSGB"
   },
   "outputs": [],
   "source": [
    "# adaptive learning orr:\n",
    "result_dict = np.array(orr_total_adaptive) / (145152)\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f}, StD: {std_result:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xe_GmWf7KSGB",
   "metadata": {
    "id": "xe_GmWf7KSGB"
   },
   "outputs": [],
   "source": [
    "# average plan number for each round:\n",
    "result_dict = {}\n",
    "for key, values in plan_number_dict.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.1f} ({std_value:.1f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMmvSSjvKSGB",
   "metadata": {
    "id": "fMmvSSjvKSGB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# early stopping probabilities:\n",
    "means = {key: np.mean(values) for key, values in stopping_dict.items()}\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oix1Ryk3KSGB",
   "metadata": {
    "id": "oix1Ryk3KSGB"
   },
   "outputs": [],
   "source": [
    "# sample size for the last round:\n",
    "print(np.mean(last_round_sample_size), np.std(last_round_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QGhgm_aVKSGB",
   "metadata": {
    "id": "QGhgm_aVKSGB"
   },
   "outputs": [],
   "source": [
    "# plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "result_dict = {}\n",
    "for key, values in sample_size_dict.items():\n",
    "    mean_value = np.nanmean(values)\n",
    "    std_value = np.nanstd(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5YVfZ2WrL_ug",
   "metadata": {
    "id": "5YVfZ2WrL_ug"
   },
   "outputs": [],
   "source": [
    "# probability of better performance compared to the benchmark:\n",
    "np.mean(better_chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NHSqkmgoMEov",
   "metadata": {
    "id": "NHSqkmgoMEov"
   },
   "source": [
    "## Ensemble learning - 3 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jtIwqgVgMEo6",
   "metadata": {
    "id": "jtIwqgVgMEo6"
   },
   "outputs": [],
   "source": [
    "# def ensemble_model_fit(data, data_pred):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         data.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1),\n",
    "#         data['response'],\n",
    "#         test_size=0.2,\n",
    "#         random_state=0\n",
    "#     )\n",
    "\n",
    "#     # Define the VotingClassifier with the individual classifiers\n",
    "#     voting_classifier = ensemble.VotingClassifier(\n",
    "#         estimators=[\n",
    "#             ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0)),\n",
    "# #             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0))\n",
    "#                     # ('SVM', svm.SVC(kernel='linear', C=1.0, random_state=0, probability=True, class_weight='balanced'))\n",
    "#                     ('RF', ensemble.RandomForestClassifier(criterion='gini', random_state=0)),\n",
    "#                     ('XGB', XGBClassifier(learning_rate=0.1, random_state=0))\n",
    "#                    ],\n",
    "#         voting='soft'\n",
    "#     )\n",
    "\n",
    "#     # Define the hyperparameter grid to search\n",
    "#     # Best Hyperparameters: {'LR__C': 0.01, 'RF__n_estimators': 50, 'XGB__n_estimators': 50}\n",
    "#     param_grid = {\n",
    "#         # 'NB__alpha': [0.01, 0.05, 0.1],  # '__' is used to specify hyperparameters for individual classifiers\n",
    "#         'LR__C': [0.01, 0.05, 0.1],\n",
    "#         # 'Ridge__C': [0.01]\n",
    "#         # 'SVM__C': [0.01, 0.05, 0.1]\n",
    "#         'RF__n_estimators': [50, 100, 200],\n",
    "#         'XGB__n_estimators': [50, 100, 200]\n",
    "#     }\n",
    "\n",
    "#     # Create a GridSearchCV object\n",
    "#     # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "#     grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "#     # Perform the grid search on the training data\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Get the best hyperparameters\n",
    "#     best_params = grid_search.best_params_\n",
    "#     # Print the best fitted parameters\n",
    "#     print(\"Best fitted parameters:\")\n",
    "#     print(best_params)\n",
    "\n",
    "#     # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "#     final_voting_classifier = grid_search.best_estimator_\n",
    "#     final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict probabilities instead of binary outcomes on the test set\n",
    "#     y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "#     y_pred_test = final_voting_classifier.predict(X_test)\n",
    "#     X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1)\n",
    "#     y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M9zTZ8-xS8uY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 651981,
     "status": "ok",
     "timestamp": 1707456468497,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "M9zTZ8-xS8uY",
    "outputId": "d3ce2cbb-f041-49ca-cc2d-b6ed270a8891"
   },
   "outputs": [],
   "source": [
    "# ensemble_model_fit(dt_design_5, dt_design_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nf1VuXIFMEo6",
   "metadata": {
    "id": "nf1VuXIFMEo6"
   },
   "outputs": [],
   "source": [
    "def ensemble_model_fit(data, data_pred):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1),\n",
    "        data['response'],\n",
    "        test_size=0.2,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Define the VotingClassifier with the individual classifiers\n",
    "    voting_classifier = ensemble.VotingClassifier(\n",
    "        estimators=[\n",
    "            ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0)),\n",
    "#             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0))\n",
    "                    # ('SVM', svm.SVC(kernel='linear', C=1.0, random_state=0, probability=True, class_weight='balanced'))\n",
    "                    ('RF', ensemble.RandomForestClassifier(criterion='gini', random_state=0)),\n",
    "                    ('XGB', XGBClassifier(learning_rate=0.1, random_state=0))\n",
    "                   ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameter grid to search\n",
    "    # Best Hyperparameters: {'LR__C': 0.01, 'RF__n_estimators': 50, 'XGB__n_estimators': 50}\n",
    "    param_grid = {\n",
    "        # 'NB__alpha': [0.01, 0.05, 0.1],  # '__' is used to specify hyperparameters for individual classifiers\n",
    "        'LR__C': [0.05],\n",
    "        # 'Ridge__C': [0.01]\n",
    "        # 'SVM__C': [0.05]\n",
    "        'RF__n_estimators': [100],\n",
    "        'XGB__n_estimators': [50]\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "    final_voting_classifier = grid_search.best_estimator_\n",
    "    final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities instead of binary outcomes on the test set\n",
    "    y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "    y_pred_test = final_voting_classifier.predict(X_test)\n",
    "    X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size','response'], axis=1)\n",
    "    y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beNEkYR6MEo6",
   "metadata": {
    "id": "beNEkYR6MEo6"
   },
   "source": [
    "### Simulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "xuzbbQ0wMEo6",
   "metadata": {
    "id": "xuzbbQ0wMEo6"
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "# create a list of random seeds\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(1, 100000) for _ in range(n_sim)]\n",
    "\n",
    "design_list = [5,8,10]\n",
    "patient_n_list = [5468,680,170] # n_patient_per_plan\n",
    "\n",
    "n_design = 8\n",
    "n_patient_per_plan = 680\n",
    "n_rounds = 6\n",
    "total_n = n_patient_per_plan * (2**n_design)\n",
    "\n",
    "## sample size determination\n",
    "beta = 0.2\n",
    "power = 1 - beta\n",
    "alpha = 0.05\n",
    "delta = 0.01 # effect size\n",
    "\n",
    "## early stopping\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "AB5nx86KMEo6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4315144,
     "status": "ok",
     "timestamp": 1707460937777,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "AB5nx86KMEo6",
    "outputId": "636624e6-c5db-4e85-b5c1-cc50bda3e2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['step1_rr: 0', 'step2_rr: 0', 'step3_rr: 0', 'step4_rr: 0', 'step5_rr: 0', 'step6_rr: 0']\n",
      "empirical response rate 0.05361587389380531\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4130.852349241689\n",
      "orr termination 0.0073241181471010875 0.09493757566676381 0.08761345751966272\n",
      "remaining size for Round 3: 141022\n",
      "Calculated size for Round 3: 8993.930846918125\n",
      "orr termination 0.04723841589502206 0.06946896867608265 0.022230552781060592\n",
      "remaining size for Round 4: 132029\n",
      "Calculated size for Round 4: 7554.869956547836\n",
      "orr termination 0.03474007104954897 0.06961066160780373 0.034870590558254755\n",
      "remaining size for Round 5: 124475\n",
      "2\n",
      "['step1_rr: 1', 'step2_rr: 1', 'step3_rr: 1', 'step4_rr: 1', 'step5_rr: 1', 'step6_rr: 1']\n",
      "empirical response rate 0.05496404867256637\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4224.492031131622\n",
      "orr termination 0.032206943535494796 0.0784829064058345 0.0462759628703397\n",
      "remaining size for Round 3: 140928\n",
      "Calculated size for Round 3: 5282.357473508496\n",
      "orr termination 0.07076688968240405 0.07407806195343922 0.0033111722710351676\n",
      "remaining size for Round 4: 135646\n",
      "Calculated size for Round 4: 6551.187636697478\n",
      "orr termination 0.02667063275171151 0.08341496352532993 0.056744330773618415\n",
      "remaining size for Round 5: 129095\n",
      "Calculated size for Round 5: 5491.6064362412\n",
      "orr termination 0.012608701929031003 0.08883901761518463 0.07623031568615363\n",
      "remaining size for last Round 6: 123604\n",
      "3\n",
      "['step1_rr: 2', 'step2_rr: 2', 'step3_rr: 2', 'step4_rr: 2', 'step5_rr: 2', 'step6_rr: 2']\n",
      "empirical response rate 0.05530973451327434\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4248.542691505998\n",
      "orr termination 0.0077534271816190655 0.08936499439698387 0.0816115672153648\n",
      "remaining size for Round 3: 140904\n",
      "Calculated size for Round 3: 4756.308861922813\n",
      "orr termination 0.006703240354577641 0.08825312067223877 0.08154988031766112\n",
      "remaining size for Round 4: 136148\n",
      "4\n",
      "['step1_rr: 3', 'step2_rr: 3', 'step3_rr: 3', 'step4_rr: 3', 'step5_rr: 3', 'step6_rr: 3']\n",
      "empirical response rate 0.055240597345132744\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4243.731238049317\n",
      "orr termination 0.007551078920972868 0.09096846362664028 0.08341738470566741\n",
      "remaining size for Round 3: 140909\n",
      "Calculated size for Round 3: 5534.738825119332\n",
      "orr termination 0.047742785544916067 0.07304431453002673 0.025301528985110663\n",
      "remaining size for Round 4: 135375\n",
      "Calculated size for Round 4: 9876.53768919144\n",
      "orr termination 0.05969547146107241 0.06900990289156592 0.009314431430493507\n",
      "remaining size for Round 5: 125499\n",
      "Calculated size for Round 5: 6098.465472005706\n",
      "orr termination 0.033453201439956504 0.07770872460161718 0.044255523161660676\n",
      "remaining size for last Round 6: 119401\n",
      "5\n",
      "['step1_rr: 4', 'step2_rr: 4', 'step3_rr: 4', 'step4_rr: 4', 'step5_rr: 4', 'step6_rr: 4']\n",
      "empirical response rate 0.05454922566371682\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4195.653045699933\n",
      "orr termination 0.028215507674330342 0.07885025578978902 0.05063474811545868\n",
      "remaining size for Round 3: 140957\n",
      "Calculated size for Round 3: 4700.000081890924\n",
      "orr termination 0.05478246430249335 0.08157265282958792 0.02679018852709457\n",
      "remaining size for Round 4: 136257\n",
      "Calculated size for Round 4: 7062.323339068085\n",
      "orr termination 0.009701529890409155 0.0977453758355249 0.08804384594511575\n",
      "remaining size for Round 5: 129195\n",
      "Calculated size for Round 5: 6592.971060328286\n",
      "orr termination 0.018434387499054516 0.07363706971236092 0.0552026822133064\n",
      "remaining size for last Round 6: 122603\n",
      "6\n",
      "['step1_rr: 5', 'step2_rr: 5', 'step3_rr: 5', 'step4_rr: 5', 'step5_rr: 5', 'step6_rr: 5']\n",
      "empirical response rate 0.053961559734513276\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4154.838554440094\n",
      "orr termination 0.059684340779810566 0.07998924343788405 0.020304902658073484\n",
      "remaining size for Round 3: 140998\n",
      "Calculated size for Round 3: 6574.516564790106\n",
      "orr termination 0.04140199849459007 0.08269741123873088 0.04129541274414081\n",
      "remaining size for Round 4: 134424\n",
      "Calculated size for Round 4: 6133.84092723723\n",
      "orr termination 0.01629233220661953 0.09286509648207644 0.07657276427545691\n",
      "remaining size for Round 5: 128291\n",
      "Calculated size for Round 5: 6897.138892188186\n",
      "orr termination 0.016913751398863003 0.08688072963562457 0.06996697823676157\n",
      "remaining size for last Round 6: 121394\n",
      "7\n",
      "['step1_rr: 6', 'step2_rr: 6', 'step3_rr: 6', 'step4_rr: 6', 'step5_rr: 6', 'step6_rr: 6']\n",
      "empirical response rate 0.054272676991150445\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4176.440275580975\n",
      "orr termination 0.0362529199623796 0.08129305150592947 0.04504013154354987\n",
      "remaining size for Round 3: 140976\n",
      "Calculated size for Round 3: 5574.310422479639\n",
      "orr termination 0.007997579375151314 0.10244843290888793 0.09445085353373661\n",
      "remaining size for Round 4: 135402\n",
      "Calculated size for Round 4: 7213.417669028429\n",
      "orr termination 0.05894337984998287 0.080354495035555 0.02141111518557213\n",
      "remaining size for Round 5: 128189\n",
      "Calculated size for Round 5: 10499.610191449412\n",
      "orr termination 0.050919598236935044 0.07681682996063338 0.025897231723698337\n",
      "remaining size for last Round 6: 117690\n",
      "8\n",
      "['step1_rr: 7', 'step2_rr: 7', 'step3_rr: 7', 'step4_rr: 7', 'step5_rr: 7', 'step6_rr: 7']\n",
      "empirical response rate 0.05520602876106195\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4241.325759063171\n",
      "orr termination 0.013285427257064233 0.0924171193471211 0.07913169209005687\n",
      "remaining size for Round 3: 140911\n",
      "Calculated size for Round 3: 5173.949945473243\n",
      "orr termination 0.027850325471072107 0.0967039691070235 0.0688536436359514\n",
      "remaining size for Round 4: 135738\n",
      "Calculated size for Round 4: 11374.928536521355\n",
      "orr termination 0.028900209959475877 0.07841235990026466 0.04951214994078878\n",
      "remaining size for Round 5: 124364\n",
      "Calculated size for Round 5: 7154.180588735466\n",
      "orr termination 0.03697394834680819 0.0750008030975368 0.03802685475072861\n",
      "remaining size for last Round 6: 117210\n",
      "9\n",
      "['step1_rr: 8', 'step2_rr: 8', 'step3_rr: 8', 'step4_rr: 8', 'step5_rr: 8', 'step6_rr: 8']\n",
      "empirical response rate 0.057107300884955754\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4373.872240981363\n",
      "orr termination 0.0124781996447005 0.09886064450431642 0.08638244485961592\n",
      "remaining size for Round 3: 140779\n",
      "Calculated size for Round 3: 9836.346386356105\n",
      "orr termination 0.041215797453652385 0.06893684445937028 0.02772104700571789\n",
      "remaining size for Round 4: 130943\n",
      "Calculated size for Round 4: 6679.800783085395\n",
      "orr termination 0.025349321532317805 0.07160590169830852 0.04625658016599071\n",
      "remaining size for Round 5: 124264\n",
      "10\n",
      "['step1_rr: 9', 'step2_rr: 9', 'step3_rr: 9', 'step4_rr: 9', 'step5_rr: 9', 'step6_rr: 9']\n",
      "empirical response rate 0.05409983407079646\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4164.437666218033\n",
      "orr termination 0.05890528280392124 0.07382174631571274 0.014916463511791503\n",
      "remaining size for Round 3: 140988\n",
      "Calculated size for Round 3: 5518.160103713361\n",
      "orr termination 0.05623133189957852 0.07737797451904774 0.02114664261946922\n",
      "remaining size for Round 4: 135470\n",
      "Calculated size for Round 4: 5618.832957351532\n",
      "orr termination 0.02362676016737999 0.08955272071387574 0.06592596054649574\n",
      "remaining size for Round 5: 129852\n",
      "Calculated size for Round 5: 5817.405716826573\n",
      "orr termination 0.024156036937298103 0.08150862938647546 0.057352592449177356\n",
      "remaining size for last Round 6: 124035\n",
      "11\n",
      "['step1_rr: 10', 'step2_rr: 10', 'step3_rr: 10', 'step4_rr: 10', 'step5_rr: 10', 'step6_rr: 10']\n",
      "empirical response rate 0.05219856194690266\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4032.6818633794564\n",
      "orr termination 0.04126880110545611 0.07661065376201498 0.03534185265655887\n",
      "remaining size for Round 3: 141120\n",
      "Calculated size for Round 3: 5363.187550828039\n",
      "orr termination 0.012902259555545274 0.09111745684118996 0.07821519728564469\n",
      "remaining size for Round 4: 135757\n",
      "Calculated size for Round 4: 6420.971168396018\n",
      "orr termination 0.061336593412371646 0.07861102197784564 0.017274428565473995\n",
      "remaining size for Round 5: 129337\n",
      "Calculated size for Round 5: 9373.89527361448\n",
      "orr termination 0.042653480534800156 0.07358632153013175 0.030932840995331598\n",
      "remaining size for last Round 6: 119964\n",
      "12\n",
      "['step1_rr: 11', 'step2_rr: 11', 'step3_rr: 11', 'step4_rr: 11', 'step5_rr: 11', 'step6_rr: 11']\n",
      "empirical response rate 0.05479120575221239\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4212.472895702519\n",
      "orr termination 0.0478477946268438 0.07733408750156283 0.02948629287471903\n",
      "remaining size for Round 3: 140940\n",
      "Calculated size for Round 3: 5503.2877631214005\n",
      "orr termination 0.04267191432627118 0.0814777737769515 0.038805859450680324\n",
      "remaining size for Round 4: 135437\n",
      "Calculated size for Round 4: 6069.472744272732\n",
      "orr termination 0.007421423977883146 0.09453815986768055 0.0871167358897974\n",
      "remaining size for Round 5: 129368\n",
      "Calculated size for Round 5: 6877.694221285034\n",
      "orr termination 0.04734144364410059 0.07283508443094897 0.025493640786848376\n",
      "remaining size for last Round 6: 122491\n",
      "13\n",
      "['step1_rr: 12', 'step2_rr: 12', 'step3_rr: 12', 'step4_rr: 12', 'step5_rr: 12', 'step6_rr: 12']\n",
      "empirical response rate 0.05399612831858407\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4157.238084390138\n",
      "orr termination 0.0017793154475562142 0.10883616705883514 0.10705685161127892\n",
      "remaining size for Round 3: 140995\n",
      "14\n",
      "['step1_rr: 13', 'step2_rr: 13', 'step3_rr: 13', 'step4_rr: 13', 'step5_rr: 13', 'step6_rr: 13']\n",
      "empirical response rate 0.05537887168141593\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4253.354805576213\n",
      "orr termination 0.043509511456213734 0.08292742250019348 0.03941791104397975\n",
      "remaining size for Round 3: 140899\n",
      "Calculated size for Round 3: 8465.374124413825\n",
      "orr termination 0.020322628369534745 0.08518277561217696 0.06486014724264222\n",
      "remaining size for Round 4: 132434\n",
      "Calculated size for Round 4: 6574.914434034778\n",
      "orr termination 0.04674535375260153 0.06959631869801881 0.02285096494541728\n",
      "remaining size for Round 5: 125860\n",
      "Calculated size for Round 5: 7204.0569749068345\n",
      "orr termination 0.0360780500188631 0.07356156939413457 0.03748351937527147\n",
      "remaining size for last Round 6: 118656\n",
      "15\n",
      "['step1_rr: 14', 'step2_rr: 14', 'step3_rr: 14', 'step4_rr: 14', 'step5_rr: 14', 'step6_rr: 14']\n",
      "empirical response rate 0.055517146017699116\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4262.981015401587\n",
      "orr termination 0.03598199079520334 0.07617371959708169 0.04019172880187835\n",
      "remaining size for Round 3: 140890\n",
      "Calculated size for Round 3: 5007.675007155526\n",
      "orr termination 0.006431005686332937 0.09403489142598502 0.08760388573965208\n",
      "remaining size for Round 4: 135883\n",
      "Calculated size for Round 4: 6351.762395609584\n",
      "orr termination 0.04380941565687174 0.07209111627890058 0.028281700622028838\n",
      "remaining size for Round 5: 129532\n",
      "Calculated size for Round 5: 9853.255638269833\n",
      "orr termination 0.04783137964304206 0.07313877149578975 0.02530739185274769\n",
      "remaining size for last Round 6: 119679\n",
      "16\n",
      "['step1_rr: 15', 'step2_rr: 15', 'step3_rr: 15', 'step4_rr: 15', 'step5_rr: 15', 'step6_rr: 15']\n",
      "empirical response rate 0.053823285398230086\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4145.242088058297\n",
      "orr termination 0.04784834403089595 0.07829992381632758 0.03045157978543163\n",
      "remaining size for Round 3: 141007\n",
      "Calculated size for Round 3: 5845.7289479053\n",
      "orr termination 0.0335199765375481 0.08293804079373321 0.049418064256185106\n",
      "remaining size for Round 4: 135162\n",
      "Calculated size for Round 4: 6411.576386874\n",
      "orr termination 0.019896900629717378 0.09034853783667988 0.07045163720696251\n",
      "remaining size for Round 5: 128751\n",
      "Calculated size for Round 5: 7308.7321754791865\n",
      "orr termination 0.014246580382714143 0.0843986566651285 0.07015207628241435\n",
      "remaining size for last Round 6: 121443\n",
      "17\n",
      "['step1_rr: 16', 'step2_rr: 16', 'step3_rr: 16', 'step4_rr: 16', 'step5_rr: 16', 'step6_rr: 16']\n",
      "empirical response rate 0.05610481194690266\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4303.921878005255\n",
      "orr termination 0.03889637149169624 0.08380099781807736 0.04490462632638112\n",
      "remaining size for Round 3: 140849\n",
      "Calculated size for Round 3: 5979.739214460552\n",
      "orr termination 0.012424891987839137 0.09533241218245225 0.08290752019461312\n",
      "remaining size for Round 4: 134870\n",
      "Calculated size for Round 4: 7057.78676392087\n",
      "orr termination 0.012183460989451398 0.07551722936072727 0.06333376837127587\n",
      "remaining size for Round 5: 127813\n",
      "18\n",
      "['step1_rr: 17', 'step2_rr: 17', 'step3_rr: 17', 'step4_rr: 17', 'step5_rr: 17', 'step6_rr: 17']\n",
      "empirical response rate 0.055724557522123894\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4277.425283765311\n",
      "orr termination 0.04068476955769942 0.08947065486592526 0.04878588530822584\n",
      "remaining size for Round 3: 140875\n",
      "Calculated size for Round 3: 7067.286535640277\n",
      "orr termination 0.02565988276932811 0.08955712543248744 0.06389724266315933\n",
      "remaining size for Round 4: 133808\n",
      "Calculated size for Round 4: 7078.226432121388\n",
      "orr termination 0.010604839267933937 0.08759327938160769 0.07698844011367376\n",
      "remaining size for Round 5: 126730\n",
      "19\n",
      "['step1_rr: 18', 'step2_rr: 18', 'step3_rr: 18', 'step4_rr: 18', 'step5_rr: 18', 'step6_rr: 18']\n",
      "empirical response rate 0.05579369469026549\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4282.241360696144\n",
      "orr termination 0.019309756365184512 0.09534463627952605 0.07603487991434155\n",
      "remaining size for Round 3: 140870\n",
      "Calculated size for Round 3: 6598.851615423816\n",
      "orr termination 0.02229960134096988 0.08583037494820567 0.06353077360723579\n",
      "remaining size for Round 4: 134272\n",
      "Calculated size for Round 4: 8192.395586537526\n",
      "orr termination 0.053422694719668165 0.07202867464330184 0.018605979923633678\n",
      "remaining size for Round 5: 126080\n",
      "Calculated size for Round 5: 7389.891818177458\n",
      "orr termination 0.04604073992801708 0.06875126868670063 0.022710528758683554\n",
      "remaining size for last Round 6: 118691\n",
      "20\n",
      "['step1_rr: 19', 'step2_rr: 19', 'step3_rr: 19', 'step4_rr: 19', 'step5_rr: 19', 'step6_rr: 19']\n",
      "empirical response rate 0.05790237831858407\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4429.448726064868\n",
      "orr termination 0.03814268909926491 0.07992806998352017 0.04178538088425526\n",
      "remaining size for Round 3: 140723\n",
      "Calculated size for Round 3: 5497.673811928546\n",
      "orr termination 0.0162505962732122 0.09404816253500825 0.07779756626179606\n",
      "remaining size for Round 4: 135226\n",
      "Calculated size for Round 4: 6872.127290187362\n",
      "orr termination 0.01691057119379773 0.0823013302566767 0.06539075906287897\n",
      "remaining size for Round 5: 128354\n",
      "Calculated size for Round 5: 8060.217825134889\n",
      "orr termination 0.03474274429204951 0.06952476453830589 0.03478202024625638\n",
      "remaining size for last Round 6: 120294\n",
      "21\n",
      "['step1_rr: 20', 'step2_rr: 20', 'step3_rr: 20', 'step4_rr: 20', 'step5_rr: 20', 'step6_rr: 20']\n",
      "empirical response rate 0.05434181415929203\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4181.242476429084\n",
      "orr termination 0.026987370111760958 0.08311743267409155 0.05613006256233059\n",
      "remaining size for Round 3: 140971\n",
      "Calculated size for Round 3: 6121.266910081627\n",
      "orr termination 0.021231142714676684 0.08972257591864652 0.06849143320396983\n",
      "remaining size for Round 4: 134850\n",
      "Calculated size for Round 4: 7199.114084876703\n",
      "orr termination 0.040452885013887614 0.07346238880955688 0.03300950379566927\n",
      "remaining size for Round 5: 127651\n",
      "Calculated size for Round 5: 7772.440308327505\n",
      "orr termination 0.03761989830518983 0.07595853927571979 0.038338640970529955\n",
      "remaining size for last Round 6: 119879\n",
      "22\n",
      "['step1_rr: 21', 'step2_rr: 21', 'step3_rr: 21', 'step4_rr: 21', 'step5_rr: 21', 'step6_rr: 21']\n",
      "empirical response rate 0.05689988938053098\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4359.388370425205\n",
      "orr termination 0.00228124473697432 0.10765108964151873 0.10536984490454442\n",
      "remaining size for Round 3: 140793\n",
      "Calculated size for Round 3: 5987.8021387549015\n",
      "orr termination 0.03734785999078647 0.07819480821837022 0.040846948227583744\n",
      "remaining size for Round 4: 134806\n",
      "23\n",
      "['step1_rr: 22', 'step2_rr: 22', 'step3_rr: 22', 'step4_rr: 22', 'step5_rr: 22', 'step6_rr: 22']\n",
      "empirical response rate 0.0556208517699115\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4270.202406591104\n",
      "orr termination 0.01712735996266686 0.08356983659856873 0.06644247663590187\n",
      "remaining size for Round 3: 140882\n",
      "Calculated size for Round 3: 5405.331866256339\n",
      "orr termination 0.02260991116868291 0.08291795023098997 0.060308039062307056\n",
      "remaining size for Round 4: 135477\n",
      "Calculated size for Round 4: 8354.490592332751\n",
      "orr termination 0.04900791081778963 0.06723094535505181 0.018223034537262178\n",
      "remaining size for Round 5: 127123\n",
      "Calculated size for Round 5: 6517.123699302535\n",
      "orr termination 0.046395338757104235 0.07346748396213476 0.027072145205030526\n",
      "remaining size for last Round 6: 120606\n",
      "24\n",
      "['step1_rr: 23', 'step2_rr: 23', 'step3_rr: 23', 'step4_rr: 23', 'step5_rr: 23', 'step6_rr: 23']\n",
      "empirical response rate 0.056623340707964605\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4340.0857788710655\n",
      "orr termination 0.020144141304074714 0.09354318027471617 0.07339903897064146\n",
      "remaining size for Round 3: 140812\n",
      "Calculated size for Round 3: 6463.557877104257\n",
      "orr termination 0.03395820622898982 0.08020056734660283 0.04624236111761301\n",
      "remaining size for Round 4: 134349\n",
      "Calculated size for Round 4: 8428.015816290146\n",
      "orr termination 0.010286199623767691 0.08183407095015605 0.07154787132638836\n",
      "remaining size for Round 5: 125921\n",
      "25\n",
      "['step1_rr: 24', 'step2_rr: 24', 'step3_rr: 24', 'step4_rr: 24', 'step5_rr: 24', 'step6_rr: 24']\n",
      "empirical response rate 0.054307245575221236\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4178.841293359456\n",
      "orr termination 0.027420028066172795 0.08869858898765826 0.06127856092148546\n",
      "remaining size for Round 3: 140974\n",
      "Calculated size for Round 3: 6862.081921429262\n",
      "orr termination 0.004217876738674771 0.09512010076460245 0.09090222402592768\n",
      "remaining size for Round 4: 134112\n",
      "26\n",
      "['step1_rr: 25', 'step2_rr: 25', 'step3_rr: 25', 'step4_rr: 25', 'step5_rr: 25', 'step6_rr: 25']\n",
      "empirical response rate 0.05316648230088496\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4099.695007857038\n",
      "orr termination 0.043299988807630696 0.07627832114632874 0.03297833233869804\n",
      "remaining size for Round 3: 141053\n",
      "Calculated size for Round 3: 5119.748608798737\n",
      "orr termination 0.03855255945071251 0.08169248753535806 0.04313992808464555\n",
      "remaining size for Round 4: 135934\n",
      "Calculated size for Round 4: 6053.546268735613\n",
      "orr termination 0.055180477790117116 0.08098280736992705 0.025802329579809935\n",
      "remaining size for Round 5: 129881\n",
      "Calculated size for Round 5: 6659.428134554694\n",
      "orr termination 0.020582328461382332 0.08190991272758488 0.06132758426620255\n",
      "remaining size for last Round 6: 123222\n",
      "27\n",
      "['step1_rr: 26', 'step2_rr: 26', 'step3_rr: 26', 'step4_rr: 26', 'step5_rr: 26', 'step6_rr: 26']\n",
      "empirical response rate 0.05361587389380531\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4130.852349241689\n",
      "orr termination 0.0700212933426196 0.07513994866796457 0.005118655325344973\n",
      "remaining size for Round 3: 141022\n",
      "Calculated size for Round 3: 18923.135720186743\n",
      "orr termination 0.01836259214237396 0.0752061209778279 0.056843528835453946\n",
      "remaining size for Round 4: 122099\n",
      "Calculated size for Round 4: 5318.308420323366\n",
      "orr termination 0.0267728220971179 0.07788375521523758 0.05111093311811968\n",
      "remaining size for Round 5: 116781\n",
      "28\n",
      "['step1_rr: 27', 'step2_rr: 27', 'step3_rr: 27', 'step4_rr: 27', 'step5_rr: 27', 'step6_rr: 27']\n",
      "empirical response rate 0.05887029867256637\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4497.224694190765\n",
      "orr termination 0.05096207045104583 0.08032367541935019 0.02936160496830436\n",
      "remaining size for Round 3: 140655\n",
      "Calculated size for Round 3: 5806.521838093614\n",
      "orr termination 0.03328768790831925 0.08576421662108416 0.05247652871276491\n",
      "remaining size for Round 4: 134849\n",
      "Calculated size for Round 4: 6099.801444955242\n",
      "orr termination 0.00716317177787107 0.09207521910870947 0.0849120473308384\n",
      "remaining size for Round 5: 128750\n",
      "Calculated size for Round 5: 7236.902351667752\n",
      "orr termination 0.03827704733674401 0.07380540897475063 0.03552836163800662\n",
      "remaining size for last Round 6: 121514\n",
      "29\n",
      "['step1_rr: 28', 'step2_rr: 28', 'step3_rr: 28', 'step4_rr: 28', 'step5_rr: 28', 'step6_rr: 28']\n",
      "empirical response rate 0.05579369469026549\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4282.241360696144\n",
      "orr termination 0.02057809760645841 0.08981614015706466 0.06923804255060624\n",
      "remaining size for Round 3: 140870\n",
      "Calculated size for Round 3: 5937.4850989204715\n",
      "orr termination 0.007111491892107711 0.0928203576586156 0.0857088657665079\n",
      "remaining size for Round 4: 134933\n",
      "Calculated size for Round 4: 8351.933663131327\n",
      "orr termination 0.0384583739963304 0.07653079641418563 0.03807242241785523\n",
      "remaining size for Round 5: 126582\n",
      "30\n",
      "['step1_rr: 29', 'step2_rr: 29', 'step3_rr: 29', 'step4_rr: 29', 'step5_rr: 29', 'step6_rr: 29']\n",
      "empirical response rate 0.055689988938053096\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4275.017492939368\n",
      "orr termination 0.01223715077339398 0.09074063362631106 0.07850348285291708\n",
      "remaining size for Round 3: 140877\n",
      "Calculated size for Round 3: 6355.442326533875\n",
      "orr termination 0.026712780216430584 0.07838940457572952 0.05167662435929893\n",
      "remaining size for Round 4: 134522\n",
      "Calculated size for Round 4: 8503.719961223014\n",
      "orr termination 0.03696572433357193 0.07785690477926872 0.04089118044569679\n",
      "remaining size for Round 5: 126019\n",
      "Calculated size for Round 5: 7636.903417131939\n",
      "orr termination 0.03721070542887343 0.07236756315069731 0.03515685772182388\n",
      "remaining size for last Round 6: 118383\n",
      "31\n",
      "['step1_rr: 30', 'step2_rr: 30', 'step3_rr: 30', 'step4_rr: 30', 'step5_rr: 30', 'step6_rr: 30']\n",
      "empirical response rate 0.05593196902654867\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4291.875495534939\n",
      "orr termination 0.05550773158065168 0.07610482036321645 0.02059708878256477\n",
      "remaining size for Round 3: 140861\n",
      "Calculated size for Round 3: 5761.254007613832\n",
      "orr termination 0.016949860168741056 0.08929137015204677 0.07234150998330571\n",
      "remaining size for Round 4: 135100\n",
      "Calculated size for Round 4: 5843.8623600657975\n",
      "orr termination 0.030571859930954313 0.08768429868663534 0.057112438755681026\n",
      "remaining size for Round 5: 129257\n",
      "Calculated size for Round 5: 10036.56845319481\n",
      "orr termination 0.03244144389533441 0.0785182653369402 0.04607682144160579\n",
      "remaining size for last Round 6: 119221\n",
      "32\n",
      "['step1_rr: 31', 'step2_rr: 31', 'step3_rr: 31', 'step4_rr: 31', 'step5_rr: 31', 'step6_rr: 31']\n",
      "empirical response rate 0.05503318584070797\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4229.300841747606\n",
      "orr termination 0.012317250228512731 0.09133781563810173 0.07902056540958899\n",
      "remaining size for Round 3: 140923\n",
      "Calculated size for Round 3: 5437.246388302823\n",
      "orr termination 0.04670059594932682 0.08139375897425954 0.03469316302493272\n",
      "remaining size for Round 4: 135486\n",
      "Calculated size for Round 4: 10546.246199109984\n",
      "orr termination 0.052944087571723676 0.07859525245004934 0.02565116487832566\n",
      "remaining size for Round 5: 124940\n",
      "Calculated size for Round 5: 6631.434217316446\n",
      "orr termination 0.0399837436514488 0.07883513365802816 0.03885139000657936\n",
      "remaining size for last Round 6: 118309\n",
      "33\n",
      "['step1_rr: 32', 'step2_rr: 32', 'step3_rr: 32', 'step4_rr: 32', 'step5_rr: 32', 'step6_rr: 32']\n",
      "empirical response rate 0.05409983407079646\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4164.437666218033\n",
      "orr termination 0.041255306941840084 0.07606673844898393 0.034811431507143845\n",
      "remaining size for Round 3: 140988\n",
      "Calculated size for Round 3: 5323.726497018787\n",
      "orr termination 0.015668426980003397 0.0901873425170541 0.0745189155370507\n",
      "remaining size for Round 4: 135665\n",
      "Calculated size for Round 4: 6411.788152756428\n",
      "orr termination 0.021854808200318977 0.08499555034613901 0.06314074214582004\n",
      "remaining size for Round 5: 129254\n",
      "Calculated size for Round 5: 8854.648508182127\n",
      "orr termination 0.024489059292931572 0.0742886346247293 0.04979957533179773\n",
      "remaining size for last Round 6: 120400\n",
      "34\n",
      "['step1_rr: 33', 'step2_rr: 33', 'step3_rr: 33', 'step4_rr: 33', 'step5_rr: 33', 'step6_rr: 33']\n",
      "empirical response rate 0.0546529314159292\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4202.860561387483\n",
      "orr termination 0.006828277282311335 0.10664750588793756 0.09981922860562623\n",
      "remaining size for Round 3: 140950\n",
      "Calculated size for Round 3: 8166.474901977015\n",
      "orr termination 0.023859504075464152 0.07163592699983702 0.04777642292437287\n",
      "remaining size for Round 4: 132784\n",
      "35\n",
      "['step1_rr: 34', 'step2_rr: 34', 'step3_rr: 34', 'step4_rr: 34', 'step5_rr: 34', 'step6_rr: 34']\n",
      "empirical response rate 0.05596653761061947\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4294.284441919237\n",
      "orr termination 0.05863391576350777 0.08273567150718913 0.024101755743681362\n",
      "remaining size for Round 3: 140858\n",
      "Calculated size for Round 3: 6378.2009755919435\n",
      "orr termination 0.04588898304955388 0.08358997282005276 0.03770098977049888\n",
      "remaining size for Round 4: 134480\n",
      "Calculated size for Round 4: 6396.690318497353\n",
      "orr termination 0.061801395638422386 0.08072363534931064 0.01892223971088825\n",
      "remaining size for Round 5: 128084\n",
      "Calculated size for Round 5: 6805.106631011082\n",
      "orr termination 0.05519033554722365 0.08215473828651113 0.026964402739287477\n",
      "remaining size for last Round 6: 121279\n",
      "36\n",
      "['step1_rr: 35', 'step2_rr: 35', 'step3_rr: 35', 'step4_rr: 35', 'step5_rr: 35', 'step6_rr: 35']\n",
      "empirical response rate 0.05486034292035398\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4217.280054231296\n",
      "orr termination 0.046380677586829566 0.07490569402381003 0.028525016436980466\n",
      "remaining size for Round 3: 140935\n",
      "Calculated size for Round 3: 5356.589253825098\n",
      "orr termination 0.06384056128993103 0.07637364686294995 0.012533085573018923\n",
      "remaining size for Round 4: 135579\n",
      "Calculated size for Round 4: 6043.049626589996\n",
      "orr termination 0.005391299154899268 0.09551223861792357 0.09012093946302431\n",
      "remaining size for Round 5: 129536\n",
      "Calculated size for Round 5: 5741.7347171557385\n",
      "orr termination 0.027151380636556824 0.07871582048864853 0.051564439852091706\n",
      "remaining size for last Round 6: 123795\n",
      "37\n",
      "['step1_rr: 36', 'step2_rr: 36', 'step3_rr: 36', 'step4_rr: 36', 'step5_rr: 36', 'step6_rr: 36']\n",
      "empirical response rate 0.053857853982300884\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4147.640956631858\n",
      "orr termination 0.025774479140692468 0.09196025348943236 0.0661857743487399\n",
      "remaining size for Round 3: 141005\n",
      "Calculated size for Round 3: 7719.888964686578\n",
      "orr termination 0.03584983251647763 0.07886975785150224 0.04301992533502461\n",
      "remaining size for Round 4: 133286\n",
      "Calculated size for Round 4: 7694.15048807883\n",
      "orr termination 0.05394320881037721 0.07521690897422181 0.021273700163844604\n",
      "remaining size for Round 5: 125592\n",
      "Calculated size for Round 5: 7012.929044575532\n",
      "orr termination 0.03165158881638719 0.07460478745596799 0.0429531986395808\n",
      "remaining size for last Round 6: 118580\n",
      "38\n",
      "['step1_rr: 37', 'step2_rr: 37', 'step3_rr: 37', 'step4_rr: 37', 'step5_rr: 37', 'step6_rr: 37']\n",
      "empirical response rate 0.053961559734513276\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4154.838554440094\n",
      "orr termination 0.010858101399502128 0.09671647343756626 0.08585837203806412\n",
      "remaining size for Round 3: 140998\n",
      "Calculated size for Round 3: 10631.157700388661\n",
      "orr termination 0.017332533863850183 0.07127195194127257 0.05393941807742239\n",
      "remaining size for Round 4: 130367\n",
      "39\n",
      "['step1_rr: 38', 'step2_rr: 38', 'step3_rr: 38', 'step4_rr: 38', 'step5_rr: 38', 'step6_rr: 38']\n",
      "empirical response rate 0.05264795353982301\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4063.778972473349\n",
      "orr termination 0.05351751365208109 0.08680527932412199 0.0332877656720409\n",
      "remaining size for Round 3: 141089\n",
      "Calculated size for Round 3: 15978.679510305388\n",
      "orr termination 0.03297439254503381 0.0709283320764603 0.037953939531426485\n",
      "remaining size for Round 4: 125111\n",
      "Calculated size for Round 4: 5223.879590938515\n",
      "orr termination 0.02691841947521692 0.06768572955297002 0.0407673100777531\n",
      "remaining size for Round 5: 119888\n",
      "Calculated size for Round 5: 6031.328467709886\n",
      "orr termination 0.036440488738733866 0.07237794934866486 0.03593746060993099\n",
      "remaining size for last Round 6: 113857\n",
      "40\n",
      "['step1_rr: 39', 'step2_rr: 39', 'step3_rr: 39', 'step4_rr: 39', 'step5_rr: 39', 'step6_rr: 39']\n",
      "empirical response rate 0.054756637168141595\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4210.069564273832\n",
      "orr termination 0.02703914304904432 0.08627554901331405 0.05923640596426973\n",
      "remaining size for Round 3: 140942\n",
      "Calculated size for Round 3: 5297.882249232559\n",
      "orr termination 0.023171216502562597 0.0899156567531695 0.0667444402506069\n",
      "remaining size for Round 4: 135645\n",
      "Calculated size for Round 4: 7741.4719747563895\n",
      "orr termination 0.00887703377759991 0.08695493491388384 0.07807790113628393\n",
      "remaining size for Round 5: 127904\n",
      "41\n",
      "['step1_rr: 40', 'step2_rr: 40', 'step3_rr: 40', 'step4_rr: 40', 'step5_rr: 40', 'step6_rr: 40']\n",
      "empirical response rate 0.0576258296460177\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4410.10788526606\n",
      "orr termination 0.03137129391202432 0.08045362721341719 0.04908233330139287\n",
      "remaining size for Round 3: 140742\n",
      "Calculated size for Round 3: 4826.98216637701\n",
      "orr termination 0.06726417933609222 0.080660059364994 0.013395880028901783\n",
      "remaining size for Round 4: 135916\n",
      "Calculated size for Round 4: 7306.273204708263\n",
      "orr termination 0.02096838055019823 0.0843264602223001 0.06335807967210187\n",
      "remaining size for Round 5: 128610\n",
      "Calculated size for Round 5: 6039.84546005316\n",
      "orr termination 0.0689077141995665 0.0754420330430487 0.006534318843482198\n",
      "remaining size for last Round 6: 122571\n",
      "42\n",
      "['step1_rr: 41', 'step2_rr: 41', 'step3_rr: 41', 'step4_rr: 41', 'step5_rr: 41', 'step6_rr: 41']\n",
      "empirical response rate 0.05409983407079646\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4164.437666218033\n",
      "orr termination 0.014176909616730996 0.09253293752092331 0.07835602790419231\n",
      "remaining size for Round 3: 140988\n",
      "Calculated size for Round 3: 13160.245418413091\n",
      "orr termination 0.035427318408330466 0.07263255451226534 0.03720523610393488\n",
      "remaining size for Round 4: 127828\n",
      "Calculated size for Round 4: 6264.766278542965\n",
      "orr termination 0.03807619693969066 0.07486821430305368 0.036792017363363023\n",
      "remaining size for Round 5: 121564\n",
      "43\n",
      "['step1_rr: 42', 'step2_rr: 42', 'step3_rr: 42', 'step4_rr: 42', 'step5_rr: 42', 'step6_rr: 42']\n",
      "empirical response rate 0.05202571902654867\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4020.7288865665637\n",
      "orr termination 0.016051024987132075 0.08523461473671477 0.06918358974958269\n",
      "remaining size for Round 3: 141132\n",
      "Calculated size for Round 3: 6704.734217973508\n",
      "orr termination 0.05385569165650459 0.07010050245480572 0.016244810798301125\n",
      "remaining size for Round 4: 134428\n",
      "Calculated size for Round 4: 7670.7512752375715\n",
      "orr termination 0.03680897192635327 0.07165816684827547 0.034849194921922205\n",
      "remaining size for Round 5: 126758\n",
      "Calculated size for Round 5: 5950.900202636592\n",
      "orr termination 0.03139044150264915 0.07809004616993823 0.04669960466728908\n",
      "remaining size for last Round 6: 120808\n",
      "44\n",
      "['step1_rr: 43', 'step2_rr: 43', 'step3_rr: 43', 'step4_rr: 43', 'step5_rr: 43', 'step6_rr: 43']\n",
      "empirical response rate 0.05534430309734513\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4250.948665966841\n",
      "orr termination 0.052526546924837395 0.08041168260331424 0.02788513567847685\n",
      "remaining size for Round 3: 140902\n",
      "Calculated size for Round 3: 6795.009422390445\n",
      "orr termination 0.005462038336034393 0.08681707654286508 0.08135503820683068\n",
      "remaining size for Round 4: 134107\n",
      "Calculated size for Round 4: 6318.362297323247\n",
      "orr termination 0.04585392843539904 0.06894473400494894 0.0230908055695499\n",
      "remaining size for Round 5: 127789\n",
      "Calculated size for Round 5: 8419.80713246016\n",
      "orr termination 0.03728657107842038 0.07243899641508296 0.03515242533666258\n",
      "remaining size for last Round 6: 119370\n",
      "45\n",
      "['step1_rr: 44', 'step2_rr: 44', 'step3_rr: 44', 'step4_rr: 44', 'step5_rr: 44', 'step6_rr: 44']\n",
      "empirical response rate 0.05537887168141593\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4253.354805576213\n",
      "orr termination 0.04305490776701619 0.0797873863269264 0.036732478559910214\n",
      "remaining size for Round 3: 140899\n",
      "Calculated size for Round 3: 5717.896786571438\n",
      "orr termination 0.020844805817621197 0.09057162174462888 0.06972681592700768\n",
      "remaining size for Round 4: 135182\n",
      "Calculated size for Round 4: 6521.695240049878\n",
      "orr termination 0.006625253022270393 0.08151004705090052 0.07488479402863013\n",
      "remaining size for Round 5: 128661\n",
      "46\n",
      "['step1_rr: 45', 'step2_rr: 45', 'step3_rr: 45', 'step4_rr: 45', 'step5_rr: 45', 'step6_rr: 45']\n",
      "empirical response rate 0.053961559734513276\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4154.838554440094\n",
      "orr termination 0.005768540201156867 0.09750668224947988 0.09173814204832301\n",
      "remaining size for Round 3: 140998\n",
      "Calculated size for Round 3: 9204.461754842758\n",
      "orr termination 0.037969869650527364 0.07345639255226463 0.03548652290173727\n",
      "remaining size for Round 4: 131794\n",
      "47\n",
      "['step1_rr: 46', 'step2_rr: 46', 'step3_rr: 46', 'step4_rr: 46', 'step5_rr: 46', 'step6_rr: 46']\n",
      "empirical response rate 0.054618362831858405\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4200.457890907699\n",
      "orr termination 0.019415817808820124 0.0948101793590945 0.07539436155027437\n",
      "remaining size for Round 3: 140952\n",
      "Calculated size for Round 3: 8941.168052781664\n",
      "orr termination 0.055552777395080744 0.07137624616251609 0.015823468767435345\n",
      "remaining size for Round 4: 132011\n",
      "Calculated size for Round 4: 7028.286295337492\n",
      "orr termination 0.0472038978595439 0.073982480453051 0.0267785825935071\n",
      "remaining size for Round 5: 124983\n",
      "Calculated size for Round 5: 6108.4650583804205\n",
      "orr termination 0.046290471039202 0.07630679885012451 0.030016327810922505\n",
      "remaining size for last Round 6: 118875\n",
      "48\n",
      "['step1_rr: 47', 'step2_rr: 47', 'step3_rr: 47', 'step4_rr: 47', 'step5_rr: 47', 'step6_rr: 47']\n",
      "empirical response rate 0.055897400442477874\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4289.466714215505\n",
      "orr termination 0.018829952752407373 0.09776807432936319 0.07893812157695582\n",
      "remaining size for Round 3: 140863\n",
      "Calculated size for Round 3: 8538.678561085419\n",
      "orr termination 0.05844391856771885 0.07455581792942502 0.01611189936170617\n",
      "remaining size for Round 4: 132325\n",
      "Calculated size for Round 4: 7442.7249778431615\n",
      "orr termination 0.011102347161843775 0.08043882773313164 0.06933648057128787\n",
      "remaining size for Round 5: 124883\n",
      "49\n",
      "['step1_rr: 48', 'step2_rr: 48', 'step3_rr: 48', 'step4_rr: 48', 'step5_rr: 48', 'step6_rr: 48']\n",
      "empirical response rate 0.05548257743362832\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4260.5742152468165\n",
      "orr termination 0.06321833697282228 0.06973928769655631 0.006520950723734029\n",
      "remaining size for Round 3: 140892\n",
      "Calculated size for Round 3: 15923.848506040878\n",
      "orr termination 0.02434117293766999 0.07326328874742044 0.04892211580975045\n",
      "remaining size for Round 4: 124969\n",
      "Calculated size for Round 4: 5499.493125963357\n",
      "orr termination 0.046865718548655684 0.06998092281791915 0.023115204269263466\n",
      "remaining size for Round 5: 119470\n",
      "Calculated size for Round 5: 6593.994344732843\n",
      "orr termination 0.03557589078042824 0.07046688211362756 0.03489099133319932\n",
      "remaining size for last Round 6: 112877\n",
      "50\n",
      "['step1_rr: 49', 'step2_rr: 49', 'step3_rr: 49', 'step4_rr: 49', 'step5_rr: 49', 'step6_rr: 49']\n",
      "empirical response rate 0.05831720132743363\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4458.479758689224\n",
      "orr termination 0.010123400446202087 0.10240357309190623 0.09228017264570415\n",
      "remaining size for Round 3: 140694\n",
      "Calculated size for Round 3: 11463.931885650072\n",
      "orr termination 0.024050129867868583 0.07517961602539744 0.05112948615752885\n",
      "remaining size for Round 4: 129231\n",
      "51\n",
      "['step1_rr: 50', 'step2_rr: 50', 'step3_rr: 50', 'step4_rr: 50', 'step5_rr: 50', 'step6_rr: 50']\n",
      "empirical response rate 0.05558628318584071\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4267.795111078634\n",
      "orr termination 0.027670565849366082 0.09046564207300131 0.06279507622363523\n",
      "remaining size for Round 3: 140885\n",
      "Calculated size for Round 3: 6357.456785738801\n",
      "orr termination 0.021041408708194924 0.08931005425515816 0.06826864554696324\n",
      "remaining size for Round 4: 134528\n",
      "Calculated size for Round 4: 7942.788672769563\n",
      "orr termination 0.05208999721041323 0.0732968417880499 0.021206844577636677\n",
      "remaining size for Round 5: 126586\n",
      "Calculated size for Round 5: 7363.934341823629\n",
      "orr termination 0.06029323803359171 0.07389970279516422 0.013606464761572512\n",
      "remaining size for last Round 6: 119223\n",
      "52\n",
      "['step1_rr: 51', 'step2_rr: 51', 'step3_rr: 51', 'step4_rr: 51', 'step5_rr: 51', 'step6_rr: 51']\n",
      "empirical response rate 0.05378871681415929\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4142.8433848402565\n",
      "orr termination 0.008175097493045635 0.09614809145972253 0.08797299396667689\n",
      "remaining size for Round 3: 141010\n",
      "Calculated size for Round 3: 6815.959409262051\n",
      "orr termination 0.049836417497258326 0.0736543414051555 0.02381792390789718\n",
      "remaining size for Round 4: 134195\n",
      "Calculated size for Round 4: 8405.825589785829\n",
      "orr termination 0.053305369779996115 0.06863526361619379 0.015329893836197678\n",
      "remaining size for Round 5: 125790\n",
      "Calculated size for Round 5: 5879.878208511263\n",
      "orr termination 0.04810743776909282 0.06963703657149029 0.021529598802397465\n",
      "remaining size for last Round 6: 119911\n",
      "53\n",
      "['step1_rr: 52', 'step2_rr: 52', 'step3_rr: 52', 'step4_rr: 52', 'step5_rr: 52', 'step6_rr: 52']\n",
      "empirical response rate 0.05527516592920354\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4246.136882198541\n",
      "orr termination 0.01051700597408381 0.0897554073009283 0.0792384013268445\n",
      "remaining size for Round 3: 140906\n",
      "Calculated size for Round 3: 5290.203592627813\n",
      "orr termination 0.03531978994055782 0.0804016590124645 0.04508186907190668\n",
      "remaining size for Round 4: 135616\n",
      "Calculated size for Round 4: 9484.85525059642\n",
      "orr termination 0.013444728381876074 0.0798884438108479 0.06644371542897183\n",
      "remaining size for Round 5: 126132\n",
      "54\n",
      "['step1_rr: 53', 'step2_rr: 53', 'step3_rr: 53', 'step4_rr: 53', 'step5_rr: 53', 'step6_rr: 53']\n",
      "empirical response rate 0.054618362831858405\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4200.457890907699\n",
      "orr termination 0.039910807992414025 0.07651569897684043 0.03660489098442641\n",
      "remaining size for Round 3: 140952\n",
      "Calculated size for Round 3: 4969.352495429872\n",
      "orr termination 0.03996079341137203 0.08283659063764937 0.04287579722627734\n",
      "remaining size for Round 4: 135983\n",
      "Calculated size for Round 4: 6301.9628933089225\n",
      "orr termination 0.06155779627001905 0.08136774148946947 0.019809945219450426\n",
      "remaining size for Round 5: 129682\n",
      "Calculated size for Round 5: 6906.746230339462\n",
      "orr termination 0.06038860601957363 0.08188904421744878 0.021500438197875144\n",
      "remaining size for last Round 6: 122776\n",
      "55\n",
      "['step1_rr: 54', 'step2_rr: 54', 'step3_rr: 54', 'step4_rr: 54', 'step5_rr: 54', 'step6_rr: 54']\n",
      "empirical response rate 0.05544800884955752\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4258.167580221075\n",
      "orr termination 0.0552393886111741 0.07317520092450402 0.017935812313329927\n",
      "remaining size for Round 3: 140894\n",
      "Calculated size for Round 3: 5174.484515635066\n",
      "orr termination 0.04641707174527161 0.08240990215510631 0.0359928304098347\n",
      "remaining size for Round 4: 135720\n",
      "Calculated size for Round 4: 5604.756310578155\n",
      "orr termination 0.057835726256205575 0.07790724194320149 0.020071515686995917\n",
      "remaining size for Round 5: 130116\n",
      "Calculated size for Round 5: 6185.374652170999\n",
      "orr termination 0.02005574234357885 0.08906508249238473 0.06900934014880589\n",
      "remaining size for last Round 6: 123931\n",
      "56\n",
      "['step1_rr: 55', 'step2_rr: 55', 'step3_rr: 55', 'step4_rr: 55', 'step5_rr: 55', 'step6_rr: 55']\n",
      "empirical response rate 0.0546529314159292\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4202.860561387483\n",
      "orr termination 0.062301442834645435 0.07734911589028037 0.015047673055634939\n",
      "remaining size for Round 3: 140950\n",
      "Calculated size for Round 3: 6109.26851657284\n",
      "orr termination 0.001681970539391833 0.09989877834894521 0.09821680780955339\n",
      "remaining size for Round 4: 134841\n",
      "57\n",
      "['step1_rr: 56', 'step2_rr: 56', 'step3_rr: 56', 'step4_rr: 56', 'step5_rr: 56', 'step6_rr: 56']\n",
      "empirical response rate 0.05534430309734513\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4250.948665966841\n",
      "orr termination 0.03929145636761473 0.07949267702225704 0.04020122065464231\n",
      "remaining size for Round 3: 140902\n",
      "Calculated size for Round 3: 5681.084112636058\n",
      "orr termination 0.03588934774334298 0.08667633470929823 0.050786986965955246\n",
      "remaining size for Round 4: 135221\n",
      "Calculated size for Round 4: 6539.831113833009\n",
      "orr termination 0.01779579683243568 0.08422754714279497 0.06643175031035929\n",
      "remaining size for Round 5: 128682\n",
      "Calculated size for Round 5: 6852.67270306694\n",
      "orr termination 0.01741943705030081 0.08772065596952032 0.0703012189192195\n",
      "remaining size for last Round 6: 121830\n",
      "58\n",
      "['step1_rr: 57', 'step2_rr: 57', 'step3_rr: 57', 'step4_rr: 57', 'step5_rr: 57', 'step6_rr: 57']\n",
      "empirical response rate 0.0546529314159292\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4202.860561387483\n",
      "orr termination 0.03118151342333216 0.07585034763138654 0.044668834208054384\n",
      "remaining size for Round 3: 140950\n",
      "Calculated size for Round 3: 5205.987756766279\n",
      "orr termination 0.0034983094790477015 0.09172616740991747 0.08822785793086978\n",
      "remaining size for Round 4: 135745\n",
      "Calculated size for Round 4: 6550.348948075516\n",
      "orr termination 0.03775129626057155 0.07448501795659575 0.0367337216960242\n",
      "remaining size for Round 5: 129195\n",
      "59\n",
      "['step1_rr: 58', 'step2_rr: 58', 'step3_rr: 58', 'step4_rr: 58', 'step5_rr: 58', 'step6_rr: 58']\n",
      "empirical response rate 0.05451465707964602\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4193.2508709813565\n",
      "orr termination 0.025705737831661562 0.08965440372754768 0.06394866589588612\n",
      "remaining size for Round 3: 140959\n",
      "Calculated size for Round 3: 9681.72475431255\n",
      "orr termination 0.007779370084195985 0.0886968888206982 0.08091751873650221\n",
      "remaining size for Round 4: 131278\n",
      "60\n",
      "['step1_rr: 59', 'step2_rr: 59', 'step3_rr: 59', 'step4_rr: 59', 'step5_rr: 59', 'step6_rr: 59']\n",
      "empirical response rate 0.05520602876106195\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4241.325759063171\n",
      "orr termination 0.02156720949609836 0.0844338638091332 0.06286665431303484\n",
      "remaining size for Round 3: 140911\n",
      "Calculated size for Round 3: 5278.395060280569\n",
      "orr termination 0.008149451421124585 0.08250617351056116 0.07435672208943658\n",
      "remaining size for Round 4: 135633\n",
      "Calculated size for Round 4: 7564.678485221776\n",
      "orr termination 0.026066172462365197 0.07848608361348167 0.052419911151116466\n",
      "remaining size for Round 5: 128069\n",
      "61\n",
      "['step1_rr: 60', 'step2_rr: 60', 'step3_rr: 60', 'step4_rr: 60', 'step5_rr: 60', 'step6_rr: 60']\n",
      "empirical response rate 0.05295907079646018\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4085.324125976102\n",
      "orr termination 0.06218637675802169 0.0755314105736726 0.013345033815650914\n",
      "remaining size for Round 3: 141067\n",
      "Calculated size for Round 3: 6232.666901723704\n",
      "orr termination 0.05902528916439028 0.07010685274694142 0.011081563582551142\n",
      "remaining size for Round 4: 134835\n",
      "Calculated size for Round 4: 5588.461525219927\n",
      "orr termination 0.01612018220439577 0.08643174921259811 0.07031156700820233\n",
      "remaining size for Round 5: 129247\n",
      "Calculated size for Round 5: 5461.627729105095\n",
      "orr termination 0.06337732894317774 0.0791743360238079 0.015797007080630157\n",
      "remaining size for last Round 6: 123786\n",
      "62\n",
      "['step1_rr: 61', 'step2_rr: 61', 'step3_rr: 61', 'step4_rr: 61', 'step5_rr: 61', 'step6_rr: 61']\n",
      "empirical response rate 0.05330475663716814\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4109.278904649849\n",
      "orr termination 0.009265702815442577 0.09112281808499254 0.08185711526954996\n",
      "remaining size for Round 3: 141043\n",
      "Calculated size for Round 3: 5868.030871882479\n",
      "orr termination 0.03359684539742633 0.08110281047695393 0.047505965079527604\n",
      "remaining size for Round 4: 135175\n",
      "Calculated size for Round 4: 10337.41891002005\n",
      "orr termination 0.0356674371932669 0.07398547751618587 0.03831804032291897\n",
      "remaining size for Round 5: 124838\n",
      "Calculated size for Round 5: 6715.1505488747525\n",
      "orr termination 0.03782864713997412 0.07553881751803694 0.03771017037806282\n",
      "remaining size for last Round 6: 118123\n",
      "63\n",
      "['step1_rr: 62', 'step2_rr: 62', 'step3_rr: 62', 'step4_rr: 62', 'step5_rr: 62', 'step6_rr: 62']\n",
      "empirical response rate 0.053477599557522126\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4121.262497623935\n",
      "orr termination 0.02099335306498902 0.08940012730386468 0.06840677423887566\n",
      "remaining size for Round 3: 141031\n",
      "Calculated size for Round 3: 6880.043128842024\n",
      "orr termination 0.047898745293553244 0.07501664499095843 0.02711789969740519\n",
      "remaining size for Round 4: 134151\n",
      "Calculated size for Round 4: 7869.070880480174\n",
      "orr termination 0.05397090637284195 0.0724285492908264 0.01845764291798445\n",
      "remaining size for Round 5: 126282\n",
      "Calculated size for Round 5: 6224.546931445199\n",
      "orr termination 0.039992850232883584 0.07692617435212872 0.03693332411924513\n",
      "remaining size for last Round 6: 120058\n",
      "64\n",
      "['step1_rr: 63', 'step2_rr: 63', 'step3_rr: 63', 'step4_rr: 63', 'step5_rr: 63', 'step6_rr: 63']\n",
      "empirical response rate 0.05458379424778761\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4198.0553856769475\n",
      "orr termination 0.03453377813178478 0.09166411759980048 0.0571303394680157\n",
      "remaining size for Round 3: 140954\n",
      "Calculated size for Round 3: 7340.827010500727\n",
      "orr termination 0.011069623394733583 0.08834649677778933 0.07727687338305575\n",
      "remaining size for Round 4: 133614\n",
      "Calculated size for Round 4: 7399.4420341716095\n",
      "orr termination 0.047551362474512096 0.07115114965374952 0.023599787179237428\n",
      "remaining size for Round 5: 126215\n",
      "Calculated size for Round 5: 8541.652449024808\n",
      "orr termination 0.03353926288212001 0.06784567296690737 0.03430641008478736\n",
      "remaining size for last Round 6: 117674\n",
      "65\n",
      "['step1_rr: 64', 'step2_rr: 64', 'step3_rr: 64', 'step4_rr: 64', 'step5_rr: 64', 'step6_rr: 64']\n",
      "empirical response rate 0.05406526548672566\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4162.037640283698\n",
      "orr termination 0.014429181626200018 0.0944595958531177 0.08003041422691767\n",
      "remaining size for Round 3: 140990\n",
      "Calculated size for Round 3: 7125.428246244944\n",
      "orr termination 0.05212257796246673 0.07644570658549119 0.024323128623024458\n",
      "remaining size for Round 4: 133865\n",
      "Calculated size for Round 4: 8658.342774679764\n",
      "orr termination 0.05103587342421452 0.07246154409647818 0.021425670672263655\n",
      "remaining size for Round 5: 125207\n",
      "Calculated size for Round 5: 6406.783062726574\n",
      "orr termination 0.015446155098573595 0.07641360496827052 0.06096744986969692\n",
      "remaining size for last Round 6: 118801\n",
      "66\n",
      "['step1_rr: 65', 'step2_rr: 65', 'step3_rr: 65', 'step4_rr: 65', 'step5_rr: 65', 'step6_rr: 65']\n",
      "empirical response rate 0.0547220685840708\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4207.666398075275\n",
      "orr termination 0.06422668795865455 0.07627861024954682 0.012051922290892275\n",
      "remaining size for Round 3: 140945\n",
      "Calculated size for Round 3: 18151.579444696166\n",
      "orr termination 0.0371400175475413 0.07459211571242269 0.03745209816488139\n",
      "remaining size for Round 4: 122794\n",
      "Calculated size for Round 4: 5276.7318540342285\n",
      "orr termination 0.0419336416512297 0.07105542815652563 0.029121786505295927\n",
      "remaining size for Round 5: 117518\n",
      "Calculated size for Round 5: 6035.514565423721\n",
      "orr termination 0.048900705077574935 0.07299190514379514 0.024091200066220203\n",
      "remaining size for last Round 6: 111483\n",
      "67\n",
      "['step1_rr: 66', 'step2_rr: 66', 'step3_rr: 66', 'step4_rr: 66', 'step5_rr: 66', 'step6_rr: 66']\n",
      "empirical response rate 0.05620851769911504\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4311.151688005828\n",
      "orr termination 0.026103659757816887 0.08273644065828084 0.056632780900463954\n",
      "remaining size for Round 3: 140841\n",
      "Calculated size for Round 3: 5010.919525107623\n",
      "orr termination 0.012468359277602323 0.09855775216768851 0.08608939289008619\n",
      "remaining size for Round 4: 135831\n",
      "Calculated size for Round 4: 7637.089006748075\n",
      "orr termination 0.0596800288583692 0.07243567317141496 0.01275564431304576\n",
      "remaining size for Round 5: 128194\n",
      "Calculated size for Round 5: 8172.916281633587\n",
      "orr termination 0.018565122865334228 0.0777909987882474 0.059225875922913174\n",
      "remaining size for last Round 6: 120022\n",
      "68\n",
      "['step1_rr: 67', 'step2_rr: 67', 'step3_rr: 67', 'step4_rr: 67', 'step5_rr: 67', 'step6_rr: 67']\n",
      "empirical response rate 0.05503318584070797\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4229.300841747606\n",
      "orr termination 0.057360978867017874 0.08002933669189935 0.022668357824881476\n",
      "remaining size for Round 3: 140923\n",
      "Calculated size for Round 3: 6515.296136715039\n",
      "orr termination 0.058178900843956896 0.07583825413648743 0.01765935329253053\n",
      "remaining size for Round 4: 134408\n",
      "Calculated size for Round 4: 6009.438023266727\n",
      "orr termination 0.001919695427418582 0.08774748309312068 0.0858277876657021\n",
      "remaining size for Round 5: 128399\n",
      "69\n",
      "['step1_rr: 68', 'step2_rr: 68', 'step3_rr: 68', 'step4_rr: 68', 'step5_rr: 68', 'step6_rr: 68']\n",
      "empirical response rate 0.05406526548672566\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4162.037640283698\n",
      "orr termination 0.050900475439948314 0.07300840665208604 0.02210793121213772\n",
      "remaining size for Round 3: 140990\n",
      "Calculated size for Round 3: 5084.242009315375\n",
      "orr termination 0.007813859784992848 0.09533247116962698 0.08751861138463413\n",
      "remaining size for Round 4: 135906\n",
      "Calculated size for Round 4: 5673.6075619967505\n",
      "orr termination 0.06468565598073302 0.0754777955620414 0.010792139581308383\n",
      "remaining size for Round 5: 130233\n",
      "Calculated size for Round 5: 10736.329924509484\n",
      "orr termination 0.03624556904907466 0.07232718262891341 0.03608161357983875\n",
      "remaining size for last Round 6: 119497\n",
      "70\n",
      "['step1_rr: 69', 'step2_rr: 69', 'step3_rr: 69', 'step4_rr: 69', 'step5_rr: 69', 'step6_rr: 69']\n",
      "empirical response rate 0.053477599557522126\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4121.262497623935\n",
      "orr termination 0.05377509072914304 0.07806903825028655 0.024293947521143507\n",
      "remaining size for Round 3: 141031\n",
      "Calculated size for Round 3: 17619.50538412818\n",
      "orr termination 0.01313940934697528 0.07507251557438076 0.06193310622740548\n",
      "remaining size for Round 4: 123412\n",
      "Calculated size for Round 4: 5622.541739979106\n",
      "orr termination 0.03770628782363917 0.07476576096386309 0.03705947314022392\n",
      "remaining size for Round 5: 117790\n",
      "71\n",
      "['step1_rr: 70', 'step2_rr: 70', 'step3_rr: 70', 'step4_rr: 70', 'step5_rr: 70', 'step6_rr: 70']\n",
      "empirical response rate 0.05250967920353982\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4054.207651666048\n",
      "orr termination 0.06289634944972486 0.07914225576595471 0.016245906316229852\n",
      "remaining size for Round 3: 141098\n",
      "Calculated size for Round 3: 16078.851041198295\n",
      "orr termination 0.021651937744299084 0.0797159910205546 0.058064053276255516\n",
      "remaining size for Round 4: 125020\n",
      "Calculated size for Round 4: 5776.1156393971205\n",
      "orr termination 0.019217847478650196 0.0759998460355471 0.056781998556896904\n",
      "remaining size for Round 5: 119244\n",
      "72\n",
      "['step1_rr: 71', 'step2_rr: 71', 'step3_rr: 71', 'step4_rr: 71', 'step5_rr: 71', 'step6_rr: 71']\n",
      "empirical response rate 0.05420353982300885\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4171.638735916014\n",
      "orr termination 0.009364621036035836 0.09840060119301201 0.08903598015697618\n",
      "remaining size for Round 3: 140981\n",
      "Calculated size for Round 3: 6862.027297029591\n",
      "orr termination 0.013608627068440501 0.0833340206709505 0.06972539360251\n",
      "remaining size for Round 4: 134119\n",
      "73\n",
      "['step1_rr: 72', 'step2_rr: 72', 'step3_rr: 72', 'step4_rr: 72', 'step5_rr: 72', 'step6_rr: 72']\n",
      "empirical response rate 0.056485066371681415\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4330.438442060034\n",
      "orr termination 0.035388225099620634 0.08657968027789338 0.05119145517827275\n",
      "remaining size for Round 3: 140822\n",
      "Calculated size for Round 3: 6955.906991083382\n",
      "orr termination 0.014987753433826332 0.09124348930949507 0.07625573587566874\n",
      "remaining size for Round 4: 133867\n",
      "Calculated size for Round 4: 7156.149075980662\n",
      "orr termination 0.04738261491778183 0.07908398262021676 0.03170136770243493\n",
      "remaining size for Round 5: 126711\n",
      "Calculated size for Round 5: 8423.596757609559\n",
      "orr termination 0.04927204681603557 0.07245690087766823 0.023184854061632662\n",
      "remaining size for last Round 6: 118288\n",
      "74\n",
      "['step1_rr: 73', 'step2_rr: 73', 'step3_rr: 73', 'step4_rr: 73', 'step5_rr: 73', 'step6_rr: 73']\n",
      "empirical response rate 0.05333932522123894\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4111.675292416549\n",
      "orr termination 0.020247094541696545 0.08705549856948114 0.0668084040277846\n",
      "remaining size for Round 3: 141041\n",
      "Calculated size for Round 3: 5318.2680367975445\n",
      "orr termination 0.05769177174019098 0.08681599092842306 0.029124219188232076\n",
      "remaining size for Round 4: 135723\n",
      "Calculated size for Round 4: 9551.696453629867\n",
      "orr termination 0.0651462740197591 0.0757833992083045 0.010637125188545399\n",
      "remaining size for Round 5: 126172\n",
      "Calculated size for Round 5: 6367.440491789687\n",
      "orr termination 0.020820537067937912 0.08212985815688248 0.06130932108894457\n",
      "remaining size for last Round 6: 119805\n",
      "75\n",
      "['step1_rr: 74', 'step2_rr: 74', 'step3_rr: 74', 'step4_rr: 74', 'step5_rr: 74', 'step6_rr: 74']\n",
      "empirical response rate 0.055413440265486724\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4255.761110329247\n",
      "orr termination 0.008788690194901007 0.09315647882320488 0.08436778862830388\n",
      "remaining size for Round 3: 140897\n",
      "Calculated size for Round 3: 5779.3637096350085\n",
      "orr termination 0.016950356639992097 0.07510720462320883 0.05815684798321673\n",
      "remaining size for Round 4: 135118\n",
      "Calculated size for Round 4: 9262.135054460083\n",
      "orr termination 0.03540784211836523 0.06689069994515066 0.031482857826785435\n",
      "remaining size for Round 5: 125856\n",
      "76\n",
      "['step1_rr: 75', 'step2_rr: 75', 'step3_rr: 75', 'step4_rr: 75', 'step5_rr: 75', 'step6_rr: 75']\n",
      "empirical response rate 0.053685011061946904\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4135.6482673646715\n",
      "orr termination 0.06319663163931086 0.0669131041475835 0.0037164725082726446\n",
      "remaining size for Round 3: 141017\n",
      "Calculated size for Round 3: 4922.921983924144\n",
      "orr termination 0.02513575494681712 0.08078886401460825 0.055653109067791126\n",
      "remaining size for Round 4: 136095\n",
      "Calculated size for Round 4: 4872.7285495091255\n",
      "orr termination 0.04291655292065723 0.0848068424683727 0.04189028954771547\n",
      "remaining size for Round 5: 131223\n",
      "Calculated size for Round 5: 7527.666782570326\n",
      "orr termination 0.017417157126946546 0.09081667568643971 0.07339951855949317\n",
      "remaining size for last Round 6: 123696\n",
      "77\n",
      "['step1_rr: 76', 'step2_rr: 76', 'step3_rr: 76', 'step4_rr: 76', 'step5_rr: 76', 'step6_rr: 76']\n",
      "empirical response rate 0.053961559734513276\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4154.838554440094\n",
      "orr termination 0.027885018146133346 0.07990304032061431 0.05201802217448096\n",
      "remaining size for Round 3: 140998\n",
      "Calculated size for Round 3: 5077.418823754934\n",
      "orr termination 0.012424688252413281 0.09771767246184743 0.08529298420943415\n",
      "remaining size for Round 4: 135921\n",
      "Calculated size for Round 4: 7484.033928098538\n",
      "orr termination 0.037721805602089035 0.07678018041209872 0.039058374810009684\n",
      "remaining size for Round 5: 128437\n",
      "Calculated size for Round 5: 8412.994602874709\n",
      "orr termination 0.04979903806255738 0.0734826471075187 0.02368360904496132\n",
      "remaining size for last Round 6: 120025\n",
      "78\n",
      "['step1_rr: 77', 'step2_rr: 77', 'step3_rr: 77', 'step4_rr: 77', 'step5_rr: 77', 'step6_rr: 77']\n",
      "empirical response rate 0.053823285398230086\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4145.242088058297\n",
      "orr termination 0.0057026208259473065 0.09758964421522105 0.09188702338927375\n",
      "remaining size for Round 3: 141007\n",
      "Calculated size for Round 3: 5390.0158854297515\n",
      "orr termination 0.050962273937332714 0.07537005344673675 0.024407779509404037\n",
      "remaining size for Round 4: 135617\n",
      "Calculated size for Round 4: 12720.17458496338\n",
      "orr termination 0.05373938465515442 0.07209244563806311 0.01835306098290869\n",
      "remaining size for Round 5: 122897\n",
      "Calculated size for Round 5: 6031.23149366737\n",
      "orr termination 0.02659865156541038 0.07551245428910831 0.048913802723697934\n",
      "remaining size for last Round 6: 116866\n",
      "79\n",
      "['step1_rr: 78', 'step2_rr: 78', 'step3_rr: 78', 'step4_rr: 78', 'step5_rr: 78', 'step6_rr: 78']\n",
      "empirical response rate 0.05482577433628318\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4214.876392356589\n",
      "orr termination 0.016790049803948163 0.09270090220816518 0.07591085240421702\n",
      "remaining size for Round 3: 140938\n",
      "Calculated size for Round 3: 6222.638864573393\n",
      "orr termination 0.02887150839000218 0.07901993327159793 0.050148424881595745\n",
      "remaining size for Round 4: 134716\n",
      "Calculated size for Round 4: 8708.727604614696\n",
      "orr termination 0.047663828950524456 0.06960620496716903 0.021942376016644574\n",
      "remaining size for Round 5: 126008\n",
      "Calculated size for Round 5: 6499.975464727905\n",
      "orr termination 0.033738190344873166 0.06813415231489582 0.034395961970022655\n",
      "remaining size for last Round 6: 119509\n",
      "80\n",
      "['step1_rr: 79', 'step2_rr: 79', 'step3_rr: 79', 'step4_rr: 79', 'step5_rr: 79', 'step6_rr: 79']\n",
      "empirical response rate 0.05676161504424779\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4349.735755102456\n",
      "orr termination 0.026414606350044407 0.07412434074593036 0.04770973439588595\n",
      "remaining size for Round 3: 140803\n",
      "Calculated size for Round 3: 4450.925145705572\n",
      "orr termination 0.02099626901271786 0.08676894940208515 0.06577268038936729\n",
      "remaining size for Round 4: 136353\n",
      "Calculated size for Round 4: 6663.65584173942\n",
      "orr termination 0.022433052976571125 0.08711563974987084 0.06468258677329972\n",
      "remaining size for Round 5: 129690\n",
      "Calculated size for Round 5: 8101.705243447238\n",
      "orr termination 0.05672630152207585 0.07653785957308876 0.01981155805101291\n",
      "remaining size for last Round 6: 121589\n",
      "81\n",
      "['step1_rr: 80', 'step2_rr: 80', 'step3_rr: 80', 'step4_rr: 80', 'step5_rr: 80', 'step6_rr: 80']\n",
      "empirical response rate 0.05548257743362832\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4260.5742152468165\n",
      "orr termination 0.03159885174258102 0.0860496398218016 0.054450788079220575\n",
      "remaining size for Round 3: 140892\n",
      "Calculated size for Round 3: 6431.4413103940005\n",
      "orr termination 0.014261390643173312 0.08519336325177074 0.07093197260859743\n",
      "remaining size for Round 4: 134461\n",
      "Calculated size for Round 4: 7120.281242384245\n",
      "orr termination 0.045040544063094455 0.0756233952215793 0.03058285115848485\n",
      "remaining size for Round 5: 127341\n",
      "Calculated size for Round 5: 8254.832812936846\n",
      "orr termination 0.04776832210078731 0.07041814394712559 0.022649821846338278\n",
      "remaining size for last Round 6: 119087\n",
      "82\n",
      "['step1_rr: 81', 'step2_rr: 81', 'step3_rr: 81', 'step4_rr: 81', 'step5_rr: 81', 'step6_rr: 81']\n",
      "empirical response rate 0.05333932522123894\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4111.675292416549\n",
      "orr termination 0.004254521312307882 0.10242226302152524 0.09816774170921735\n",
      "remaining size for Round 3: 141041\n",
      "Calculated size for Round 3: 5643.596142795413\n",
      "orr termination 0.034917355725679375 0.07000305181250957 0.035085696086830195\n",
      "remaining size for Round 4: 135398\n",
      "Calculated size for Round 4: 11821.499372822056\n",
      "orr termination 0.033506395662932106 0.06724085944847881 0.0337344637855467\n",
      "remaining size for Round 5: 123577\n",
      "83\n",
      "['step1_rr: 82', 'step2_rr: 82', 'step3_rr: 82', 'step4_rr: 82', 'step5_rr: 82', 'step6_rr: 82']\n",
      "empirical response rate 0.05451465707964602\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4193.2508709813565\n",
      "orr termination 0.0317358443398861 0.08172614368170583 0.049990299341819734\n",
      "remaining size for Round 3: 140959\n",
      "Calculated size for Round 3: 5816.847396729656\n",
      "orr termination 0.007293519950593436 0.09713078501191438 0.08983726506132095\n",
      "remaining size for Round 4: 135143\n",
      "Calculated size for Round 4: 7488.822397188862\n",
      "orr termination 0.048731726161027555 0.07497087338695889 0.026239147225931334\n",
      "remaining size for Round 5: 127655\n",
      "Calculated size for Round 5: 8595.66774925519\n",
      "orr termination 0.036565845128139186 0.07188154583580497 0.035315700707665786\n",
      "remaining size for last Round 6: 119060\n",
      "84\n",
      "['step1_rr: 83', 'step2_rr: 83', 'step3_rr: 83', 'step4_rr: 83', 'step5_rr: 83', 'step6_rr: 83']\n",
      "empirical response rate 0.055689988938053096\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4275.017492939368\n",
      "orr termination 0.03522702978658778 0.08409402258165771 0.04886699279506993\n",
      "remaining size for Round 3: 140877\n",
      "Calculated size for Round 3: 5770.176917238792\n",
      "orr termination 0.008986835517914163 0.10242926911347108 0.09344243359555691\n",
      "remaining size for Round 4: 135107\n",
      "Calculated size for Round 4: 7162.685327472363\n",
      "orr termination 0.0619857939286132 0.08266087776471293 0.020675083836099728\n",
      "remaining size for Round 5: 127945\n",
      "Calculated size for Round 5: 9654.364489255498\n",
      "orr termination 0.05290705209489645 0.07409236672307012 0.021185314628173676\n",
      "remaining size for last Round 6: 118291\n",
      "85\n",
      "['step1_rr: 84', 'step2_rr: 84', 'step3_rr: 84', 'step4_rr: 84', 'step5_rr: 84', 'step6_rr: 84']\n",
      "empirical response rate 0.05530973451327434\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4248.542691505998\n",
      "orr termination 0.00821390744829811 0.09871642792035668 0.09050252047205856\n",
      "remaining size for Round 3: 140904\n",
      "Calculated size for Round 3: 9367.957052302869\n",
      "orr termination 0.026725894434657987 0.07918280771883736 0.05245691328417937\n",
      "remaining size for Round 4: 131537\n",
      "86\n",
      "['step1_rr: 85', 'step2_rr: 85', 'step3_rr: 85', 'step4_rr: 85', 'step5_rr: 85', 'step6_rr: 85']\n",
      "empirical response rate 0.05513689159292035\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4236.515296599448\n",
      "orr termination 0.023115210021386556 0.08683181690341639 0.06371660688202983\n",
      "remaining size for Round 3: 140916\n",
      "Calculated size for Round 3: 5339.355655092228\n",
      "orr termination 0.06200861189111606 0.08238980261708788 0.020381190725971818\n",
      "remaining size for Round 4: 135577\n",
      "Calculated size for Round 4: 8704.926178398353\n",
      "orr termination 0.008758466861680704 0.09228866055404919 0.08353019369236848\n",
      "remaining size for Round 5: 126873\n",
      "Calculated size for Round 5: 6432.840052399901\n",
      "orr termination 0.0383476230701075 0.07900071084154138 0.04065308777143388\n",
      "remaining size for last Round 6: 120441\n",
      "87\n",
      "['step1_rr: 86', 'step2_rr: 86', 'step3_rr: 86', 'step4_rr: 86', 'step5_rr: 86', 'step6_rr: 86']\n",
      "empirical response rate 0.05503318584070797\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4229.300841747606\n",
      "orr termination 0.0142337857139167 0.08620197693302216 0.07196819121910546\n",
      "remaining size for Round 3: 140923\n",
      "Calculated size for Round 3: 4553.463839217159\n",
      "orr termination 0.0037124496171324585 0.10281855843474089 0.09910610881760842\n",
      "remaining size for Round 4: 136370\n",
      "88\n",
      "['step1_rr: 87', 'step2_rr: 87', 'step3_rr: 87', 'step4_rr: 87', 'step5_rr: 87', 'step6_rr: 87']\n",
      "empirical response rate 0.05454922566371682\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4195.653045699933\n",
      "orr termination 0.04191824263927186 0.08330789278107747 0.041389650141805616\n",
      "remaining size for Round 3: 140957\n",
      "Calculated size for Round 3: 5947.850547209403\n",
      "orr termination 0.009821116058941311 0.094814686978518 0.08499357091957668\n",
      "remaining size for Round 4: 135010\n",
      "Calculated size for Round 4: 6786.4675158554255\n",
      "orr termination 0.04731513939824885 0.07913743518040003 0.031822295782151185\n",
      "remaining size for Round 5: 128224\n",
      "Calculated size for Round 5: 8948.22709783676\n",
      "orr termination 0.02512681900896862 0.07450035227924638 0.04937353327027776\n",
      "remaining size for last Round 6: 119276\n",
      "89\n",
      "['step1_rr: 88', 'step2_rr: 88', 'step3_rr: 88', 'step4_rr: 88', 'step5_rr: 88', 'step6_rr: 88']\n",
      "empirical response rate 0.055897400442477874\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4289.466714215505\n",
      "orr termination 0.0357449402443874 0.08237342895471735 0.04662848871032994\n",
      "remaining size for Round 3: 140863\n",
      "Calculated size for Round 3: 5545.352623230899\n",
      "orr termination 0.01892668231736322 0.09270213993406064 0.07377545761669742\n",
      "remaining size for Round 4: 135318\n",
      "Calculated size for Round 4: 7221.055641912987\n",
      "orr termination 0.007631729143093855 0.08275129030609751 0.07511956116300365\n",
      "remaining size for Round 5: 128097\n",
      "90\n",
      "['step1_rr: 89', 'step2_rr: 89', 'step3_rr: 89', 'step4_rr: 89', 'step5_rr: 89', 'step6_rr: 89']\n",
      "empirical response rate 0.05530973451327434\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4248.542691505998\n",
      "orr termination 0.01243669533947464 0.09500982343142744 0.0825731280919528\n",
      "remaining size for Round 3: 140904\n",
      "Calculated size for Round 3: 16709.677811619196\n",
      "orr termination 0.037918374705854956 0.07162927619928085 0.03371090149342589\n",
      "remaining size for Round 4: 124195\n",
      "91\n",
      "['step1_rr: 90', 'step2_rr: 90', 'step3_rr: 90', 'step4_rr: 90', 'step5_rr: 90', 'step6_rr: 90']\n",
      "empirical response rate 0.05610481194690266\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4303.921878005255\n",
      "orr termination 0.008946442581616412 0.10219116924770347 0.09324472666608706\n",
      "remaining size for Round 3: 140849\n",
      "Calculated size for Round 3: 8530.498404907356\n",
      "orr termination 0.039677593973941896 0.06559270400046778 0.025915110026525884\n",
      "remaining size for Round 4: 132319\n",
      "Calculated size for Round 4: 6561.173070661551\n",
      "orr termination 0.02770949883530985 0.0732520871545635 0.045542588319253646\n",
      "remaining size for Round 5: 125758\n",
      "92\n",
      "['step1_rr: 91', 'step2_rr: 91', 'step3_rr: 91', 'step4_rr: 91', 'step5_rr: 91', 'step6_rr: 91']\n",
      "empirical response rate 0.05268252212389381\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4066.1722164512635\n",
      "orr termination 0.025137529472251695 0.09435630333836001 0.06921877386610831\n",
      "remaining size for Round 3: 141086\n",
      "Calculated size for Round 3: 8493.397065817937\n",
      "orr termination 0.04367928041817616 0.07792223658818684 0.03424295617001068\n",
      "remaining size for Round 4: 132593\n",
      "Calculated size for Round 4: 7477.641780407988\n",
      "orr termination 0.0251335568390691 0.08535753051921674 0.06022397368014764\n",
      "remaining size for Round 5: 125116\n",
      "Calculated size for Round 5: 6729.646133599333\n",
      "orr termination 0.03657989946531508 0.0740998639844278 0.03751996451911273\n",
      "remaining size for last Round 6: 118387\n",
      "93\n",
      "['step1_rr: 92', 'step2_rr: 92', 'step3_rr: 92', 'step4_rr: 92', 'step5_rr: 92', 'step6_rr: 92']\n",
      "empirical response rate 0.05620851769911504\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4311.151688005828\n",
      "orr termination 0.026411431917957237 0.09257622659031514 0.0661647946723579\n",
      "remaining size for Round 3: 140841\n",
      "Calculated size for Round 3: 8444.403300560183\n",
      "orr termination 0.07017740811503627 0.07628546677726344 0.006108058662227175\n",
      "remaining size for Round 4: 132397\n",
      "Calculated size for Round 4: 7808.315351957705\n",
      "orr termination 0.056666099276101445 0.07434758070987085 0.017681481433769408\n",
      "remaining size for Round 5: 124589\n",
      "Calculated size for Round 5: 6094.707315822704\n",
      "orr termination 0.028621426852497047 0.07997348433472452 0.05135205748222747\n",
      "remaining size for last Round 6: 118495\n",
      "94\n",
      "['step1_rr: 93', 'step2_rr: 93', 'step3_rr: 93', 'step4_rr: 93', 'step5_rr: 93', 'step6_rr: 93']\n",
      "empirical response rate 0.05392699115044248\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4152.439189827319\n",
      "orr termination 0.04000141291786834 0.07125201495606348 0.031250602038195144\n",
      "remaining size for Round 3: 141000\n",
      "Calculated size for Round 3: 4692.377868360018\n",
      "orr termination 0.013510771714775218 0.09086950049141443 0.07735872877663921\n",
      "remaining size for Round 4: 136308\n",
      "Calculated size for Round 4: 5936.621683025374\n",
      "orr termination 0.035689906729625714 0.07786242525962575 0.042172518530000036\n",
      "remaining size for Round 5: 130372\n",
      "Calculated size for Round 5: 8495.870962646402\n",
      "orr termination 0.05841764590806641 0.07069979855654517 0.012282152648478754\n",
      "remaining size for last Round 6: 121877\n",
      "95\n",
      "['step1_rr: 94', 'step2_rr: 94', 'step3_rr: 94', 'step4_rr: 94', 'step5_rr: 94', 'step6_rr: 94']\n",
      "empirical response rate 0.054307245575221236\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4178.841293359456\n",
      "orr termination 0.0354210990200056 0.07981754295684451 0.04439644393683891\n",
      "remaining size for Round 3: 140974\n",
      "Calculated size for Round 3: 5364.929690320428\n",
      "orr termination 0.00513042595543591 0.09249462415226069 0.08736419819682478\n",
      "remaining size for Round 4: 135610\n",
      "Calculated size for Round 4: 6588.541940436449\n",
      "orr termination 0.026201659114788892 0.07660532297557925 0.05040366386079036\n",
      "remaining size for Round 5: 129022\n",
      "96\n",
      "['step1_rr: 95', 'step2_rr: 95', 'step3_rr: 95', 'step4_rr: 95', 'step5_rr: 95', 'step6_rr: 95']\n",
      "empirical response rate 0.05734928097345133\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4390.777592437488\n",
      "orr termination 0.014035648921993722 0.09743347171276201 0.0833978227907683\n",
      "remaining size for Round 3: 140762\n",
      "Calculated size for Round 3: 7670.924680786065\n",
      "orr termination 0.051728737480910836 0.07397773150954343 0.02224899402863259\n",
      "remaining size for Round 4: 133092\n",
      "Calculated size for Round 4: 7742.454648082148\n",
      "orr termination 0.058035534638964166 0.06664947889184683 0.008613944252882665\n",
      "remaining size for Round 5: 125350\n",
      "Calculated size for Round 5: 5884.680982899807\n",
      "orr termination 0.059031193364208666 0.06967521042228733 0.010644017058078666\n",
      "remaining size for last Round 6: 119466\n",
      "97\n",
      "['step1_rr: 96', 'step2_rr: 96', 'step3_rr: 96', 'step4_rr: 96', 'step5_rr: 96', 'step6_rr: 96']\n",
      "empirical response rate 0.05496404867256637\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4224.492031131622\n",
      "orr termination 0.013541104012850823 0.09711800880786305 0.08357690479501223\n",
      "remaining size for Round 3: 140928\n",
      "Calculated size for Round 3: 10064.246397360492\n",
      "orr termination 0.027724518075370477 0.0699128635590013 0.04218834548363082\n",
      "remaining size for Round 4: 130864\n",
      "Calculated size for Round 4: 6663.963912163864\n",
      "orr termination 0.03754588824558863 0.07277769490794961 0.035231806662360976\n",
      "remaining size for Round 5: 124201\n",
      "98\n",
      "['step1_rr: 97', 'step2_rr: 97', 'step3_rr: 97', 'step4_rr: 97', 'step5_rr: 97', 'step6_rr: 97']\n",
      "empirical response rate 0.05420353982300885\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4171.638735916014\n",
      "orr termination 0.04382701739614564 0.07665123331622876 0.032824215920083125\n",
      "remaining size for Round 3: 140981\n",
      "Calculated size for Round 3: 5252.000930353601\n",
      "orr termination 0.03505800368528244 0.08245285975121203 0.04739485606592959\n",
      "remaining size for Round 4: 135729\n",
      "Calculated size for Round 4: 6237.419461182588\n",
      "orr termination 0.013279287473045307 0.09472849180275436 0.08144920432970905\n",
      "remaining size for Round 5: 129492\n",
      "Calculated size for Round 5: 7266.2117549454\n",
      "orr termination 0.03606123263173753 0.07262965990335525 0.03656842727161772\n",
      "remaining size for last Round 6: 122226\n",
      "99\n",
      "['step1_rr: 98', 'step2_rr: 98', 'step3_rr: 98', 'step4_rr: 98', 'step5_rr: 98', 'step6_rr: 98']\n",
      "empirical response rate 0.05278622787610619\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4073.3529413786146\n",
      "orr termination 0.020172655107341023 0.08136736251402348 0.06119470740668245\n",
      "remaining size for Round 3: 141079\n",
      "Calculated size for Round 3: 6114.284670365321\n",
      "orr termination 0.05709850222694615 0.0770566821879254 0.019958179960979247\n",
      "remaining size for Round 4: 134965\n",
      "Calculated size for Round 4: 8055.218258775574\n",
      "orr termination 0.028917198687872396 0.08112425844807405 0.05220705976020166\n",
      "remaining size for Round 5: 126910\n",
      "Calculated size for Round 5: 6560.799383302038\n",
      "orr termination 0.031818164994756794 0.07781245059646572 0.04599428560170892\n",
      "remaining size for last Round 6: 120350\n",
      "100\n",
      "['step1_rr: 99', 'step2_rr: 99', 'step3_rr: 99', 'step4_rr: 99', 'step5_rr: 99', 'step6_rr: 99']\n",
      "empirical response rate 0.05517146017699115\n",
      "remaining size for Round 2: 145152\n",
      "Calculated size for Round 2: 4238.9204452449385\n",
      "orr termination 0.049391083254248104 0.08860105227852878 0.03920996902428068\n",
      "remaining size for Round 3: 140914\n",
      "Calculated size for Round 3: 29013\n",
      "orr termination 0.05331199332579213 0.07221008085163494 0.018898087525842808\n",
      "remaining size for Round 4: 111901\n",
      "Calculated size for Round 4: 5291.108789343447\n",
      "orr termination 0.025432636558447755 0.07385077431035891 0.04841813775191116\n",
      "remaining size for Round 5: 106610\n"
     ]
    }
   ],
   "source": [
    "rr_dict = {\"step{}_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "sample_size_dict = {\"step{}_sample_size\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "last_round_sample_size = []\n",
    "random_rr_dict = {\"step{}_random_max_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "random_rr_dict.update({\n",
    "    \"step{}_random_mean_rr\".format(r): [] for r in range(1, n_rounds + 1)\n",
    "})\n",
    "\n",
    "stopping_dict = {\"early_stopping\": [],\n",
    "                 \"early_stopping_plan\":[],\n",
    "                 \"early_stopping_orr\":[],\n",
    "                 \"early_stopping_size\":[]}\n",
    "final_plan_number = []\n",
    "highest_rr_overall = []\n",
    "\n",
    "better_chance = []\n",
    "\n",
    "orr_total = []\n",
    "orr_total_adaptive = []\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for seed in random_seeds:\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "    print([f\"{key}: {len(value)}\" for key, value in rr_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "    event_list = 0\n",
    "    event_list_adaptive = 0\n",
    "    max_rr = []\n",
    "\n",
    "    # step 1:\n",
    "    ## Generate dataset\n",
    "    dt_design_5 = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed)\n",
    "    print(\"empirical response rate\", dt_design_5['response'].mean())\n",
    "    ## save for benchmark results\n",
    "    maxidx = np.argmax(dt_design_5['plan_response_rate'])\n",
    "    random_rr_dict['step1_random_max_rr'].append(dt_design_5.iloc[maxidx,6])\n",
    "    random_rr_dict['step1_random_mean_rr'].append(dt_design_5['plan_response_rate'].mean())\n",
    "\n",
    "    ## Ensemble modelling:\n",
    "    y_pred = ensemble_model_fit(data=dt_design_5, data_pred = dt_design_5)\n",
    "\n",
    "    ## select recruitment plan\n",
    "    pred_df = pd.DataFrame(np.hstack((dt_design_5,  y_pred[:, 1].reshape(-1, 1))),\n",
    "                             columns=list(dt_design_5.columns) + ['predicted_response_rate'])\n",
    "    pred_df_rr = pred_df.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "    x = pred_df_rr['predicted_response_rate'].values\n",
    "    if len(x) <= 10:\n",
    "        best_k = 2\n",
    "    else:\n",
    "        best_k = kmeans_fit(data = x)['best_k']\n",
    "    kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "    merged_df = pd.merge(pred_df_rr, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "    cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "    highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "    highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "    p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "    highest_cluster['p_vec'] = p_vec_next\n",
    "    highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "    # highest_cluster = pred_df_rr\n",
    "\n",
    "    # highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "    # highest_cluster['cluster_number'] = highest_cluster['recruitment_plan']\n",
    "\n",
    "    ## prepare to chance of better performance:\n",
    "    temp = dt_design_5[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "    event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(\"step1\", np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(temp)\n",
    "    p0 = (temp['plan_response_rate'].mean())\n",
    "\n",
    "    rr_dict[\"step1_rr\"].append(dt_design_5['plan_response_rate'].mean())\n",
    "    sample_size_dict[\"step1_sample_size\"].append(len(dt_design_5))\n",
    "\n",
    "    for round in range(2, n_rounds+1):\n",
    "\n",
    "        rr_dict[\"step\"+str(round)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "        plan_number_dict[\"step\"+str(round)+\"_plan_number\"].append(len(p_vec_next))\n",
    "\n",
    "        ## when it comes to the last round:\n",
    "        if round == n_rounds:\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            if round == 2: # if the last round is round 2\n",
    "                remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "            else:\n",
    "                remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "            print(\"remaining size for last Round \" + str(round) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            ## data generation, combine previous data:\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                          design_number = n_design, n_rounds = n_rounds,\n",
    "                                                          n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(0)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(len(temp), temp['plan_response_rate'].mean())\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        ## If haven't reached the last round:\n",
    "        ## check remaining sample size:\n",
    "        if round == 2:\n",
    "            remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "        else:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "        print(\"remaining size for Round \" + str(round) + \": \" + str(remaining_size))\n",
    "\n",
    "        ## determine whether move on to step 2:\n",
    "        if len(highest_cluster) == 1: # if there is only one plan left\n",
    "\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size) # record the last round sample size\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(1)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        ## sample size determination:\n",
    "        if round == 2:\n",
    "            dt_design_5_step2up_overall = dt_design_5\n",
    "        orr_1 = dt_design_5_step2up_overall['response'].mean() # observed overall response rates for previous rounds\n",
    "        orr_2 = orr_1 + delta\n",
    "        n_1 = len(dt_design_5_step2up_overall)\n",
    "\n",
    "        size_step2up = sample_size_calc(orr_1, n_1, delta=delta, alpha=alpha, power=power) # total size for dataset\n",
    "        if size_step2up > 0 and size_step2up < 1000:\n",
    "            size_step2up = 1000 # if size in [0,1000], then it is 1000 for this round.\n",
    "        elif size_step2up >= 1000:\n",
    "            size_step2up = min(size_step2up, int(total_n/n_rounds)) # dataset size capped by the n_patient_per_plan\n",
    "        else:\n",
    "        # if size_step2up <= 0:\n",
    "            # the process stops at this step\n",
    "            print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up) + 'lt 0, break')\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(1)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up))\n",
    "\n",
    "        sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(size_step2up)\n",
    "\n",
    "        ## save benchmark results for last round:\n",
    "        dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                            n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "        maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "        ## data generation, combine previous data:\n",
    "        dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                  design_number = n_design, n_rounds = n_rounds,\n",
    "                                                  n_patient_per_plan = n_patient_per_plan, size = int(size_step2up), seed=seed)\n",
    "\n",
    "        plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "        if round == 2:\n",
    "            supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)]\n",
    "        else:\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "        dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "        dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "        ## Ensemble model fitting:\n",
    "        dt_design_5_step2up.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "        y_pred2up = ensemble_model_fit(data = dt_design_5_step2up_overall, data_pred = dt_design_5_step2up)\n",
    "        pred_df_step2up = pd.DataFrame(np.hstack((dt_design_5_step2up,  y_pred2up[:, 1].reshape(-1, 1))),\n",
    "                                    columns=list(dt_design_5_step2up.columns) + ['predicted_response_rate'])\n",
    "        pred_df_rr_step2up = pred_df_step2up.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "        ## select recruitment plans:\n",
    "        x = pred_df_rr_step2up['predicted_response_rate'].values\n",
    "\n",
    "        if len(x) <= 10:\n",
    "            best_k = 2\n",
    "        else:\n",
    "            best_k = kmeans_fit(data = x)['best_k']\n",
    "        kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "        ## match the cluster results back to the original data\n",
    "        highest_cluster_previous = highest_cluster # save previous cluster results\n",
    "\n",
    "        merged_df = pd.merge(pred_df_rr_step2up, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "        cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "        highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "        highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "        # p_vec_previous = p_vec_next # save p_vec of previous round\n",
    "        p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "\n",
    "        highest_cluster['p_vec'] = p_vec_next\n",
    "\n",
    "        highest_cluster = pd.merge(highest_cluster, dt_design_5_step2up[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "        ## prepare to chance of better performance:\n",
    "        temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "        event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(temp)\n",
    "\n",
    "        ## check early stopping predicted ORR:\n",
    "        orr_df = pd.merge(highest_cluster_previous, highest_cluster[['recruitment_plan','predicted_response_rate','p_vec']], on='recruitment_plan', how='left')\n",
    "        orr_df.fillna(0, inplace=True)\n",
    "        p_orr_1 = np.dot(np.array(orr_df['p_vec_x']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        p_orr_2 = np.dot(np.array(orr_df['p_vec_y']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        print(\"orr termination\", p_orr_1, p_orr_2, p_orr_2 - p_orr_1)\n",
    "\n",
    "        if (p_orr_2 - p_orr_1 < epsilon):\n",
    "            # step 3 use the same strategy of step2\n",
    "            print(i, p_orr_1, p_orr_2, \"early stop at Round \" + str(round))\n",
    "            rr_dict[\"step\"+str(round+1)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                                n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            ### update remaining size:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "            print(\"early stop, remaining size for Round\" + str(round + 1) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(1)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round+1), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equ-RpwzMEo7",
   "metadata": {
    "id": "equ-RpwzMEo7"
   },
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gZTkQ-gRMEo7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1707460937778,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "gZTkQ-gRMEo7",
    "outputId": "ee6ac4ed-2453-4bb8-8bd7-daa4eae37431"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step1_rr</th>\n",
       "      <th>step2_rr</th>\n",
       "      <th>step3_rr</th>\n",
       "      <th>step4_rr</th>\n",
       "      <th>step5_rr</th>\n",
       "      <th>step6_rr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.064562</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>0.060126</td>\n",
       "      <td>0.060528</td>\n",
       "      <td>0.064144</td>\n",
       "      <td>0.068471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.057896</td>\n",
       "      <td>0.060381</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.060490</td>\n",
       "      <td>0.062526</td>\n",
       "      <td>0.066628</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.057485</td>\n",
       "      <td>0.060359</td>\n",
       "      <td>0.061847</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.060281</td>\n",
       "      <td>0.063160</td>\n",
       "      <td>0.065119</td>\n",
       "      <td>0.066407</td>\n",
       "      <td>0.068471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.063272</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.059315</td>\n",
       "      <td>0.059989</td>\n",
       "      <td>0.061316</td>\n",
       "      <td>0.061722</td>\n",
       "      <td>0.068471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.061489</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.066912</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    step1_rr  step2_rr  step3_rr  step4_rr  step5_rr  step6_rr\n",
       "0      0.055  0.064562  0.068471  0.068471  0.068471       NaN\n",
       "1      0.055  0.059355  0.060126  0.060528  0.064144  0.068471\n",
       "2      0.055  0.057896  0.060381  0.068471       NaN       NaN\n",
       "3      0.055  0.060490  0.062526  0.066628  0.068471  0.068471\n",
       "4      0.055  0.057485  0.060359  0.061847  0.068471  0.068471\n",
       "..       ...       ...       ...       ...       ...       ...\n",
       "95     0.055  0.060281  0.063160  0.065119  0.066407  0.068471\n",
       "96     0.055  0.063684  0.063272  0.068471  0.068471       NaN\n",
       "97     0.055  0.059315  0.059989  0.061316  0.061722  0.068471\n",
       "98     0.055  0.061489  0.063207  0.066912  0.068471  0.068471\n",
       "99     0.055  0.066450  0.068471  0.068471  0.068471       NaN\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_df = pd.DataFrame(rr_dict)\n",
    "rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "BwsZMji-MEo7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1707460937778,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "BwsZMji-MEo7",
    "outputId": "b3e99081-0184-45c8-fe7c-127c766e4c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5 (0.8)\n"
     ]
    }
   ],
   "source": [
    "# Calculate average rounds:\n",
    "row_non_nan_counts = rr_df.count(axis=1)\n",
    "\n",
    "print(f\"{np.mean(row_non_nan_counts):.1f} ({np.std(row_non_nan_counts):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eUIMiTdEMEo7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1707460937778,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "eUIMiTdEMEo7",
    "outputId": "a71efab3-afff-4902-e063-4228fbcbb9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1_rr: 0.055 (0.000)\n",
      "step2_rr: 0.061 (0.002)\n",
      "step3_rr: 0.063 (0.003)\n",
      "step4_rr: 0.066 (0.003)\n",
      "step5_rr: 0.068 (0.002)\n",
      "step6_rr: 0.068 (0.001)\n"
     ]
    }
   ],
   "source": [
    "# orr for each round:\n",
    "result_dict = {}\n",
    "for key, values in rr_df.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "KetRvXYDMEo7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1707461007981,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "KetRvXYDMEo7",
    "outputId": "9d5eff88-85f6-458b-a364-2d86fc377236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.066 (0.001)\n"
     ]
    }
   ],
   "source": [
    "# overall orr:\n",
    "result_dict = np.array(orr_total) / total_n\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "RbNxS-49MEo8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1707461054722,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "RbNxS-49MEo8",
    "outputId": "05dae317-7ca0-4b86-814a-c36d97b68603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.068 (0.001)\n"
     ]
    }
   ],
   "source": [
    "# adaptive learning orr:\n",
    "result_dict = np.array(orr_total_adaptive) / (145152)\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fJ4Sy4igMEo7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1707461031343,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "fJ4Sy4igMEo7",
    "outputId": "5435420b-eaf1-4cb4-a1b2-44f3a62a89e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.068 (0.001)\n"
     ]
    }
   ],
   "source": [
    "# highest true rr:\n",
    "mean_result = (np.mean(highest_true_rr))\n",
    "std_result = (np.std(highest_true_rr))\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f} ({std_result:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "EwuXO4KlMEo8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1707460937971,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "EwuXO4KlMEo8",
    "outputId": "359acfea-bd68-402d-8bcd-7b2af904a3b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1_plan_number: nan (nan)\n",
      "step2_plan_number: 106.3 (53.9)\n",
      "step3_plan_number: 37.2 (33.0)\n",
      "step4_plan_number: 12.9 (16.9)\n",
      "step5_plan_number: 5.2 (7.4)\n",
      "step6_plan_number: 3.1 (3.7)\n"
     ]
    }
   ],
   "source": [
    "# average plan number for each round:\n",
    "result_dict = {}\n",
    "for key, values in plan_number_dict.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.1f} ({std_value:.1f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "TFLSgDLvMEo8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1707460937971,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "TFLSgDLvMEo8",
    "outputId": "03617b0a-dc79-4cfb-83da-83c6e5da4cc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': 0.38,\n",
       " 'early_stopping_plan': 0.38,\n",
       " 'early_stopping_orr': 0.0,\n",
       " 'early_stopping_size': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# early stopping probabilities:\n",
    "means = {key: np.mean(values) for key, values in stopping_dict.items()}\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "xxj1uDGnMEo8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1707460937971,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "xxj1uDGnMEo8",
    "outputId": "be35c129-da76-4594-a220-b8c0c5bfeb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122846.55 5679.735295548552\n"
     ]
    }
   ],
   "source": [
    "# sample size for the last round:\n",
    "print(np.mean(last_round_sample_size), np.std(last_round_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "xSzWgZBMnTe6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1707461140686,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "xSzWgZBMnTe6",
    "outputId": "a320702a-e71e-428e-e3be-970351e1d475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of better performance compared to the benchmark:\n",
    "np.mean(better_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bnDgAaq6MEo8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1707460937971,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "bnDgAaq6MEo8",
    "outputId": "7f4b4357-3335-4573-ac3b-c74bebdf7741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1_sample_size: 28928.000 (0.000)\n",
      "step2_sample_size: 4218.041 (88.467)\n",
      "step3_sample_size: 8767.954 (13826.758)\n",
      "step4_sample_size: 23679.497 (42320.049)\n",
      "step5_sample_size: 39976.305 (52653.682)\n",
      "step6_sample_size: 119912.097 (2445.153)\n"
     ]
    }
   ],
   "source": [
    "# plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "result_dict = {}\n",
    "for key, values in sample_size_dict.items():\n",
    "    mean_value = np.nanmean(values)\n",
    "    std_value = np.nanstd(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43JPK1OrNIH6",
   "metadata": {
    "id": "43JPK1OrNIH6"
   },
   "source": [
    "## Ensemble learning - 7 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "wuWSEB4bNIH9",
   "metadata": {
    "id": "wuWSEB4bNIH9"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn import ensemble, linear_model\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# def ensemble_model_fit(data, data_pred):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         data.drop(['recruitment_plan', 'plan_response_rate', 'group_size', 'response'], axis=1),\n",
    "#         data['response'],\n",
    "#         test_size=0.2,\n",
    "#         random_state=0\n",
    "#     )\n",
    "\n",
    "#     # Define the VotingClassifier with the individual classifiers\n",
    "#     voting_classifier = ensemble.VotingClassifier(\n",
    "#         estimators=[\n",
    "#             ('LR', linear_model.LogisticRegression(penalty = 'none', max_iter=200, random_state=0)),\n",
    "#             ('Lasso', linear_model.LogisticRegression(penalty = \"l1\", max_iter=200, random_state=0,\n",
    "#                                                      solver=\"liblinear\")),\n",
    "#             ('Ridge', linear_model.LogisticRegression(penalty = \"l2\", max_iter=200, random_state=0)),\n",
    "#             ('GBM', ensemble.GradientBoostingClassifier(random_state=0)),\n",
    "#             ('RF', ensemble.RandomForestClassifier(random_state=0)),\n",
    "#             ('XGB', XGBClassifier(random_state=0)),\n",
    "#             ('NN', MLPClassifier(random_state=0))\n",
    "#         ],\n",
    "#         voting='soft'\n",
    "#     )\n",
    "\n",
    "#     # Define the hyperparameter grid to search\n",
    "#     param_grid = {\n",
    "# #         'LR__C': [0.01, 0.1, 1.0],  # Regularization parameter for logistic regression\n",
    "#         'Lasso__C': [0.01, 0.1, 1.0],  # Regularization parameter for lasso regression\n",
    "#         'Ridge__C': [0.01, 0.1, 1.0],  # Regularization parameter for ridge regression\n",
    "#         'GBM__learning_rate': [0.01, 0.1, 0.5],  # Learning rate for gradient boosting machine\n",
    "#         'GBM__n_estimators': [50, 100, 200],  # Number of trees for gradient boosting machine\n",
    "#         'RF__n_estimators': [50, 100, 200],  # Number of trees for random forest\n",
    "# #         'RF__max_depth': [10, 20],  # Maximum depth of trees for random forest\n",
    "#         'XGB__learning_rate': [0.01, 0.1, 0.5],  # Learning rate for XGBoost\n",
    "#         'XGB__n_estimators': [50, 100, 200],  # Number of trees for XGBoost\n",
    "#         'NN__hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Size of hidden layers for neural networks\n",
    "#         'NN__alpha': [0.0001, 0.001, 0.01]  # Regularization parameter for neural networks\n",
    "#     }\n",
    "# #     param_grid = {\n",
    "# # #         'LR__C': [0.01, 0.1, 1.0],  # Regularization parameter for logistic regression\n",
    "# #         'Lasso__C': [0.01],  # Regularization parameter for lasso regression\n",
    "# #         'Ridge__C': [0.01],  # Regularization parameter for ridge regression\n",
    "# #         'Lasso__solver': ['liblinear','saga'],\n",
    "# #         'GBM__learning_rate': [0.01],  # Learning rate for gradient boosting machine\n",
    "# #         'GBM__n_estimators': [50],  # Number of trees for gradient boosting machine\n",
    "# #         'RF__n_estimators': [50],  # Number of trees for random forest\n",
    "# # #         'RF__max_depth': [10, 20],  # Maximum depth of trees for random forest\n",
    "# #         'XGB__learning_rate': [0.01],  # Learning rate for XGBoost\n",
    "# #         'XGB__n_estimators': [50],  # Number of trees for XGBoost\n",
    "# #         'NN__hidden_layer_sizes': [(50,)],  # Size of hidden layers for neural networks\n",
    "# #         'NN__alpha': [0.0001]  # Regularization parameter for neural networks\n",
    "# #     }\n",
    "\n",
    "#     # Create a GridSearchCV object\n",
    "#     grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "#     # Perform the grid search on the training data\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Get the best hyperparameters\n",
    "#     best_params = grid_search.best_params_\n",
    "#     print(\"Best fit parameters:\", best_params)\n",
    "\n",
    "#     # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "#     final_voting_classifier = grid_search.best_estimator_\n",
    "#     final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict probabilities instead of binary outcomes on the test set\n",
    "#     y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "#     y_pred_test = final_voting_classifier.predict(X_test)\n",
    "#     X_dt = data_pred.drop(['recruitment_plan', 'plan_response_rate', 'group_size', 'response'], axis=1)\n",
    "#     y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "#     return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ixImG0x4NIH-",
   "metadata": {
    "id": "ixImG0x4NIH-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import ensemble, linear_model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def ensemble_model_fit(data, data_pred):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(['recruitment_plan', 'plan_response_rate', 'group_size', 'response'], axis=1),\n",
    "        data['response'],\n",
    "        test_size=0.2,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Define the VotingClassifier with the individual classifiers\n",
    "    voting_classifier = ensemble.VotingClassifier(\n",
    "        estimators=[\n",
    "            ('LR', linear_model.LogisticRegression(penalty = 'none', max_iter=200, random_state=0)),\n",
    "            ('Lasso', linear_model.LogisticRegression(penalty = \"l1\", max_iter=200, random_state=0,\n",
    "                                                     solver=\"liblinear\")),\n",
    "            ('Ridge', linear_model.LogisticRegression(penalty = \"l2\", max_iter=200, random_state=0)),\n",
    "            ('GBM', ensemble.GradientBoostingClassifier(random_state=0)),\n",
    "            ('RF', ensemble.RandomForestClassifier(random_state=0)),\n",
    "            ('XGB', XGBClassifier(random_state=0)),\n",
    "            ('NN', MLPClassifier(random_state=0))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Define the hyperparameter grid to search\n",
    "#     param_grid = {\n",
    "# #         'LR__C': [0.01, 0.1, 1.0],  # Regularization parameter for logistic regression\n",
    "#         'Lasso__C': [0.01, 0.1, 1.0],  # Regularization parameter for lasso regression\n",
    "#         'Ridge__C': [0.01, 0.1, 1.0],  # Regularization parameter for ridge regression\n",
    "#         'GBM__learning_rate': [0.01, 0.1, 0.5],  # Learning rate for gradient boosting machine\n",
    "#         'GBM__n_estimators': [50, 100, 200],  # Number of trees for gradient boosting machine\n",
    "#         'RF__n_estimators': [50, 100, 200],  # Number of trees for random forest\n",
    "# #         'RF__max_depth': [10, 20],  # Maximum depth of trees for random forest\n",
    "#         'XGB__learning_rate': [0.01, 0.1, 0.5],  # Learning rate for XGBoost\n",
    "#         'XGB__n_estimators': [50, 100, 200],  # Number of trees for XGBoost\n",
    "#         'NN__hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Size of hidden layers for neural networks\n",
    "#         'NN__alpha': [0.0001, 0.001, 0.01]  # Regularization parameter for neural networks\n",
    "#     }\n",
    "    param_grid = {\n",
    "#         'LR__C': [0.01, 0.1, 1.0],  # Regularization parameter for logistic regression\n",
    "        'Lasso__C': [0.1],  # Regularization parameter for lasso regression\n",
    "        'Ridge__C': [0.01],  # Regularization parameter for ridge regression\n",
    "        'GBM__learning_rate': [0.01],  # Learning rate for gradient boosting machine\n",
    "        'GBM__n_estimators': [50],  # Number of trees for gradient boosting machine\n",
    "        'RF__n_estimators': [50],  # Number of trees for random forest\n",
    "#         'RF__max_depth': [10, 20],  # Maximum depth of trees for random forest\n",
    "        'XGB__learning_rate': [0.01],  # Learning rate for XGBoost\n",
    "        'XGB__n_estimators': [50],  # Number of trees for XGBoost\n",
    "        'NN__hidden_layer_sizes': [(50,)],  # Size of hidden layers for neural networks\n",
    "        'NN__alpha': [0.01]  # Regularization parameter for neural networks\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "#     print(\"Best fit parameters:\", best_params)\n",
    "\n",
    "    # Train the final VotingClassifier with the best hyperparameters on the full training set\n",
    "    final_voting_classifier = grid_search.best_estimator_\n",
    "    final_voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities instead of binary outcomes on the test set\n",
    "    y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n",
    "    y_pred_test = final_voting_classifier.predict(X_test)\n",
    "    X_dt = data_pred.drop(['recruitment_plan', 'plan_response_rate', 'group_size', 'response'], axis=1)\n",
    "    y_pred = final_voting_classifier.predict_proba(X_dt)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gXsJYLIiNIH-",
   "metadata": {
    "id": "gXsJYLIiNIH-"
   },
   "source": [
    "### Simulation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "IT4WmJdVNIH-",
   "metadata": {
    "id": "IT4WmJdVNIH-"
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "# create a list of random seeds\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(1, 100000) for _ in range(n_sim)]\n",
    "\n",
    "design_list = [5,8,10]\n",
    "patient_n_list = [5468,680,170] # n_patient_per_plan\n",
    "\n",
    "n_design = 8\n",
    "n_patient_per_plan = 680\n",
    "n_rounds = 6\n",
    "total_n = n_patient_per_plan * (2**n_design)\n",
    "\n",
    "## sample size determination\n",
    "beta = 0.2\n",
    "power = 1 - beta\n",
    "alpha = 0.05\n",
    "delta = 0.01 # effect size\n",
    "\n",
    "## early stopping\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WNHSDDGNIH-",
   "metadata": {
    "id": "6WNHSDDGNIH-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['step1_rr: 0', 'step2_rr: 0', 'step3_rr: 0', 'step4_rr: 0', 'step5_rr: 0', 'step6_rr: 0']\n",
      "empirical response rate 0.05361587389380531\n"
     ]
    }
   ],
   "source": [
    "rr_dict = {\"step{}_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "plan_number_dict = {\"step{}_plan_number\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "sample_size_dict = {\"step{}_sample_size\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "last_round_sample_size = []\n",
    "random_rr_dict = {\"step{}_random_max_rr\".format(r): [] for r in range(1, n_rounds + 1)}\n",
    "random_rr_dict.update({\n",
    "    \"step{}_random_mean_rr\".format(r): [] for r in range(1, n_rounds + 1)\n",
    "})\n",
    "\n",
    "stopping_dict = {\"early_stopping\": [],\n",
    "                 \"early_stopping_plan\":[],\n",
    "                 \"early_stopping_orr\":[],\n",
    "                 \"early_stopping_size\":[]}\n",
    "final_plan_number = []\n",
    "highest_rr_overall = []\n",
    "highest_true_rr = []\n",
    "\n",
    "better_chance = []\n",
    "\n",
    "orr_total = []\n",
    "orr_total_adaptive = []\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for seed in random_seeds:\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "    print([f\"{key}: {len(value)}\" for key, value in rr_dict.items()])\n",
    "\n",
    "\n",
    "\n",
    "    event_list = 0\n",
    "    event_list_adaptive = 0\n",
    "    max_rr = []\n",
    "\n",
    "    # step 1:\n",
    "    ## Generate dataset\n",
    "    dt_design_5 = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed)\n",
    "    highest_true_rr.append(np.max(dt_design_5['plan_response_rate']))\n",
    "    print(\"empirical response rate\", dt_design_5['response'].mean())\n",
    "    ## save for benchmark results\n",
    "    maxidx = np.argmax(dt_design_5['plan_response_rate'])\n",
    "    random_rr_dict['step1_random_max_rr'].append(dt_design_5.iloc[maxidx,6])\n",
    "    random_rr_dict['step1_random_mean_rr'].append(dt_design_5['plan_response_rate'].mean())\n",
    "\n",
    "    ## Ensemble modelling:\n",
    "    y_pred = ensemble_model_fit(data=dt_design_5, data_pred = dt_design_5)\n",
    "\n",
    "    ## select recruitment plan\n",
    "    pred_df = pd.DataFrame(np.hstack((dt_design_5,  y_pred[:, 1].reshape(-1, 1))),\n",
    "                             columns=list(dt_design_5.columns) + ['predicted_response_rate'])\n",
    "    pred_df_rr = pred_df.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "    x = pred_df_rr['predicted_response_rate'].values\n",
    "    if len(x) <= 10:\n",
    "        best_k = 2\n",
    "    else:\n",
    "        best_k = kmeans_fit(data = x)['best_k']\n",
    "    kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "    merged_df = pd.merge(pred_df_rr, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "    cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "    highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "    highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "    p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "    highest_cluster['p_vec'] = p_vec_next\n",
    "    highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "    # highest_cluster = pred_df_rr\n",
    "\n",
    "    # highest_cluster = pd.merge(highest_cluster, dt_design_5[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "    # highest_cluster['cluster_number'] = highest_cluster['recruitment_plan']\n",
    "\n",
    "    ## prepare to chance of better performance:\n",
    "    temp = dt_design_5[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "    event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(\"step1\", np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "    # print(temp)\n",
    "    p0 = (temp['plan_response_rate'].mean())\n",
    "\n",
    "    rr_dict[\"step1_rr\"].append(dt_design_5['plan_response_rate'].mean())\n",
    "    sample_size_dict[\"step1_sample_size\"].append(len(dt_design_5))\n",
    "\n",
    "    for round in range(2, n_rounds+1):\n",
    "\n",
    "        rr_dict[\"step\"+str(round)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "        plan_number_dict[\"step\"+str(round)+\"_plan_number\"].append(len(p_vec_next))\n",
    "\n",
    "        ## when it comes to the last round:\n",
    "        if round == n_rounds:\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            if round == 2: # if the last round is round 2\n",
    "                remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "            else:\n",
    "                remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "            print(\"remaining size for last Round \" + str(round) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            ## data generation, combine previous data:\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                          design_number = n_design, n_rounds = n_rounds,\n",
    "                                                          n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(0)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(len(temp), temp['plan_response_rate'].mean())\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        ## If haven't reached the last round:\n",
    "        ## check remaining sample size:\n",
    "        if round == 2:\n",
    "            remaining_size = total_n - len(dt_design_5) # apply to the rest of the patients\n",
    "        else:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "\n",
    "        print(\"remaining size for Round \" + str(round) + \": \" + str(remaining_size))\n",
    "\n",
    "        ## determine whether move on to step 2:\n",
    "        if len(highest_cluster) == 1: # if there is only one plan left\n",
    "\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size) # record the last round sample size\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(1)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        ## sample size determination:\n",
    "        if round == 2:\n",
    "            dt_design_5_step2up_overall = dt_design_5\n",
    "        orr_1 = dt_design_5_step2up_overall['response'].mean() # observed overall response rates for previous rounds\n",
    "        orr_2 = orr_1 + delta\n",
    "        n_1 = len(dt_design_5_step2up_overall)\n",
    "\n",
    "        size_step2up = sample_size_calc(orr_1, n_1, delta=delta, alpha=alpha, power=power) # total size for dataset\n",
    "        if size_step2up > 0 and size_step2up < 1000:\n",
    "            size_step2up = 1000 # if size in [0,1000], then it is 1000 for this round.\n",
    "        elif size_step2up >= 1000:\n",
    "            size_step2up = min(size_step2up, int(total_n/n_rounds)) # dataset size capped by the n_patient_per_plan\n",
    "        else:\n",
    "        # if size_step2up <= 0:\n",
    "            # the process stops at this step\n",
    "            print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up) + 'lt 0, break')\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                      n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "            if round == 2:\n",
    "                supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)] # data from step 1\n",
    "            else:\n",
    "                # step from previous rounds\n",
    "                supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "\n",
    "            dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(0)\n",
    "            stopping_dict['early_stopping_size'].append(1)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_step2up_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+1, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare for chance of better performance:\n",
    "            temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "            break\n",
    "\n",
    "        print('Calculated size for Round ' + str(round) + ': ' + str(size_step2up))\n",
    "\n",
    "        sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(size_step2up)\n",
    "\n",
    "        ## save benchmark results for last round:\n",
    "        dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                            n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "        maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "        random_rr_dict[\"step\"+str(round)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "        ## data generation, combine previous data:\n",
    "        dt_design_5_step2up = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next,round=round,\n",
    "                                                  design_number = n_design, n_rounds = n_rounds,\n",
    "                                                  n_patient_per_plan = n_patient_per_plan, size = int(size_step2up), seed=seed)\n",
    "\n",
    "        plan_list = dt_design_5_step2up['recruitment_plan'].unique()\n",
    "        if round == 2:\n",
    "            supp = dt_design_5[dt_design_5['recruitment_plan'].isin(plan_list)]\n",
    "        else:\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "        dt_design_5_step2up_overall = pd.concat([dt_design_5_step2up, supp.reset_index(drop=True)], axis = 0)\n",
    "        dt_design_5_step2up_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "        ## Ensemble model fitting:\n",
    "        dt_design_5_step2up.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "        y_pred2up = ensemble_model_fit(data = dt_design_5_step2up_overall, data_pred = dt_design_5_step2up)\n",
    "        pred_df_step2up = pd.DataFrame(np.hstack((dt_design_5_step2up,  y_pred2up[:, 1].reshape(-1, 1))),\n",
    "                                    columns=list(dt_design_5_step2up.columns) + ['predicted_response_rate'])\n",
    "        pred_df_rr_step2up = pred_df_step2up.groupby([f'Design_Feature_{i+1}' for i in range(n_design)]+['recruitment_plan'])['predicted_response_rate'].mean().reset_index(name='predicted_response_rate')\n",
    "\n",
    "        ## select recruitment plans:\n",
    "        x = pred_df_rr_step2up['predicted_response_rate'].values\n",
    "\n",
    "        if len(x) <= 10:\n",
    "            best_k = 2\n",
    "        else:\n",
    "            best_k = kmeans_fit(data = x)['best_k']\n",
    "        kmeans_results = kmeans_bhattacharyya(data=x, k=best_k)\n",
    "\n",
    "        ## match the cluster results back to the original data\n",
    "        highest_cluster_previous = highest_cluster # save previous cluster results\n",
    "\n",
    "        merged_df = pd.merge(pred_df_rr_step2up, kmeans_results['clusters'][['predicted_response_rate', 'cluster_number']], on='predicted_response_rate')\n",
    "        cluster_with_highest_rate = kmeans_results['clusters'].groupby('cluster_number')['predicted_response_rate'].mean().idxmax()\n",
    "        highest_cluster = merged_df[merged_df['cluster_number']==cluster_with_highest_rate].reset_index(drop=True)\n",
    "        highest_cluster.sort_values(by='predicted_response_rate', ascending=False, inplace=True)\n",
    "\n",
    "        # p_vec_previous = p_vec_next # save p_vec of previous round\n",
    "        p_vec_next = np.array(highest_cluster['predicted_response_rate']/np.sum(highest_cluster['predicted_response_rate']))\n",
    "\n",
    "        highest_cluster['p_vec'] = p_vec_next\n",
    "\n",
    "        highest_cluster = pd.merge(highest_cluster, dt_design_5_step2up[['recruitment_plan','plan_response_rate']].drop_duplicates(), how='left', on='recruitment_plan')\n",
    "\n",
    "        ## prepare to chance of better performance:\n",
    "        temp = dt_design_5_step2up[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "        event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(\"step\"+str(round), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "        # print(temp)\n",
    "\n",
    "        ## check early stopping predicted ORR:\n",
    "        orr_df = pd.merge(highest_cluster_previous, highest_cluster[['recruitment_plan','predicted_response_rate','p_vec']], on='recruitment_plan', how='left')\n",
    "        orr_df.fillna(0, inplace=True)\n",
    "        p_orr_1 = np.dot(np.array(orr_df['p_vec_x']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        p_orr_2 = np.dot(np.array(orr_df['p_vec_y']), np.array(orr_df['predicted_response_rate_y']))\n",
    "        print(\"orr termination\", p_orr_1, p_orr_2, p_orr_2 - p_orr_1)\n",
    "\n",
    "        if (p_orr_2 - p_orr_1 < epsilon):\n",
    "            # step 3 use the same strategy of step2\n",
    "            print(i, p_orr_1, p_orr_2, \"early stop at Round \" + str(round))\n",
    "            rr_dict[\"step\"+str(round+1)+\"_rr\"].append(np.dot(np.array(highest_cluster['p_vec']), np.array(highest_cluster['plan_response_rate'])))\n",
    "\n",
    "            ## save benchmark results for last round:\n",
    "            dt_benchmark = generate_data_step1(design_number = n_design, n_rounds = n_rounds,\n",
    "                                                n_patient_per_plan = n_patient_per_plan, seed=seed+round)\n",
    "            maxidx = np.argmax(dt_benchmark['plan_response_rate'])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_max_rr'].append(dt_benchmark.iloc[maxidx,6])\n",
    "            random_rr_dict[\"step\"+str(round+1)+'_random_mean_rr'].append(dt_benchmark['plan_response_rate'].mean())\n",
    "\n",
    "            ### update remaining size:\n",
    "            remaining_size -= len(dt_design_5_step2up)\n",
    "            print(\"early stop, remaining size for Round\" + str(round + 1) + \": \" + str(remaining_size))\n",
    "            sample_size_dict[\"step\"+str(round)+\"_sample_size\"].append(remaining_size)\n",
    "            last_round_sample_size.append(remaining_size)\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                sample_size_dict[\"step\"+str(r)+\"_sample_size\"].append(np.nan)\n",
    "\n",
    "            dt_design_5_rest = generate_data_step2up(highest_cluster=highest_cluster, p_vec = p_vec_next, round=round,\n",
    "                                                      design_number = n_design, n_rounds = n_rounds,\n",
    "                                                      n_patient_per_plan = n_patient_per_plan, size = int(remaining_size), seed=seed)\n",
    "            plan_list = dt_design_5_rest['recruitment_plan'].unique()\n",
    "            supp = dt_design_5_step2up_overall[dt_design_5_step2up_overall['recruitment_plan'].isin(plan_list)]\n",
    "            dt_design_5_rest_overall = pd.concat([dt_design_5_rest, supp.reset_index(drop=True)], axis = 0)\n",
    "            dt_design_5_rest_overall.drop(['predicted_response_rate','cluster_number','p_vec'], axis=1, inplace=True)\n",
    "\n",
    "            stopping_dict['early_stopping'].append(1)\n",
    "            stopping_dict['early_stopping_plan'].append(0)\n",
    "            stopping_dict['early_stopping_orr'].append(1)\n",
    "            stopping_dict['early_stopping_size'].append(0)\n",
    "\n",
    "            ## record final plan number\n",
    "            final_plan_number.append(len(p_vec_next))\n",
    "\n",
    "            ## record highest responserate overall\n",
    "            highest_rr_overall.append(np.max(dt_design_5_rest_overall['plan_response_rate']))\n",
    "\n",
    "            for r in range(round+2, n_rounds+1):\n",
    "                rr_dict[\"step\"+str(r)+\"_rr\"].append(np.nan)\n",
    "\n",
    "            ## prepare to chance of better performance:\n",
    "            temp = dt_design_5_rest[['recruitment_plan','plan_response_rate', 'group_size']].drop_duplicates()\n",
    "            event_list += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            event_list_adaptive += (np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(\"step\"+str(round+1), np.dot(temp['plan_response_rate'].values, temp['group_size'].values))\n",
    "            # print(temp)\n",
    "\n",
    "            result = binomtest(int(event_list), n=total_n, p=p0)\n",
    "            # print(int(event_list), total_n, p0)\n",
    "            orr_total.append(event_list)\n",
    "            orr_total_adaptive.append(event_list_adaptive)\n",
    "            better_chance.append(1 if result.pvalue < 0.05 else 0)\n",
    "            # print(result.pvalue)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nk18kzKRNIH_",
   "metadata": {
    "id": "nk18kzKRNIH_"
   },
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9J1mW4QbNIH_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1707448236231,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "9J1mW4QbNIH_",
    "outputId": "338e681d-1cf5-4fba-cdf9-37d207e4cf10"
   },
   "outputs": [],
   "source": [
    "rr_df = pd.DataFrame(rr_dict)\n",
    "rr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6NYOdlLXNIH_",
   "metadata": {
    "id": "6NYOdlLXNIH_"
   },
   "outputs": [],
   "source": [
    "# Calculate average rounds:\n",
    "row_non_nan_counts = rr_df.count(axis=1)\n",
    "\n",
    "print(f\"{np.mean(row_non_nan_counts):.1f} ({np.std(row_non_nan_counts):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iu_OS2wcNIH_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1707448237526,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "iu_OS2wcNIH_",
    "outputId": "27d70514-4ac7-40f9-8373-5ed213bdb833"
   },
   "outputs": [],
   "source": [
    "# orr for each round:\n",
    "result_dict = {}\n",
    "for key, values in rr_df.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x86hoXfQNIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1707448339675,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "x86hoXfQNIIA",
    "outputId": "44c42256-3c95-48ff-b6a1-2d7484750b9e"
   },
   "outputs": [],
   "source": [
    "# overall orr:\n",
    "result_dict = np.array(orr_total) / total_n\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f}, StD: {std_result:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NoLQzsrsNIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1707448339008,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "NoLQzsrsNIIA",
    "outputId": "3e8dab14-08fa-4902-87dd-f18235e86888"
   },
   "outputs": [],
   "source": [
    "# highest true rr:\n",
    "print(np.mean(highest_true_rr))\n",
    "print(np.std(highest_true_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43XsahNUNIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1707447643064,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "43XsahNUNIIA",
    "outputId": "d8e676ad-a73e-4e62-f939-ebf6efadaa9e"
   },
   "outputs": [],
   "source": [
    "# adaptive learning orr:\n",
    "result_dict = np.array(orr_total_adaptive) / (145152)\n",
    "mean_result = np.mean(result_dict)\n",
    "std_result = np.std(result_dict)\n",
    "\n",
    "print(f\"Mean: {mean_result:.3f}, StD: {std_result:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-vHcWwS5NIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1707448246018,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "-vHcWwS5NIIA",
    "outputId": "7e9f5fc0-3a8c-4472-f0c2-bb0392711f79"
   },
   "outputs": [],
   "source": [
    "# average plan number for each round:\n",
    "result_dict = {}\n",
    "for key, values in plan_number_dict.items():\n",
    "    mean_value = np.mean(values)\n",
    "    std_value = np.std(values)\n",
    "    result_dict[key] = f\"{mean_value:.1f} ({std_value:.1f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ICQ_3-9NIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1707448245147,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "4ICQ_3-9NIIA",
    "outputId": "b4a9a3b4-4878-418d-da2f-545cc90cc55b"
   },
   "outputs": [],
   "source": [
    "# early stopping probabilities:\n",
    "means = {key: np.mean(values) for key, values in stopping_dict.items()}\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lpvh9zRkNIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1707447645503,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "lpvh9zRkNIIA",
    "outputId": "8d467010-aa11-47ae-85d1-b6e431ff7e3b"
   },
   "outputs": [],
   "source": [
    "# sample size for the last round:\n",
    "print(np.mean(last_round_sample_size), np.std(last_round_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IT1BZInpNIIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1707447644111,
     "user": {
      "displayName": "Xinying Fang",
      "userId": "08585996645312082255"
     },
     "user_tz": 300
    },
    "id": "IT1BZInpNIIA",
    "outputId": "05483b1d-c213-419c-c926-cb56090014dd"
   },
   "outputs": [],
   "source": [
    "\n",
    "result_dict = {}\n",
    "for key, values in sample_size_dict.items():\n",
    "    mean_value = np.nanmean(values)\n",
    "    std_value = np.nanstd(values)\n",
    "    result_dict[key] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "#     result_dict[key + \"_std\"] = std_value\n",
    "for key, value in list(result_dict.items())[:12]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7zsQ_p2wNIIA",
   "metadata": {
    "id": "7zsQ_p2wNIIA"
   },
   "outputs": [],
   "source": [
    "# probability of better performance compared to the benchmark:\n",
    "np.mean(better_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eefce4-684d-4270-925e-56ca0504990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a228e-8def-406f-915b-96e3a9e73277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7264-c94f-4dd0-b61b-e93207be4ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c62dd-2655-457e-b197-9bb651dc2a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
