{"cells":[{"cell_type":"code","execution_count":null,"id":"0258cfab-6a79-43f7-94d6-f239665231ae","metadata":{"id":"0258cfab-6a79-43f7-94d6-f239665231ae"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import time\n","import warnings\n","from itertools import product\n","from scipy.stats import norm\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n","\n","from sklearn import linear_model, svm, naive_bayes, ensemble\n","from sklearn.model_selection import cross_validate, train_test_split, RepeatedStratifiedKFold\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n","from xgboost import XGBClassifier\n","# from lightgbm import LGBMClassifier\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, roc_auc_score, matthews_corrcoef\n","\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.neural_network import MLPClassifier\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import roc_auc_score\n"]},{"cell_type":"markdown","id":"4f5bccf9-9c6a-442d-8cae-11082f32c6c1","metadata":{"id":"4f5bccf9-9c6a-442d-8cae-11082f32c6c1"},"source":["## K-means functions"]},{"cell_type":"code","execution_count":null,"id":"08814844-1867-4e1e-9307-716737dd08d2","metadata":{"id":"08814844-1867-4e1e-9307-716737dd08d2"},"outputs":[],"source":["def bhattacharyya_distance(p, q, n):\n","    PQ = np.sqrt(p * q)\n","    PQC = np.sqrt((1 - p) * (1 - q))\n","    return -n * np.log((PQ / PQC) + 1) - n * np.log(PQC)\n","\n","def silhouette_score_new(data, cluster_labels):\n","  num_samples = len(data)\n","\n","  # Initialize arrays to store a(i) and b(i) values for each sample\n","  a_values = np.zeros(num_samples)\n","  b_values = np.zeros(num_samples)\n","\n","  # Iterate over each data point\n","  for i in range(num_samples):\n","      cluster_i = cluster_labels[i]\n","\n","      # Calculate a(i) (average distance within the same cluster)\n","      indices_same_cluster = np.where(cluster_labels == cluster_i)[0]\n","      indices_same_cluster = indices_same_cluster[indices_same_cluster != i]  # Exclude the current point\n","      if len(indices_same_cluster) > 0:\n","        a_values[i] = np.mean([bhattacharyya_distance(data[i], data[j], n_patient_per_plan) for j in indices_same_cluster])\n","      else:\n","        a_values[i] = 0  # Set to 0 for singleton clusters\n","\n","      # Calculate b(i) (smallest average distance to a different cluster)\n","      unique_clusters = np.unique(cluster_labels)\n","      b_values_i = np.zeros(len(unique_clusters))\n","\n","      for j, cluster_j in enumerate(unique_clusters):\n","          if cluster_j != cluster_i:\n","              indices_other_cluster = np.where(cluster_labels == cluster_j)[0]\n","              b_values_i[j] = np.mean([bhattacharyya_distance(data[i], data[k], n_patient_per_plan) for k in indices_other_cluster])\n","          else:\n","              b_values_i[j] = np.inf\n","\n","      b_values[i] = b_values_i.min()\n","\n","  # Calculate silhouette scores for each data point\n","  silhouette_scores = (b_values - a_values) / np.maximum(a_values, b_values)\n","\n","  # Calculate the average silhouette score\n","  average_silhouette_score = np.mean(silhouette_scores)\n","\n","  # print(\"Silhouette Scores for each data point:\")\n","  # print(silhouette_scores)\n","  # print(\"\\nAverage Silhouette Score:\", average_silhouette_score)\n","\n","  return average_silhouette_score\n","\n","def kmeans_bhattacharyya(data, k, max_iter = 100, seed=42):\n","  X = {'predicted_response_rate':data}\n","  X = pd.DataFrame(X)\n","  X_orig = X.copy()\n","\n","#   print(X_orig)\n","\n","  K = k\n","\n","  # Randomly initialize centroids\n","  Centroids_orig = X_orig.sample(K, random_state=seed)\n","  custom_index = ['Distance_from_' + str(n) for n in range(1,k+1)]\n","  Centroids_orig = Centroids_orig.set_index(pd.Index(custom_index))\n","#   Centroids_orig\n","#   print(\"Centroids:\", Centroids)\n","\n","  diff = 1\n","  j = 0\n","\n","  while diff != 0 and j <= max_iter:\n","    XD = X_orig.copy()\n","    Centroids = Centroids_orig.copy()\n","\n","    for i in range(K):\n","        centroid_values = Centroids.iloc[i].values[0]\n","\n","        distances = []\n","        for data_point in X['predicted_response_rate'].values:\n","            distance_result = bhattacharyya_distance(data_point, centroid_values, n = n_patient_per_plan)\n","            distances.append(distance_result)\n","\n","        XD['Distance_from_{}'.format(i+1)] = distances\n","\n","    # Assign each point to the closest cluster\n","    X['Cluster'] = XD.iloc[:, 1:K+1].idxmin(axis=1)\n","#     print(X)\n","\n","    # Calculate new centroids\n","    Centroids_new = X.groupby(['Cluster']).mean()\n","#     print(\"Centroids_new:\", Centroids_new)\n","\n","    # Calculate the difference between old and new centroids\n","    if j == 0:\n","        diff = 1\n","        j += 1\n","    else:\n","#         print(K, Centroids_new.shape[0], Centroids.shape[0])\n","        Centroids = Centroids_orig.loc[Centroids_new.index]\n","        diff = np.abs(Centroids_new.values - Centroids.values).sum()\n","#         print(diff)\n","#         print(K, Centroids_new.shape[0], Centroids.shape[0])\n","\n","    # Update centroids\n","    Centroids = Centroids_new\n","    j += 1\n","\n","\n","  X['cluster_number'] = X['Cluster'].str.extract(r'(\\d+)').astype(int)\n","\n","  results = {'clusters': X,\n","                  'centroids': Centroids}\n","\n","  return results\n"]},{"cell_type":"code","execution_count":null,"id":"c047d404-0eff-4af5-a861-cbf16ba65f98","metadata":{"id":"c047d404-0eff-4af5-a861-cbf16ba65f98"},"outputs":[],"source":["def kmeans_fit(data, max_iter=100):\n","    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","\n","    x_train, x_test = train_test_split(x, test_size=0.2, random_state=42)\n","    length = min(len(x_train), 10)\n","    k_values = range(2, length)\n","#     print(k_values)\n","    # Initialize a list to store silhouette scores for different values of k\n","    silhouette_scores = []\n","\n","    # Iterate over different values of k\n","    for k in k_values:\n","        # Fit the KMeans model to the training data\n","        kmeans_results = kmeans_bhattacharyya(data=x_train, k=k, max_iter = 10)\n","        x_test_tmp = pd.DataFrame(x_test, columns=['predicted_response_rate'])\n","        k_range = np.sort(kmeans_results['clusters']['cluster_number'].unique())\n","        for i in range(len(kmeans_results['centroids'])):\n","#             print(k, i)\n","            centroid_values = kmeans_results['centroids']['predicted_response_rate'].values[i]\n","\n","            distances = []\n","            for data_point in x_test_tmp['predicted_response_rate']:\n","                distance_result = bhattacharyya_distance(data_point, centroid_values, n=n_patient_per_plan)\n","                distances.append(distance_result)\n","\n","            x_test_tmp['Distance_from_{}'.format(k_range[i])] = distances\n","\n","        x_test_tmp['Cluster'] = x_test_tmp.iloc[:, 1:k+1].idxmin(axis=1)\n","        x_test_tmp['cluster_number'] = x_test_tmp['Cluster'].str.extract(r'(\\d+)').astype(int)\n","\n","        silhouette_avg = silhouette_score_new(x_test_tmp['predicted_response_rate'], x_test_tmp['cluster_number'])\n","        silhouette_scores.append(silhouette_avg)\n","\n","    # Find the k value with the highest silhouette score\n","#     print(silhouette_scores)\n","    best_k = k_values[np.argmax(silhouette_scores)]\n","\n","    return {'best_k': best_k, 'silhouette_score': silhouette_scores}"]},{"cell_type":"code","execution_count":null,"id":"87b63fb7-40d2-486c-b9ae-93ea78002ef9","metadata":{"id":"87b63fb7-40d2-486c-b9ae-93ea78002ef9"},"outputs":[],"source":["# def kmeans_fit(data, max_iter=10, n_rep = 10):\n","#     warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","#     random.seed(42)\n","#     random_seeds = [random.randint(1, 100000) for _ in range(n_rep)]\n","\n","#     df_scores = pd.DataFrame()\n","\n","#     for seed in random_seeds:\n","#         x_train, x_test = train_test_split(data, test_size=0.2, random_state=seed)\n","#         length = min(len(x_train), 10)\n","#         k_values = range(2, length)\n","\n","#         # Initialize a list to store silhouette scores for different values of k\n","#         silhouette_scores = []\n","\n","#         # Iterate over different values of k\n","#         for k in k_values:\n","#             # Fit the KMeans model to the training data\n","#             # print(k)\n","#             kmeans_results = kmeans_bhattacharyya(data=x_train, k=k, max_iter = max_iter)\n","#             x_test_tmp = pd.DataFrame(x_test, columns=['predicted_response_rate'])\n","#             for i in range(k):\n","#                 centroid_values = kmeans_results['centroids']['predicted_response_rate'].values[i]\n","\n","#                 distances = []\n","#                 for data_point in x_test_tmp['predicted_response_rate']:\n","#                     distance_result = bhattacharyya_distance(data_point, centroid_values, n=n_patient_per_plan)\n","#                     distances.append(distance_result)\n","\n","#                 x_test_tmp['Distance_from_{}'.format(i + 1)] = distances\n","\n","#             x_test_tmp['Cluster'] = x_test_tmp.iloc[:, 1:k+1].idxmin(axis=1)\n","#             x_test_tmp['cluster_number'] = x_test_tmp['Cluster'].str.extract(r'(\\d+)').astype(int)\n","\n","#             silhouette_avg = silhouette_score_new(x_test_tmp['predicted_response_rate'], x_test_tmp['cluster_number'])\n","#             silhouette_scores.append(silhouette_avg)\n","\n","# #         df_scores = df_scores.concat(pd.Series(silhouette_scores), ignore_index=True)\n","#         df_scores = pd.concat([df_scores, pd.Series(silhouette_scores)], ignore_index=True)\n","\n","#     score_means = df_scores.mean()\n","#     # Find the k value with the highest silhouette score\n","#     best_k = k_values[np.argmax(score_means)]\n","\n","#     return {'best_k': best_k, 'silhouette_score': score_means}"]},{"cell_type":"markdown","id":"91477026-5bc1-4e06-bab8-be27253abbb9","metadata":{"id":"91477026-5bc1-4e06-bab8-be27253abbb9"},"source":["## Ensemble model"]},{"cell_type":"code","execution_count":null,"id":"8e1a47ab-5efa-4789-a8fd-47bbe87cef14","metadata":{"id":"8e1a47ab-5efa-4789-a8fd-47bbe87cef14"},"outputs":[],"source":["def ensemble_model_fit(data, data_pred):\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        data.drop(['recruitment_plan','plan_response_rate','group_size'], axis=1),\n","        data['response'],\n","        test_size=0.2,\n","        random_state=0\n","    )\n","\n","    # Define the VotingClassifier with the individual classifiers\n","    voting_classifier = ensemble.VotingClassifier(\n","        estimators=[\n","            ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0)),\n","#             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0))\n","                    # ('SVM', svm.SVC(kernel='linear', C=1.0, random_state=0, probability=True, class_weight='balanced'))\n","#                     ('RF', ensemble.RandomForestClassifier(n_estimators=200, criterion='gini', random_state=0))\n","                    ('XGB', XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=0))\n","                   ],\n","        voting='hard'\n","    )\n","\n","    # Define the hyperparameter grid to search\n","    # Best Hyperparameters: {'LR__C': 0.01, 'RF__n_estimators': 50, 'XGB__n_estimators': 50}\n","    param_grid = {\n","        # 'NB__alpha': [0.01, 0.05, 0.1],  # '__' is used to specify hyperparameters for individual classifiers\n","        'LR__C': [0.01, 0.5, 1.0], # [0.01],\n","        # 'Ridge__C': [0.01]\n","        # 'SVM__C': [0.01, 0.05, 0.1]\n","#         'RF__n_estimators': [50] # [10, 30, 50]\n","        'XGB__n_estimators': [50]\n","    }\n","\n","    # Create a GridSearchCV object\n","    # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n","    grid_search = GridSearchCV(voting_classifier, param_grid, cv=10, scoring='roc_auc')\n","\n","    # Perform the grid search on the training data\n","    grid_search.fit(X_train, y_train)\n","\n","    # Get the best hyperparameters\n","    best_params = grid_search.best_params_\n","\n","    # Train the final VotingClassifier with the best hyperparameters on the full training set\n","    final_voting_classifier = grid_search.best_estimator_\n","    final_voting_classifier.fit(X_train, y_train)\n","\n","    # Predict probabilities instead of binary outcomes on the test set\n","    y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n","    y_pred_test = final_voting_classifier.predict(X_test)\n","    X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size'], axis=1)\n","    y_pred = final_voting_classifier.predict_proba(X_dt)\n","\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"id":"85396cd3-5b42-468c-8b16-b09971bc06e2","metadata":{"id":"85396cd3-5b42-468c-8b16-b09971bc06e2"},"outputs":[],"source":["# from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n","# from xgboost import XGBClassifier\n","# from sklearn.neural_network import MLPClassifier\n","# from sklearn.model_selection import GridSearchCV\n","# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score"]},{"cell_type":"code","execution_count":null,"id":"33d93a27-ca16-4a38-a13f-1079efa2b924","metadata":{"id":"33d93a27-ca16-4a38-a13f-1079efa2b924"},"outputs":[],"source":["# def ensemble_model_fit(data, data_pred):\n","#     X_train, X_test, y_train, y_test = train_test_split(\n","#         data.drop(['recruitment_plan','plan_response_rate','group_size'], axis=1),\n","#         data['response'],\n","#         test_size=0.2,\n","#         random_state=0\n","#     )\n","\n","#     # Define the VotingClassifier with the individual classifiers\n","#     voting_classifier = VotingClassifier(\n","#         estimators=[\n","#             ('LR', linear_model.LogisticRegression(max_iter=200, random_state=0)),\n","#             ('Ridge', linear_model.LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=0)),\n","#             ('Lasso', linear_model.LogisticRegression(penalty='l1', solver='liblinear', max_iter=200, random_state=0)),\n","#             ('RF', RandomForestClassifier(n_estimators=200, criterion='gini', random_state=0)),\n","#             ('XGB', XGBClassifier(n_estimators=200, learning_rate=1.0, random_state=0)),\n","#             ('GBM', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=0)),\n","#             ('NN', MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=200, random_state=0))\n","#         ],\n","#         voting='soft'\n","#     )\n","\n","#     # Define the hyperparameter grid to search\n","#     param_grid = {\n","#         'LR__C': [0.01],\n","#         'Ridge__C': [0.01],\n","#         'RF__n_estimators': [50],\n","#         'XGB__n_estimators': [50],\n","#         'GBM__n_estimators': [50],\n","#         'NN__hidden_layer_sizes': [(100,)],\n","#     }\n","\n","\n","#     # Create a GridSearchCV object\n","#     # custom_scorer_auc = make_scorer(roc_auc_score, needs_proba=True)\n","#     grid_search = GridSearchCV(voting_classifier, param_grid, cv=5, scoring='roc_auc')\n","\n","#     # Perform the grid search on the training data\n","#     grid_search.fit(X_train, y_train)\n","\n","#     # Get the best hyperparameters\n","#     best_params = grid_search.best_params_\n","\n","#     # Train the final VotingClassifier with the best hyperparameters on the full training set\n","#     final_voting_classifier = grid_search.best_estimator_\n","#     final_voting_classifier.fit(X_train, y_train)\n","\n","#     # Predict probabilities instead of binary outcomes on the test set\n","#     y_pred_proba_test = final_voting_classifier.predict_proba(X_test)\n","#     y_pred_test = final_voting_classifier.predict(X_test)\n","#     X_dt = data_pred.drop(['recruitment_plan','plan_response_rate','group_size'], axis=1)\n","#     y_pred = final_voting_classifier.predict_proba(X_dt)\n","\n","#     return y_pred"]},{"cell_type":"markdown","id":"7808048e-661e-46b9-91be-d556665569a8","metadata":{"id":"7808048e-661e-46b9-91be-d556665569a8"},"source":["## Sample size determination"]},{"cell_type":"code","execution_count":null,"id":"82293c29-5cf7-4403-b229-9c7a9e15f830","metadata":{"id":"82293c29-5cf7-4403-b229-9c7a9e15f830"},"outputs":[],"source":["def sample_size_calc(orr_1, n_1, delta, alpha, power):\n","    orr_2 = orr_1 + delta\n","    numerator = orr_2 * (1-orr_2)\n","\n","    denominator1 = (delta/(norm.ppf(1 - alpha) + norm.ppf(power)))**2\n","    denominator2 = (orr_1 * (1-orr_1))/n_1\n","\n","#     print(numerator, denominator1, denominator2)\n","\n","    return numerator/(denominator1 - denominator2)"]},{"cell_type":"markdown","id":"3a872d8a-03e7-4da4-a596-f1827c457f4b","metadata":{"id":"3a872d8a-03e7-4da4-a596-f1827c457f4b"},"source":["## Data generations for each steps"]},{"cell_type":"code","execution_count":null,"id":"9a7bd385-be9b-478b-a6d5-4e3822990a79","metadata":{"id":"9a7bd385-be9b-478b-a6d5-4e3822990a79"},"outputs":[],"source":["design_list = [5,8,10]\n","patient_n_list = [5400,680,170] # n_patient_per_plan\n","\n","def generate_data_step1(design_number = 5, n_rounds = 4, n_patient_per_plan = 5400, seed=42):\n","\n","  # design_list = [5,8,10]\n","\n","  # Record the start time\n","#   start_time = time.time()\n","\n","  random.seed(seed)\n","\n","  n_design = design_number\n","  n_patient_per_plan_round = n_patient_per_plan/n_rounds # each round is capped by this patient number\n","\n","#   total_n = n_patient_per_plan * (2 ** n_design)\n","\n","  # Create all possible combinations of design features\n","  design_combinations = list(product([1, 0], repeat=n_design))\n","\n","  # Create a DataFrame from the combinations\n","  design_feature_df = pd.DataFrame(design_combinations, columns=[f'Design_Feature_{i+1}' for i in range(n_design)])\n","  design_feature_df['recruitment_plan'] = [x + 1 for x in range(2 ** n_design)]\n","\n","  # scenario 1:\n","  random.seed(seed)\n","  plan_response_rate = [random.uniform(0.01, 0.1) for _ in range(2**n_design)]\n","  design_feature_df['plan_response_rate'] = plan_response_rate\n","\n","  # each combination repeat for n_patient_per_plan_round\n","  design_feature_df = pd.DataFrame(np.repeat(design_feature_df.values, n_patient_per_plan_round, axis=0), columns=design_feature_df.columns)\n","\n","  # add response outcome column:\n","  grouped_df = design_feature_df.groupby(list(design_feature_df.columns)).size().reset_index(name='group_size')\n","\n","  def generate_responses(row):\n","      rate = row['plan_response_rate']\n","      num = row['group_size']\n","      # random.seed(42)\n","      np.random.seed(seed)\n","      return np.random.binomial(n=1, p=rate, size=int(num))\n","\n","  grouped_df['response'] = grouped_df.apply(generate_responses, axis=1)\n","\n","  # Explode the 'response' arrays to expand the DataFrame\n","  expanded_df = grouped_df.explode('response').reset_index(drop=True)\n","  expanded_df['response'] = expanded_df['response'].astype(float)\n","\n","#   end_time = time.time()\n","\n","#   elapsed_time = end_time - start_time\n","\n","#   print(f\"Elapsed time: {elapsed_time} seconds\")\n","\n","  return expanded_df"]},{"cell_type":"code","execution_count":null,"id":"d80b7dc7-5c70-4745-8c69-472b414d046d","metadata":{"id":"d80b7dc7-5c70-4745-8c69-472b414d046d"},"outputs":[],"source":["design_list = [5,8,10]\n","patient_n_list = [5400,680,170]\n","\n","def generate_data_step2up(highest_cluster, p_vec, design_number = 5, n_rounds = 4, n_patient_per_plan = 5400, size = 5400, seed=42):\n","\n","  random.seed(seed)\n","\n","  n_design = design_number\n","  n_patient_per_plan_round = n_patient_per_plan/n_rounds # each round is capped by this patient number\n","\n","  # Use numpy.random.choice to sample rows with replacement\n","#   selected_rows_indices = np.random.choice(highest_cluster.index, size=int(n_patient_per_plan_round*(2**n_design)), p=p_vec)\n","  selected_rows_indices = np.random.choice(highest_cluster.index, size=size, p=p_vec) # total data size = size\n","  sampled_rows_step2 = highest_cluster.iloc[selected_rows_indices]\n","\n","  # add response outcome column:\n","  grouped_df = sampled_rows_step2.groupby(list(sampled_rows_step2.columns)).size().reset_index(name='group_size')\n","\n","  def generate_responses(row):\n","      rate = row['plan_response_rate']\n","      num = row['group_size']\n","      np.random.seed(seed)\n","      return np.random.binomial(n=1, p=rate, size=int(num))\n","\n","  grouped_df['response'] = grouped_df.apply(generate_responses, axis=1)\n","\n","  # Explode the 'response' arrays to expand the DataFrame\n","  expanded_df = grouped_df.explode('response').reset_index(drop=True)\n","  expanded_df['response'] = expanded_df['response'].astype(float)\n","\n","\n","  return expanded_df"]},{"cell_type":"code","execution_count":null,"id":"2ac19472-4111-401c-b76d-737207fe7ef5","metadata":{"id":"2ac19472-4111-401c-b76d-737207fe7ef5"},"outputs":[],"source":["def generate_data_step3(design_number = 5):\n","\n","  random.seed(42)\n","  n_design = design_number\n","  n_patient_per_plan_round = n_patient_per_plan/n_rounds # each round is capped by this patient number\n","\n","  # Use numpy.random.choice to sample rows with replacement\n","  selected_rows_indices = np.random.choice(highest_cluster_step3.index, size=n_patient_per_plan*(2**n_design), p=p_vec_step3)\n","\n","  sampled_rows_step3 = highest_cluster_step3.iloc[selected_rows_indices]\n","\n","  # add response outcome column:\n","  grouped_df = sampled_rows_step3.groupby(list(sampled_rows_step3.columns)).size().reset_index(name='group_size')\n","\n","  def generate_responses(row):\n","      rate = row['plan_response_rate']\n","      num = row['group_size']\n","      np.random.seed(44)\n","      return np.random.binomial(n=1, p=rate, size=int(num))\n","\n","  grouped_df['response'] = grouped_df.apply(generate_responses, axis=1)\n","\n","  # Explode the 'response' arrays to expand the DataFrame\n","  expanded_df = grouped_df.explode('response').reset_index(drop=True)\n","  expanded_df['response'] = expanded_df['response'].astype(float)\n","\n","  return expanded_df"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}